
!split
========= Variational forms for systems of PDEs =========
label{ch:femsys}

Many mathematical models involve $m+1$ unknown functions
governed by a system of $m+1$ differential equations. In abstract form
we may denote the unknowns by $u^{(0)},\ldots,
u^{(m)}$ and write the governing equations as

!bt
\begin{align*}
\mathcal{L}_0(u^{(0)},\ldots,u^{(m)}) &= 0,\\
&\vdots\\
\mathcal{L}_{m}(u^{(0)},\ldots,u^{(m)}) &= 0,
\end{align*}
!et
where $\mathcal{L}_i$ is some differential operator defining differential
equation number $i$.

======= Variational forms =======
label{fem:sys:vform}

There are basically two ways of formulating a variational form
for a system of differential equations. The first method treats
each equation independently as a scalar equation, while the other
method views the total system as a vector equation with a vector function
as unknown.

===== Sequence of scalar PDEs formulation =====

Let us start with the approach that treats one equation at a time.
We multiply equation number $i$ by
some test function $v^{(i)}\in V^{(i)}$ and integrate over the domain:

!bt
\begin{align}
\int_\Omega \mathcal{L}^{(0)}(u^{(0)},\ldots,u^{(m)}) v^{(0)}\dx &= 0,
label{fem:sys:vform:1by1a}\\
&\vdots\\
\int_\Omega \mathcal{L}^{(m)}(u^{(0)},\ldots,u^{(m)}) v^{(m)}\dx &= 0
label{fem:sys:vform:1by1b}
\tp
\end{align}
!et
Terms with second-order derivatives may be integrated by parts, with
Neumann conditions inserted in boundary integrals.
Let

!bt
\[ V^{(i)} = \hbox{span}\{\baspsi_0^{(i)},\ldots,\baspsi_{N_i}^{(i)}\},\]
!et
such that

!bt
\[ u^{(i)} = B^{(i)}(\x) + \sum_{j=0}^{N_i} c_j^{(i)} \baspsi_j^{(i)}(\x),
\]
!et
where $B^{(i)}$ is a boundary function to handle nonzero Dirichlet conditions.
Observe that different unknowns may live in different spaces with different
basis functions and numbers of degrees of freedom.

From the $m$ equations in the variational forms we can derive
$m$ coupled systems of algebraic equations for the
$\Pi_{i=0}^{m} N_i$ unknown coefficients $c_j^{(i)}$, $j=0,\ldots,N_i$,
$i=0,\ldots,m$.

===== Vector PDE formulation =====

The alternative method for deriving a variational form for a system of
differential equations introduces a vector of unknown functions

!bt
\[ \u = (u^{(0)},\ldots,u^{(m)}),\]
!et
a vector of test functions

!bt
\[ \v = (v^{(0)},\ldots,v^{(m)}),\]
!et
with


!bt
\[ \u, \v \in  \V = V^{(0)}\times \cdots \times V^{(m)}
\tp \]
!et
With nonzero Dirichlet conditions, we have a vector
$\bm{B} = (B^{(0)},\ldots,B^{(m)})$ with boundary functions and then
it is $\u - \bm{B}$ that lies in $\V$, not $\u$ itself.

The governing system of differential equations is written

!bt
\[ \bm{\mathcal{L}}(\u ) = 0,\]
!et
where

!bt
\[ \bm{\mathcal{L}}(\u ) = (\mathcal{L}^{(0)}(\u),\ldots, \mathcal{L}^{(m)}(\u))
\tp \]
!et
The variational form is derived by taking the inner product of
the vector of equations and the test function vector:

!bt
\begin{equation}
\int_\Omega \bm{\mathcal{L}}(\u )\cdot\v = 0\quad\forall\v\in\V\tp
label{fem:sys:vform:inner}
\end{equation}
!et

Observe that (ref{fem:sys:vform:inner}) is one scalar equation. To derive
systems of algebraic equations for the unknown coefficients in the
expansions of the unknown functions, one chooses $m$ linearly
independent $\v$ vectors to generate $m$ independent variational forms
from (ref{fem:sys:vform:inner}).  The particular choice $\v =
(v^{(0)},0,\ldots,0)$ recovers (ref{fem:sys:vform:1by1a}), $\v =
(0,\ldots,0,v^{(m)}$ recovers (ref{fem:sys:vform:1by1b}), and $\v =
(0,\ldots,0,v^{(i)},0,\ldots,0)$ recovers the variational form number
$i$, $\int_\Omega \mathcal{L}^{(i)} v^{(i)}\dx =0$, in
(ref{fem:sys:vform:1by1a})-(ref{fem:sys:vform:1by1b}).

======= A worked example =======
label{fem:sys:uT:ex}

We now consider a specific system of two partial differential equations
in two space dimensions:

!bt
\begin{align}
\mu \nabla^2 w &= -\beta,
label{fem:sys:wT:ex:weq}\\
\kappa\nabla^2 T &= - \mu ||\nabla w||^2
\tp
label{fem:sys:wT:ex:Teq}
\end{align}
!et
The unknown functions $w(x,y)$ and $T(x,y)$ are defined in a domain $\Omega$,
while $\mu$, $\beta$,
and $\kappa$ are given constants. The norm in
(ref{fem:sys:wT:ex:Teq}) is the standard Euclidean norm:

!bt
\[ ||\nabla w||^2 = \nabla w\cdot\nabla w = w_x^2 + w_y^2
\tp \]
!et

The boundary conditions associated with
(ref{fem:sys:wT:ex:weq})-(ref{fem:sys:wT:ex:Teq}) are $w=0$ on
$\partial\Omega$ and $T=T_0$ on $\partial\Omega$.
Each of the equations (ref{fem:sys:wT:ex:weq}) and (ref{fem:sys:wT:ex:Teq})
needs one condition at each point on the boundary.

The system (ref{fem:sys:wT:ex:weq})-(ref{fem:sys:wT:ex:Teq}) arises
from fluid flow in a straight pipe, with the $z$ axis in the direction
of the pipe. The domain $\Omega$ is a cross section of the pipe, $w$
is the velocity in the $z$ direction, $\mu$
is the viscosity of the fluid, $\beta$ is the pressure gradient along
the pipe, $T$ is the temperature,
and $\kappa$ is the heat conduction coefficient of the
fluid. The equation (ref{fem:sys:wT:ex:weq}) comes from the Navier-Stokes
equations, and (ref{fem:sys:wT:ex:Teq}) follows from the energy equation.
The term $- \mu ||\nabla w||^2$ models heating of the fluid
due to internal friction.

Observe that the system (ref{fem:sys:wT:ex:weq})-(ref{fem:sys:wT:ex:Teq}) has
only a one-way coupling: $T$ depends on $w$, but $w$ does not depend on
$T$, because we can solve (ref{fem:sys:wT:ex:weq}) with respect
to $w$ and then (ref{fem:sys:wT:ex:Teq}) with respect to $T$.
Some may argue that this is not a real system of PDEs, but just two scalar
PDEs. Nevertheless, the one-way coupling
is convenient when comparing different variational forms
and different implementations.

======= Identical function spaces for the unknowns =======

Let us first apply the same function space $V$ for $w$ and $T$
(or more precisely, $w\in V$ and $T-T_0 \in V$).
With

!bt
\[ V = \hbox{span}\{\baspsi_0(x,y),\ldots,\baspsi_N(x,y)\}, \]
!et
we write

!bt
\begin{equation}
w = \sum_{j=0}^N c^{(w)}_j \baspsi_j,\quad T = T_0 + \sum_{j=0}^N c^{(T)}_j
\baspsi_j\tp
label{fem:sys:wT:ex:sum}
\end{equation}
!et
Note that $w$ and $T$ in (ref{fem:sys:wT:ex:weq})-(ref{fem:sys:wT:ex:Teq})
denote the exact solution of the PDEs, while $w$ and $T$
(ref{fem:sys:wT:ex:sum}) are the discrete functions that approximate
the exact solution. It should be clear from the context whether a
symbol means the exact or approximate solution, but when we need both
at the same time, we use a subscript e to denote the exact solution.

===== Variational form of each individual PDE =====

Inserting the expansions (ref{fem:sys:wT:ex:sum})
in the governing PDEs, results in a residual in each equation,

!bt
\begin{align}
R_w &= \mu \nabla^2 w + \beta,
label{fem:sys:wT:ex:weq:R}\\
R_T &= \kappa\nabla^2 T + \mu ||\nabla w||^2
\tp
label{fem:sys:wT:ex:Teq:R}
\end{align}
!et
A Galerkin method demands $R_w$ and $R_T$ do be orthogonal to $V$:

!bt
\begin{align*}
\int_\Omega R_w v \dx &=0\quad\forall v\in V,\\
\int_\Omega R_T v \dx &=0\quad\forall v\in V
\tp
\end{align*}
!et
Because of the Dirichlet conditions, $v=0$ on $\partial\Omega$.
We integrate the Laplace terms by parts and note that the boundary terms
vanish since $v=0$ on $\partial\Omega$:

!bt
\begin{align}
\int_\Omega \mu \nabla w\cdot\nabla v \dx &= \int_\Omega \beta v\dx
\quad\forall v\in V,
label{fem:sys:wT:ex:w:vf1}\\
\int_\Omega \kappa \nabla T\cdot\nabla v \dx &= \int_\Omega \mu
\nabla w\cdot\nabla w\, v\dx \quad\forall v\in V
label{fem:sys:wT:ex:T:vf1}
\tp
\end{align}
!et
The equation $R_w$ in (ref{fem:sys:wT:ex:weq:R}) is linear
in $w$, while the equation $R_T$ in (ref{fem:sys:wT:ex:Teq:R})
is linear in $T$ and nonlinear in $w$.

===== Compound scalar variational form =====

The alternative way of deriving the variational from is to
introduce a test vector function $\v\in\V = V\times V$ and take
the inner product of $\v$ and the residuals, integrated over the domain:

!bt
\[ \int_{\Omega} (R_w, R_T)\cdot\v \dx = 0\quad\forall\v\in\V
\tp \]
!et
With $\v = (v_0,v_1)$ we get

!bt
\[ \int_{\Omega} (R_w v_0 + R_T v_1) \dx = 0\quad\forall\v\in\V
\tp \]
!et
Integrating the Laplace terms by parts results in

!bt
\begin{equation}
\int_\Omega (\mu\nabla w\cdot\nabla v_0 + \kappa\nabla T\cdot\nabla v_1)\dx
= \int_\Omega (\beta v_0 + \mu\nabla w\cdot\nabla w\, v_1)\dx,
\quad\forall \v\in\V
\tp
label{fem:sys:wT:ex:wT:vf2}
\end{equation}
!et
Choosing $v_0=v$ and $v_1=0$ gives the variational form
(ref{fem:sys:wT:ex:w:vf1}), while $v_0=0$ and $v_1=v$ gives
(ref{fem:sys:wT:ex:T:vf1}).

With the inner product notation, $(p,q) = \int_\Omega pq\dx$, we
can alternatively write (ref{fem:sys:wT:ex:w:vf1}) and
(ref{fem:sys:wT:ex:T:vf1}) as

!bt
\begin{align*}
 (\mu\nabla w,\nabla v) &= (\beta, v)
\quad\forall v\in V,\\
(\kappa \nabla T,\nabla v) &= (\mu\nabla w\cdot\nabla w, v)\quad\forall v\in V,
\end{align*}
!et
or since $\mu$ and $\kappa$ are considered constant,

!bt
\begin{align}
\mu (\nabla w,\nabla v) &= (\beta, v)
\quad\forall v\in V,
label{fem:sys:wT:ex:w:vf1i}\\
\kappa(\nabla T,\nabla v) &= \mu(\nabla w\cdot\nabla w, v)\quad\forall v\in V
label{fem:sys:wT:ex:T:vf1i}
\tp
\end{align}
!et
Note that the left-hand side of (ref{fem:sys:wT:ex:w:vf1i}) is
again linear in $w$, the left-hand side
of (ref{fem:sys:wT:ex:T:vf1i}) is linear in $T$
and the nonlinearity of $w$ appears in the right-hand side
of  (ref{fem:sys:wT:ex:T:vf1i})


===== Decoupled linear systems =====

The linear systems governing the coefficients $c_j^{(w)}$ and
$c_j^{(T)}$, $j=0,\ldots,N$, are derived by inserting the
expansions (ref{fem:sys:wT:ex:sum}) in (ref{fem:sys:wT:ex:w:vf1})
and (ref{fem:sys:wT:ex:T:vf1}), and choosing $v=\baspsi_i$ for
$i=0,\ldots,N$. The result becomes

!bt
\begin{align}
\sum_{j=0}^N A^{(w)}_{i,j} c^{(w)}_j &= b_i^{(w)},\quad i=0,\ldots,N,
label{fem:sys:wT:ex:linsys:w1}\\
\sum_{j=0}^N A^{(T)}_{i,j} c^{(T)}_j &= b_i^{(T)},\quad i=0,\ldots,N,
label{fem:sys:wT:ex:linsys:T1}\\
A^{(w)}_{i,j} &= \mu(\nabla \baspsi_j,\nabla \baspsi_i),\\
b_i^{(w)} &= (\beta, \baspsi_i),\\
A^{(T)}_{i,j} &= \kappa(\nabla \baspsi_j,\nabla \baspsi_i),\\
b_i^{(T)} &= \mu((\sum_j c^{(w)}_j\nabla\baspsi_j)\cdot (\sum_k
c^{(w)}_k\nabla\baspsi_k), \baspsi_i)
\tp
\end{align}
!et

It can also be instructive to write the linear systems using matrices
and vectors. Define $K$ as the matrix corresponding to the Laplace
operator $\nabla^2$. That is, $K_{i,j} = (\nabla \baspsi_j,\nabla \baspsi_i)$.
Let us introduce the vectors

!bt
\begin{align*}
b^{(w)} &= (b_0^{(w)},\ldots,b_{N}^{(w)}),\\
b^{(T)} &= (b_0^{(T)},\ldots,b_{N}^{(T)}),\\
c^{(w)} &= (c_0^{(w)},\ldots,c_{N}^{(w)}),\\
c^{(T)} &= (c_0^{(T)},\ldots,c_{N}^{(T)})\tp
\end{align*}
!et
The system (ref{fem:sys:wT:ex:linsys:w1})-(ref{fem:sys:wT:ex:linsys:T1})
can now be expressed in matrix-vector form as

!bt
\begin{align}
\mu K c^{(w)} &= b^{(w)},\\
\kappa K c^{(T)} &= b^{(T)}\tp
\end{align}
!et

We can solve the first system for $c^{(w)}$, and then
the right-hand side $b^{(T)}$ is known such that we can
solve the second system for $c^{(T)}$. Hence, the
decoupling of the unknowns $w$ and $T$ reduces the
system of nonlinear PDEs to two linear PDEs.


===== Coupled linear systems =====

Despite the fact that $w$ can be computed first, without knowing $T$,
we shall now pretend that $w$ and $T$ enter a two-way coupling such
that we need to derive the
algebraic equations as *one system* for all the unknowns
$c_j^{(w)}$ and $c_j^{(T)}$, $j=0,\ldots,N$. This system is
nonlinear in $c_j^{(w)}$ because of the $\nabla w\cdot\nabla w$ product.
To remove this nonlinearity, imagine that we introduce an iteration
method where we replace $\nabla w\cdot\nabla w$ by
$\nabla w_{-}\cdot\nabla w$, $w_{-}$ being the $w$
computed in the previous iteration. Then the term
$\nabla w_{-}\cdot\nabla w$ is linear in $w$ since $w_{-}$ is
known. The total linear system becomes

!bt
\begin{align}
\sum_{j=0}^N A^{(w,w)}_{i,j} c^{(w)}_j + \sum_{j=0}^N A^{(w,T)}_{i,j} c^{(T)}_j
&= b_i^{(w)},\quad i=0,\ldots,N,
label{fem:sys:wT:ex:linsys:w2}\\
\sum_{j=0}^N A^{(T,w)}_{i,j} c^{(w)}_j + \sum_{j=0}^N A^{(T,T)}_{i,j} c^{(T)}_j &= b_i^{(T)},\quad i=0,\ldots,N,
label{fem:sys:wT:ex:linsys:T2}\\
A^{(w,w)}_{i,j} &= \mu(\nabla \baspsi_j,\nabla \baspsi_i),\\
A^{(w,T)}_{i,j} &= 0,\\
b_i^{(w)} &= (\beta, \baspsi_i),\\
A^{(w,T)}_{i,j} &= \mu((\nabla w_{-})\cdot\nabla\baspsi_j), \baspsi_i),\\
A^{(T,T)}_{i,j} &= \kappa(\nabla \baspsi_j,\nabla \baspsi_i),\\
b_i^{(T)} &= 0
\tp
\end{align}
!et
This system can alternatively be written in matrix-vector form as

!bt
\begin{align}
\mu K c^{(w)} &= b^{(w)},\\
L c^{(w)} + \kappa K c^{(T)} & =0,
\end{align}
!et
with $L$ as the matrix from the $\nabla w_{-}\cdot\nabla$ operator:
$L_{i,j} = A^{(w,T)}_{i,j}$. The matrix $K$ is $K_{i,j} =
A^{(w,w)}_{i,j} = A^{(T,T)}_{i,j}$.

The matrix-vector equations are often conveniently written in block form:

!bt
\[
\left(\begin{array}{cc}
\mu K & 0\\
L & \kappa K
\end{array}\right)
\left(\begin{array}{c}
c^{(w)}\\
c^{(T)}
\end{array}\right) =
\left(\begin{array}{c}
b^{(w)}\\
0
\end{array}\right),
\]
!et

Note that in the general case where all unknowns enter all equations,
we have to solve the compound system
(ref{fem:sys:wT:ex:linsys:w2})-(ref{fem:sys:wT:ex:linsys:T2}) since
then we cannot utilize the special property that
(ref{fem:sys:wT:ex:linsys:w1}) does not involve $T$ and can be solved
first.

When the viscosity depends on the temperature, the
$\mu\nabla^2w$ term must be replaced by $\nabla\cdot (\mu(T)\nabla w)$,
and then $T$ enters the equation for $w$. Now we have a two-way coupling
since both equations contain $w$ and $T$ and therefore
must be solved simultaneously.
The equation $\nabla\cdot (\mu(T)\nabla w)=-\beta$ is nonlinear,
and if some iteration procedure is invoked, where we use a previously
computed $T_{-}$ in the viscosity ($\mu(T_{-})$), the coefficient is known,
and the equation involves only one unknown, $w$. In that case we are
back to the one-way coupled set of PDEs.


We may also formulate our PDE system as a vector equation. To this end,
we introduce the vector of unknowns $\u = (u^{(0)},u^{(1)})$,
where $u^{(0)}=w$ and $u^{(1)}=T$. We then have

!bt
\[
\nabla^2 \u = \left(\begin{array}{cc}
-{\mu}^{-1}{\beta}\\
-{\kappa}^{-1}\mu \nabla u^{(0)}\cdot\nabla u^{(0)}
\end{array}\right)\tp
\]
!et

======= Different function spaces for the unknowns =======

idx{mixed finite elements}

It is easy to generalize the previous formulation to the case where
$w\in V^{(w)}$ and $T\in V^{(T)}$, where $V^{(w)}$ and $V^{(T)}$
can be different spaces with different numbers of degrees of freedom.
For example, we may use quadratic basis functions for $w$ and linear
for $T$. Approximation of the unknowns by different finite element
spaces is known as *mixed finite element methods*.

We write

!bt
\begin{align*}
V^{(w)} &= \hbox{span}\{\baspsi_0^{(w)},\ldots,\baspsi_{N_w}^{(w)}\},\\
V^{(T)} &= \hbox{span}\{\baspsi_0^{(T)},\ldots,\baspsi_{N_T}^{(T)}\}
\tp
\end{align*}
!et
The next step is to
multiply (ref{fem:sys:wT:ex:weq}) by a test function $v^{(w)}\in V^{(w)}$
and (ref{fem:sys:wT:ex:Teq}) by a $v^{(T)}\in V^{(T)}$, integrate by
parts and arrive at

!bt
\begin{align}
\int_\Omega \mu \nabla w\cdot\nabla v^{(w)} \dx &= \int_\Omega \beta v^{(w)}\dx
\quad\forall v^{(w)}\in V^{(w)},
label{fem:sys:wT:ex:w:vf3}\\
\int_\Omega \kappa \nabla T\cdot\nabla v^{(T)} \dx &= \int_\Omega \mu
\nabla w\cdot\nabla w\, v^{(T)}\dx \quad\forall v^{(T)}\in V^{(T)}
label{fem:sys:wT:ex:T:vf3}
\tp
\end{align}
!et

The compound scalar variational formulation applies a test vector function
$\v = (v^{(w)}, v^{(T)})$ and reads

!bt
\begin{equation}
\int_\Omega (\mu\nabla w\cdot\nabla v^{(w)} +
\kappa\nabla T\cdot\nabla v^{(T)})\dx
= \int_\Omega (\beta v^{(w)} + \mu\nabla w\cdot\nabla w\, v^{(T)})\dx,
label{fem:sys:wT:ex:wT:vf3}
\end{equation}
!et
valid $\forall \v\in\V = V^{(w)}\times V^{(T)}$.

As earlier, we may decoupled the system in terms
of two linear PDEs as we did with
(ref{fem:sys:wT:ex:linsys:w1})-(ref{fem:sys:wT:ex:linsys:T1})
or linearize the coupled system by introducing the previous
iterate $w_{-}$ as in
(ref{fem:sys:wT:ex:linsys:w2})-(ref{fem:sys:wT:ex:linsys:T2}).
However, we need to distinguish between $\baspsi_i^{(w)}$
and $\baspsi_i^{(T)}$, and the range in the sums over $j$
must match the number of degrees of freedom in the spaces $V^{(w)}$
and $V^{(T)}$. The formulas become

!bt
\begin{align}
\sum_{j=0}^{N_w} A^{(w)}_{i,j} c^{(w)}_j &= b_i^{(w)},\quad i=0,\ldots,N_w,
label{fem:sys:wT:ex:linsys:w1:mixed}\\
\sum_{j=0}^{N_T} A^{(T)}_{i,j} c^{(T)}_j &= b_i^{(T)},\quad i=0,\ldots,N_T,
label{fem:sys:wT:ex:linsys:T1:mixed}\\
A^{(w)}_{i,j} &= \mu(\nabla \baspsi_j^{(w)},\nabla \baspsi_i^{(w)}),\\
b_i^{(w)} &= (\beta, \baspsi_i^{(w)}),\\
A^{(T)}_{i,j} &= \kappa(\nabla \baspsi_j^{(T)},\nabla \baspsi_i^{(T)}),\\
b_i^{(T)} &= \mu(\sum_{j=0}^{N_w} c^{(w)}_j\nabla\baspsi_j^{(w)})\cdot (\sum_{k=0}^{N_w}
c^{(w)}_k\nabla\baspsi_k^{(w)}) , \baspsi_i^{(T)})
\tp
\end{align}
!et

In the case we formulate one compound linear system involving
both $c^{(w)}_j$, $j=0,\ldots,N_w$, and $c^{(T)}_j$, $j=0,\ldots,N_T$,
(ref{fem:sys:wT:ex:linsys:w2})-(ref{fem:sys:wT:ex:linsys:T2})
becomes

!bt
\begin{align}
\sum_{j=0}^{N_w} A^{(w,w)}_{i,j} c^{(w)}_j + \sum_{j=0}^{N_T} A^{(w,T)}_{i,j} c^{(T)}_j
&= b_i^{(w)},\quad i=0,\ldots,N_w,
label{fem:sys:wT:ex:linsys:w2b}\\
\sum_{j=0}^{N_w} A^{(T,w)}_{i,j} c^{(w)}_j + \sum_{j=0}^{N_T} A^{(T,T)}_{i,j} c^{(T)}_j &= b_i^{(T)},\quad i=0,\ldots,N_T,
label{fem:sys:wT:ex:linsys:T2b}\\
A^{(w,w)}_{i,j} &= \mu(\nabla \baspsi_j^{(w)},\nabla \baspsi_i^{(w)}),\\
A^{(w,T)}_{i,j} &= 0,\\
b_i^{(w)} &= (\beta, \baspsi_i^{(w)}),\\
A^{(w,T)}_{i,j} &= \mu (\nabla w_{-}\cdot\nabla\baspsi_j^{(w)}), \baspsi_i^{(T)}),\\
A^{(T,T)}_{i,j} &= \kappa(\nabla \baspsi_j^{(T)},\nabla \baspsi_i^{(T)}),\\
b_i^{(T)} &= 0
\tp
\end{align}
!et
Here, we have again performed a linearization by employing a previous iterate $w_{-}$.
The corresponding block form

!bt
\[
\left(\begin{array}{cc}
\mu K^{(w)} & 0\\
L & \kappa K^{(T)}
\end{array}\right)
\left(\begin{array}{c}
c^{(w)}\\
c^{(T)}
\end{array}\right) =
\left(\begin{array}{c}
b^{(w)}\\
0
\end{array}\right),
\]
!et
has square and rectangular block matrices: $K^{(w)}$ is $N_w\times N_w$,
$K^{(T)}$ is $N_T\times N_T$, while $L$ is $N_T\times N_w$,

======= Computations in 1D =======
label{femsys:cooling:1D}
# 2DO
# show analytical solution in [0,H]
# use global polynomials (x^i(H-x)), exact sol
# compute uncoupled and coupled discrete systems, N=4
# note: coupled can use exact w_{-}
# P1-P1, n elements, 2 elements as special case
# uncoupled and coupled (can use exercises for variants)
# P1 for w, P2 for T or P4 for T
# similar computations for circle can be done as project
# extensions to time-dep versions in projects
# any geophysical applications? flowing ice sheet ("half channel") and
# temp gradient through, check with Jed
# the time-dependent system can be introduced in diffusion and
# a finite difference scheme can be devised

We can reduce the system (ref{fem:sys:wT:ex:weq})-(ref{fem:sys:wT:ex:Teq})
to one space dimension, which corresponds to flow in a channel between
two flat plates. Alternatively, one may consider flow in a circular
pipe, introduce cylindrical coordinates, and utilize the radial symmetry
to reduce the equations to a one-dimensional problem in the radial
coordinate. The former model becomes

!bt
\begin{align}
\mu w_{xx} &= -\beta,
label{fem:sys:wT:ex1D:weq}\\
\kappa T_{xx} &= - \mu w_x^2,
label{fem:sys:wT:ex1D:Teq}
\end{align}
!et
while the model in the radial coordinate $r$ reads

!bt
\begin{align}
\mu\frac{1}{r}\frac{d}{dr}\left( r\frac{dw}{dr}\right) &= -\beta,
label{fem:sys:wT:ex1Dr:weq}\\
\kappa \frac{1}{r}\frac{d}{dr}\left( r\frac{dT}{dr}\right) &= - \mu \left(
\frac{dw}{dr}\right)^2
\tp
label{fem:sys:wT:ex1Dr:Teq}
\end{align}
!et

The domain for (ref{fem:sys:wT:ex1D:weq})-(ref{fem:sys:wT:ex1D:Teq})
is $\Omega = [0,H]$, with boundary conditions $w(0)=w(H)=0$ and
$T(0)=T(H)=T_0$. For
(ref{fem:sys:wT:ex1Dr:weq})-(ref{fem:sys:wT:ex1Dr:Teq}) the domain
is $[0,R]$ ($R$ being the radius of the pipe) and the boundary
conditions are $du/dr = dT/dr =0$ for $r=0$, $u(R)=0$, and $T(R)=T_0$.

The exact solutions, $w_e$ and $T_e$,  to (ref{fem:sys:wT:ex1D:weq})
and (ref{fem:sys:wT:ex1D:Teq}) are computed
as
!bt
\begin{align*}
w_{e,x} &= - \int \frac{\beta}{\mu} \dx + C_w, \\
w_e &= \int w_x \dx + D_w, \\
T_{e,x} &= - \int \mu w_x^2 \dx + C_T,\\
w_e &= \int w_x \dx + D_T, \\
\end{align*}
!et
where we determine the constants $C_w$, $D_w$, $C_T$, and $D_T$
by the boundary conditions $w(0)=w(H)=0$ and
$T(0)=T(H)=T_0$. The calculations
may be performed in  `sympy` as
!bc pycod
import sympy as sym

x, mu, beta, k, H, C, D, T0 = sym.symbols("x mu beta k H C D T0")
wx = sym.integrate(-beta/mu, (x, 0, x)) + C
w = sym.integrate(wx, x) + D
s = sym.solve([w.subs(x, 0)-0,  # x=0 condition
               w.subs(x,H)-0],  # x=H condition
               [C, D])       # unknowns
w = w.subs(C, s[C]).subs(D, s[D])
w = sym.simplify(sym.expand(w))

Tx = sym.integrate(-mu*sym.diff(w,x)**2, x) + C
T = sym.integrate(Tx, x) + D
s = sym.solve([T.subs(x, 0)-T0,  # x=0 condition
               T.subs(x, H)-T0],  # x=H condition
               [C, D])       # unknowns
T = T.subs(C, s[C]).subs(D, s[D])
T = sym.simplify(sym.expand(T))
!ec
We find that the solutions are
!bt
\begin{align*}
w_e(x) &= \frac{\beta x}{2 \mu} \left(H - x\right), \\
T_e(x) &= \frac{\beta^{2}}{\mu} \left(\frac{H^{3} x}{24}  - \frac{H^{2}}{8} x^{2} + \frac{H}{6} x^{3} - \frac{ x^{4}}{12}\right)  + T_{0} \tp
\end{align*}
!et

The figure ref{femsys:cooling:w:plot} shows $w$ computed by the finite element method using the  decoupled
approach with P1 elements, 
that is; implementing (ref{fem:sys:wT:ex:linsys:w1}). The analytical solution $w_e$ is a quadratic
polynomial.  The linear finite elements result in a poor approximation on the
coarse meshes, $N=2$ and $N=4$, but the approximation 
improves fast and already at $N=8$ the 
approximation appears adequate. 
The figure ref{femsys:cooling:w:plot} shows the approximation of $T$ and also here
we see that the fourth order polynomial is poorly approximated at coarse resolution, but
that the approximation quickly improves.




FIGURE: [fig/cooling_w, width=800] The solution $w$ of (ref{fem:sys:wT:ex1D:weq}) with $\beta=\mu=1$ for different mesh resolutions. label{femsys:cooling:w:plot}

The figure ref{femsys:cooling:w:plot} shows $T$ for different resolutions. The same tendency is apparent
although the coarse grid solutions are worse for $T$ than for $w$. The solutions at $N=16$ and $N=32$, however, 
appear almost identical. 


FIGURE: [fig/cooling_T, width=800] The solution $T$ of (ref{fem:sys:wT:ex1D:Teq}) for $\kappa=H=1$. label{femsys:cooling:T:plot}

!bc pycod
def boundary(x):
  return x[0] < DOLFIN_EPS or x[0] > 1.0 - DOLFIN_EPS

from dolfin import *
import matplotlib.pyplot as plt

Ns = [2, 4, 8, 16, 32]
for N in Ns: 
    mesh = UnitIntervalMesh(N)
    V = FunctionSpace(mesh, "Lagrange", 1)
    u = TrialFunction(V)
    v = TestFunction(V) 

    beta = Constant(1)
    mu = Constant(1) 

    bc = DirichletBC(V, Constant(0), boundary)
    a = mu*inner(grad(u), grad(v))*dx 
    L = -beta*v*dx 

    w = Function(V)
    solve(a == L, w, bc)

    T0 = Constant(1)
    kappa = Constant(1)
    bc = DirichletBC(V, T0, boundary)
    a = kappa*inner(grad(u), grad(v))*dx 
    L = -mu*inner(grad(w), grad(w))*v*dx 

    T = Function(V)
    solve(a == L, T, bc)

    plt.plot(V.dofmap().tabulate_all_coordinates(mesh), T.vector().array())
    plt.hold(True)
    plt.legend(["N=%d"%N for N in Ns], loc="upper left")
plt.show()
!ec


Most of the FEniCS code should be familiar to the reader, but 
we remark that we use the function `V.dofmap().tabulate_all_coordinates(mesh)` to obtain the coordinates of the nodal points. This is a general
function that works for any finite element implemented in 
FEniCS and also in a parallel setting. 

[kam: do not need extensive description as it should be covered elsewhere]

The calculations for (ref{fem:sys:wT:ex1Dr:weq})
and (ref{fem:sys:wT:ex1Dr:Teq}) are similar.
The `sympy` code
!bc pycod
import sympy as sym

r, R = sym.symbols("r R")
rwr = sym.integrate(-(beta/mu)*r, r) + C
w = sym.integrate(rwr/r, r) + D
s = sym.solve([sym.diff(w,r).subs(r, 0)-0,  # r=0 condition
               w.subs(r,R)-0],              # r=R condition
               [C, D])                      # unknowns
w = w.subs(C, s[C]).subs(D, s[D])
w = sym.simplify(sym.expand(w))

rTr = sym.integrate(-mu*sym.diff(w,r)**2*r, r) + C
T = sym.integrate(rTr/r, r) + D
s = sym.solve([sym.diff(T,r).subs(r, 0)-T0,  # r=0 condition
               T.subs(r, R)-T0],             # r=R condition
               [C, D])                       # unknowns
T = T.subs(C, s[C]).subs(D, s[D])
T = sym.simplify(sym.expand(T))
!ec
and we obtain the solutions
!bt
\begin{align*}
w(r) &= \frac{\beta \left(R^{2} - r^{2}\right)}{4 \mu}, \\
T(r) &= \frac{1}{64 \mu} \left(R^{4} \beta^{2} + 64 T_{0} \mu - \beta^{2} r^{4}\right)\tp
\end{align*}
!et

[kam: so how do we do we conclude this? FEniCS simulation in 3D or
FEniCS simulation in 1D with r? Or perhaps both? There are some
stuff happening with piecewise integration in the presence of the r.]




===== Another example in 1D =====

label{fem:sys:up:1D}
Consider the problem
!bt
\begin{align}
label{femsys:varcoeff:1D}
 -(a u')' &= 0,\\
u(0) &= 0,\\
u(1) &= 1 \tp
\end{align}
!et
For any scalar $a$ (larger than 0), we may easily verify that the solution is $u(x)=x$.
In many applications, such as for example porous media flow
or heat conduction, the parameter $a$ contains a jump that represents
the transition from one material to another. Hence,
let us consider the problem where $a$ is on the following
form
!bt
\[
a(x) = \left\{ \begin{array}{ll}
            1 & \mbox{  if } x\le\half, \\
            a_0 &  \mbox{  if } x>\half\tp
           \end{array} \right.
\]
!et
Notice that for such an $a(x)$, the equation (ref{femsys:varcoeff:1D}) does not necessarily make
sense because we cannot differentiate $a(x)$. Strictly speaking $a'(x)$ would
be a Dirac's delta function in $x=\half$, that is; $a'(x)$ is $\infty$ at $x=\half$ and zero
everywhere else.

Hand-calculations do however show that we may be able to
compute the solution. Integrating (ref{femsys:varcoeff:1D})
yields the expression
!bt
\[
-(a u') = C
\]
!et
A trick now is to divide by $a(x)$ on both sides to obtain
!bt
\[
- u' = \frac{C}{a}
\]
!et
and hence
!bt
\[
u(x) = \frac{C}{a(x)} x + D
\]
!et
The boundary conditions demand that $u(0) = 0$, which means that $D=0$. 
In order to obtain an expression for $u(1)=1$ 
we note that $u$ is 
a piecewise linear function. 
with $u' = C$ for $x\in(0,0.5)$
and $u'=\frac{C}{a_0}$ for $x\in(0.5,1)$. 
We may therefore express $u(1)$ as 
$u(1) = u(0) + u'|_{x=0.25} 0.5 + u'|_{x=0.75} 0.5 = 0.5 (C + \frac{C}{a_0}) = 1$. 
In other words, $C=\frac{2 a_0}{a_0 +1}$. 
!bt
\begin{equation}
label{varcoeff:analytical:solution}
u(x) = 
\left\{ \begin{array}{ll}
\frac{2 a_0}{a_0 +1}   x & \mbox{ for } 0 < x \le 0.5 \\
\frac{2}{a_0 +1} (x-0.5) + \frac{a_0}{a_0 +1}  & \mbox{ for } 0.5 < x \le 1
\end{array}
\right. 
\end{equation}
!et


To obtain a variational form of this problem suitable for
finite element simulation, we transform the problem
to a homogeneous Dirichlet problem.
Let the solution be $u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_j(x)$ where
$B(x)=x$.

[kam: the right hand side is hard to compute. Perhaps find
a problem with homogeneous BC. Or perhaps just comment that we take the
linear algebra approach.
Can take the opportunity to say that the BC trick requires extra regularity
and that we instead do the LA approach.  
]

The variational problem derived from a standard Galerkin method reads: Find $u$ such that
!bt
\[
\int_{\Omega} a u' v' \, dx = \int_\Omega f v dx
\]
!et

We observe that in the variational problem, the discontinuity of $a$ does not cause any problem
as the differentiation is moved from $a$ (and $u'$) to $v$ by using integration by parts.
Letting $u=\sum_{j\in\If} c_j\baspsi_j(x)$ and $v=\baspsi_i(x)$ the corresponding linear system is $\sum_j A_{i,j}c_j=b_i$
with


!bt
\begin{align*}
A_{i,j} &= (a \baspsi_j', \baspsi_i') = \int_{\Omega} a(x) \baspsi_j'(x)
\baspsi_i'(x)\dx,\\
b_i &= (f,\baspsi_i)= 0 \tp
\end{align*}
!et

The solution of the problem is shown in Figure ref{femsys:varcoeff:1D:Galerkin:plotu} at different mesh resolutions.
The analytical solution in (ref{varcoeff:analytical:solution}) is a piecewise polynomial, linear
for $x$ in $[0,\half)$ and $(\half,1]$ and it seems that the numerical strategy gives a good approximation
of the solution.

The flux $a u'$ is often a quantity of interest. Because the flux involves differentiation with respect
to $x$ we do not have an direct access to it and 
have to compute it. A natural approach is to take the Galerkin approximation,
that is we seek a $w \approx a u'$ on the form  $w=\sum_{j\in\If} d_j\baspsi_j$ and
require  Galerkin orthogonality. In other words, we require 
that $w-a u'$ is orthogonal to $\{\baspsi_i\}$. This is done by solving
the linear system  $\sum_j M_{i,j}d_j=b_i$ with


!bt
\begin{align*}
M_{i,j} &= (a \baspsi_j, \baspsi_i) = \int_{\Omega} a(x) \baspsi_j(x) \baspsi_i(x)\dx,\\
b_i &= (a u',\baspsi_i)=  \int_\Omega a(x)  \sum_j c_j\baspsi_j'(x) \dx\tp
\end{align*}
!et

As shown in Figure ref{femsys:varcoeff:1D:Galerkin:plotsux}, this
approach does not produce a good approximation of the flux.

FIGURE: [fig/darcy_a1D, width=800] Solution of the Darcy problem with discontinuous coefficient for different number of elements $N$. label{femsys:varcoeff:1D:Galerkin:plotu}



To improve the approximation of the flux, it is common to consider an equivalent form
of (ref{femsys:varcoeff:1D}) where the flux is one of the unknowns.
The equations reads:
!bt
\begin{align}
\frac{\partial w}{\partial x} &= 0,
label{fem:sys:up:1D:eq:mass}\\
w &=-a\frac{\partial u}{\partial x}
label{fem:sys:up:1D:eq:Darcy}
\tp
\end{align}
!et
A straightforward calculation shows that inserting (ref{fem:sys:up:1D:eq:Darcy}) into (ref{fem:sys:up:1D:eq:mass}) yields
the equation (ref{femsys:varcoeff:1D}). We also note that we have replaced the second order differential
equation with a system of two first order differential equations.

It is common to swap the order of the equations and also divide equation
(ref{fem:sys:up:1D:eq:Darcy}) by $a$ in order to get a symmetric system of
equations. Then variational formulation of the problem, having the two unknowns $w$ and $u$
and corresponding test functions  $v^{(w)}$ and $v^{(u)}$,
 becomes
!bt
\begin{align}
label{fem:sys:up:1D:eq:Darcy:var}
\int_\Omega \frac{1}{a} w v^{(w)} + \frac{\partial u}{\partial x} v^{(w)} \dx &=0, \\
label{fem:sys:up:1D:eq:mass:var}
\int_\Omega \frac{\partial w}{\partial x} v^{(u)} \dx &= 0\tp
\end{align}
!et
and letting
$u=\sum_{j\in\If} c_j\baspsi_j^{(u)}$,
$w=\sum_{j\in\If} c_j\baspsi_j^{(w)}$,
$v^{(u)} =  \baspsi_i^{(u)}$, and
$v^{(w)} =  \baspsi_i^{(w)}$, we obtain the following system
of linear equations
!bt
\[
A\, c = \left[ \begin{array}{cc} A^{(w,w)} & A^{(w,u)}\\ A^{(u,w)} & 0 \end{array} \right]
\left[ \begin{array}{c} c^{(w)} \\ c^{(u)} \end{array} \right] =
\left[ \begin{array}{c} b^{(w)} \\ b^{(u)} \end{array} \right]
= b,
\]
!et
where
!bt
\begin{align*}
A^{(w,w)}_{i,j} &=  \int_{\Omega} \frac{1}{a(x)} \baspsi^{(w)}_j(x) \baspsi^{(w)}_i(x)\dx & i,j = 0\ldots N^{(w)}-1, \\
A^{(w,u)}_{i,j} &=  \int_{\Omega} \frac{\partial}{\partial x} \baspsi^{(u)}_j(x) \baspsi^{(w)}_i(x)\dx &  i=0\ldots N^{(w)}-1, j=0, \ldots N^{(u)}-1, \\
A^{(u,w)}_{i,j} &= A^{(w,u)}_{j,i}, \\
b^{(w)}_i &= (0,\baspsi^{(w)}_i)= 0, \\
b^{(u)}_i &= (0,\baspsi^{(u)}_i)= 0 \tp
\end{align*}
!et




FIGURE: [fig/darcy_a1D_mx, width=800] Solution of the mixed Darcy problem with discontinuous coefficient for different number of elements $N$. label{femsys:varcoeff:1D:Galerkin:plotu:mx}



It is interesting to note that the standard Galerkin formulation
of the problem results in a perfect approximation of $u$, while
the flux $-a u'$ is badly represented. On the other hand,
for the mixed formulation, the flux is well approximated but
$u$ is approximated only to first order yielding a staircase
approximation. These observations naturally suggest
that we should employ P1 approximation of both $u$ and
its flux. We should then get a perfect approximation of
both unknowns. This is however not possible. The
linear system we obtain with P1 elements for both variables is singular.

This example shows that when we are solving systems of PDEs with
several unknowns, we can not choose the approximation arbitrary.
The polynomial spaces of the different unknowns have to be compatible
and the accuracy of the different unknowns depend on each other. 
We will not discuss the reasons for the need of compatibility here
as it is rather mathematical and 
well presented in many books, e.g. cite{Brenner_Scott, Braess, Silvester_et_al_2015}.


FIGURE: [fig/darcy_adx1D, width=800] The corresponding flux $a u'$ for the Darcy problem with discontinuous coefficient for different number of elements $N$. label{femsys:varcoeff:1D:Galerkin:plotsux}

[kam: Not sure why the I don't get $a_0$. Need to debug. ]
[kam: integration by parts done above, but not commented. Also bc, not considered.]
[kam: wrong figure]

======= Exercises =======

===== Problem: Estimate order of convergence for the Cooling law =====
label{femsys:exer:cooling:1}
Consider the 1D Example of the fluid flow in a straight pipe 
coupled to heat conduction in Section ref{femsys:cooling:1D}. 
The example demonstrated fast convergence when using linear elements
for both variables $w$ and $T$. In this exercise we quantify the order 
of convergence. That is, we expect that
!bt
\begin{align*}
\|w - w_e \|_{L_2} &\le C_w h^{\beta_w}, \\
\|T - T_e \|_{L_2} &\le C_T h^{\beta_T},
\end{align*}
!et  
for some $C_w$, $C_T$, $\beta_w$ and $\beta_T$. 
Assume therefore that
!bt
\begin{align*}
\|w - w_e \|_{L_2} &= C_w h^{\beta_w},\\ 
\|T - T_e \|_{L_2} &= C_T h^{\beta_T},
\end{align*}
!et  
and estimate  $C_w$, $C_T$, $\beta_w$ and $\beta_T$.


===== Problem: Estimate order of convergence for the Cooling law =====
label{femsys:exer:cooling:2}
Repeat Exercise ref{femsys:exer:cooling:1} with quadratic finite
elements for both $w$ and $T$. 

# 2DO

_Calculations to be continued..._



# #ifdef 2DO
 * why we don't use least squares for finite elements (exercise?)
 * write more about `fe_approx1D_numint`, at least link to the file
 * read what we have written about 2D

!split
======= Examples on 1D finite element computations =======


Other ways of treating boundary conditions.

Next: non-uniform partition, give result and make exercise.

Next: variable coefficient, numerical integration

Then: old Exercise 2.7 symbolic when the software is ready :-)

Compulsory exercise: wave equation with P1 and P2 elements.
# #endif
