
========= Function approximation by global functions =========
label{ch:approx:global}

Many successful numerical solution methods for differential equations,
including the finite element method,
aim at approximating the unknown function by a sum

!bt
\begin{equation}
 u(x) \approx \sum_{i=0}^N c_i\baspsi_i(x),
label{fem:u}
\end{equation}
!et
where $\baspsi_i(x)$ are prescribed functions and $c_0,\ldots,c_N$
are unknown coefficients to be determined.
Solution methods for differential equations
utilizing (ref{fem:u}) must
have a *principle* for constructing $N+1$ equations to
determine $c_0,\ldots,c_N$. Then there is a *machinery* regarding
the actual constructions of the equations for $c_0,\ldots,c_N$, in a
particular problem. Finally, there is a *solve* phase for computing
the solution $c_0,\ldots,c_N$ of the $N+1$ equations.

Especially in the finite element method, the machinery for
constructing the discrete equations to be implemented on a computer is
quite comprehensive, with many mathematical and implementational
details entering the scene at the same time. From an ease-of-learning
perspective it can therefore be wise to follow an idea of Larson and
Bengzon cite{Larson_2013} and introduce the computational machinery
for a trivial equation: $u=f$. Solving this equation with $f$ given
and $u$ on the form (ref{fem:u}), means that we seek an approximation
$u$ to $f$.  This approximation problem has the advantage of
introducing most of the finite element toolbox, but without involving
demanding topics related to differential equations (e.g., integration
by parts, boundary conditions, and coordinate mappings).  This is the
reason why we shall first become familiar with finite element
*approximation* before addressing finite element methods for
differential equations.

First, we refresh some linear algebra concepts about approximating
vectors in vector spaces. Second, we extend these concepts to
approximating functions in function spaces, using the same principles
and the same notation.  We present examples on approximating functions
by global basis functions with support throughout the entire
domain. That is, the functions are in general nonzero on the entire
domain.  Third, we introduce the finite element type of basis
functions globally.  These basis functions will later,
in ref {ch:approx:fe},
be used with local support
(meaning that each function is nonzero except in a small part of the domain)
to enhance stability and efficiency.
We explain all details of the
computational algorithms involving such functions.  Four types of
approximation principles are covered: 1) the least squares method, 2)
the $L_2$ projection or Galerkin method, 3) interpolation or
collocation, and 4) the regression method.

!split
======= Approximation of vectors =======
label{fem:approx:vec}

We shall start with introducing two fundamental methods for
determining the coefficients $c_i$ in (ref{fem:u}). These methods
will be introduce for
approximation of vectors. Using vectors in vector
spaces to bring across the ideas is believed to appear more intuitive
to the reader than starting directly with functions in function spaces.
The extension from vectors to functions will be trivial as soon as
the fundamental ideas are understood.


The first method of approximation is called the *least squares method*
and consists in finding $c_i$ such that the difference $f-u$, measured
in a certain norm, is minimized. That is, we aim at finding the best
approximation $u$ to $f$, with the given norm as measure of ``distance''.
The second method is not
as intuitive: we find $u$ such that the error $f-u$ is orthogonal to
the space where $u$ lies. This is known as *projection*, or
in the context of differential equations, the idea is
also well known as *Galerkin's method*.
When approximating vectors and functions, the two methods are
equivalent, but this is no longer the case when applying the
principles to differential equations.


===== Approximation of planar vectors =====
label{fem:approx:vec:plane}
idx{approximation!of vectors in the plane}
idx{basis vector}
idx{norm}

Let $\f = (3,5)$ be a vector in the $xy$ plane and suppose we want to
approximate this vector by a vector aligned in the direction of
another vector that is restricted to be aligned with some vector
$(a,b)$. Figure ref{fem:approx:vec:plane:fig} depicts the
situation. This is the simplest approximation problem for
vectors. Nevertheless, for many readers it will be wise to refresh
some basic linear algebra by consulting a textbook.  Exercise
ref{fem:approx:exer:linalg1} suggests specific tasks to regain
familiarity with fundamental operations on inner product vector
spaces. Familiarity with such operations are assumed in the
forthcoming text.



FIGURE: [fig/vecapprox_plane, width=400] Approximation of a two-dimensional vector in a one-dimensional vector space. label{fem:approx:vec:plane:fig}

We introduce the vector space $V$
spanned by the vector $\psib_0=(a,b)$:

!bt
\begin{equation}
V = \mbox{span}\,\{ \psib_0\}\tp  \end{equation}
!et
We say that $\psib_0$ is a *basis vector* in the space $V$.
Our aim is to find the vector

!bt
\begin{equation}
label{uc0}
\u = c_0\psib_0\in V
\end{equation}
!et
which best approximates
the given vector $\f = (3,5)$. A reasonable criterion for a best
approximation could be to minimize the length of the difference between
the approximate $\u$ and the given $\f$. The difference, or error
$\e = \f -\u$, has its length given by the *norm*

!bt
\begin{equation*} ||\e|| = (\e,\e)^{\half},\end{equation*}
!et
where $(\e,\e)$ is the *inner product* of $\e$ and itself. The inner
product, also called *scalar product* or *dot product*, of two vectors
$\u=(u_0,u_1)$ and $\v =(v_0,v_1)$ is defined as

!bt
\begin{equation}
(\u, \v) = u_0v_0 + u_1v_1\tp  \end{equation}
!et

__Remark.__ We should point out that we use the notation
$(\cdot,\cdot)$ for two different things: $(a,b)$ for scalar
quantities $a$ and $b$ means the vector starting in the origin and
ending in the point $(a,b)$, while $(\u,\v)$ with vectors $\u$ and
$\v$ means the inner product of these vectors.  Since vectors are here
written in boldface font there should be no confusion.  We may add
that the norm associated with this inner product is the usual
Euclidean length of a vector, i.e.,
!bt
\[
\|\u\| = \sqrt{(\u,\u)} = \sqrt{u_0^2 + u_1^2}
\]
!et
idx{least squreas method!vectors}

=== The least squares method ===

We now want to determine the $\u$ that minimizes  $||\e||$, that is
we want to compute the optimal $c_0$ in (ref{uc0}). The algebra
is simplified if we minimize the square of the norm, $||\e||^2 = (\e, \e)$,
instead of the norm itself.
Define the function

!bt
\begin{equation}
E(c_0) = (\e,\e) = (\f - c_0\psib_0, \f - c_0\psib_0)
\tp
\end{equation}
!et
We can rewrite the expressions of the right-hand side in a more
convenient form for further work:

!bt
\begin{equation}
E(c_0) = (\f,\f) - 2c_0(\f,\psib_0) + c_0^2(\psib_0,\psib_0)\tp
label{fem:vec:E}
\end{equation}
!et
This rewrite results from using the following fundamental rules for inner
product spaces:

!bt
\begin{equation}
(\alpha\u,\v)=\alpha(\u,\v),\quad \alpha\in\Real,
label{fem:vec:rule:scalarmult}
\end{equation}
!et

!bt
\begin{equation}
(\u +\v,\w) = (\u,\w) + (\v, \w),
label{fem:vec:rule:sum}
\end{equation}
!et

!bt
\begin{equation}
label{fem:vec:rule:symmetry}
(\u, \v) = (\v, \u)\tp
\end{equation}
!et

Minimizing $E(c_0)$ implies finding $c_0$ such that

!bt
\begin{equation*} \frac{\partial E}{\partial c_0} = 0\tp  \end{equation*}
!et
It turns out that $E$ has one unique minimum and no maximum point.
Now, when differentiating (ref{fem:vec:E}) with respect to $c_0$, note
that none of the inner product expressions depend on $c_0$, so we simply get

!bt
\begin{equation}
\frac{\partial E}{\partial c_0} = -2(\f,\psib_0) + 2c_0 (\psib_0,\psib_0)
\tp
label{fem:vec:dEdc0:v1}
\end{equation}
!et
Setting the above expression equal to zero and solving for $c_0$ gives

!bt
\begin{equation}
c_0 = \frac{(\f,\psib_0)}{(\psib_0,\psib_0)},
label{fem:vec:c0}
\end{equation}
!et
which in the present case, with $\psib_0=(a,b)$, results in

!bt
\begin{equation}
c_0 = \frac{3a + 5b}{a^2 + b^2}\tp  \end{equation}
!et

For later, it is worth mentioning that setting
the key equation (ref{fem:vec:dEdc0:v1}) to zero and ordering
the terms lead to


!bt
\[
(\f-c_0\psib_0,\psib_0) = 0,
\]
!et
or

!bt
\begin{equation}
(\e, \psib_0) = 0
\tp
label{fem:vec:dEdc0:Galerkin}
\end{equation}
!et
This implication of minimizing $E$ is an important result that we shall
make much use of.



idx{Galerkin method!vectors} idx{projection!vectors}

=== The projection method ===

We shall now show that minimizing $||\e||^2$ implies that $\e$ is
orthogonal to *any* vector $\v$ in the space $V$. This result is
visually quite clear from Figure ref{fem:approx:vec:plane:fig} (think of
other vectors along the line $(a,b)$: all of them will lead to
a larger distance between the approximation and $\f$).
Then we see mathematically that $\e$ is orthogonal to any vector $\v$
in the space $V$ and we may
express any $\v\in V$ as $\v=s\psib_0$ for any scalar parameter $s$
(recall that two vectors are orthogonal when their inner product vanishes).
Then we calculate the inner product
!bt
\begin{align*}
(\e, s\psib_0) &= (\f - c_0\psib_0, s\psib_0)\\
&= (\f,s\psib_0) - (c_0\psib_0, s\psib_0)\\
&= s(\f,\psib_0) - sc_0(\psib_0, \psib_0)\\
&= s(\f,\psib_0) - s\frac{(\f,\psib_0)}{(\psib_0,\psib_0)}(\psib_0,\psib_0)\\
&= s\left( (\f,\psib_0) - (\f,\psib_0)\right)\\
&=0\tp
\end{align*}
!et
Therefore, instead of minimizing the square of the norm, we could
demand that $\e$ is orthogonal to any vector in $V$, which in our present
simple case amounts to a single vector only.
This method is known as *projection*.
(The approach can also be referred to as a Galerkin method as
explained at the end of Section ref{fem:approx:vec:Np1dim}.)

Mathematically, the projection method is stated
by the equation

!bt
\begin{equation}
(\e, \v) = 0,\quad\forall\v\in V\tp
label{fem:vec:Galerkin1}
\end{equation}
!et
An arbitrary $\v\in V$ can be expressed as
$s\psib_0$, $s\in\Real$, and therefore
(ref{fem:vec:Galerkin1}) implies

!bt
\begin{equation*} (\e,s\psib_0) = s(\e, \psib_0) = 0,\end{equation*}
!et
which means that the error must be orthogonal to the basis vector in
the space $V$:

!bt
\begin{equation*}
(\e, \psib_0)=0\quad\hbox{or}\quad
(\f - c_0\psib_0, \psib_0)=0,
\end{equation*}
!et
which is what we found in (ref{fem:vec:dEdc0:Galerkin}) from
the least squares computations.




===== Approximation of general vectors =====
label{fem:approx:vec:Np1dim}
idx{approximation!of general vectors}


Let us generalize the vector approximation from the previous section
to vectors in spaces with arbitrary dimension. Given some vector $\f$,
we want to find the best approximation to this vector in
the space

!bt
\begin{equation*}
V = \hbox{span}\,\{\psib_0,\ldots,\psib_N\}
\tp
\end{equation*}
!et
We assume that the space has dimension $N+1$ and
that *basis vectors* $\psib_0,\ldots,\psib_N$ are
linearly independent so that none of them are redundant.
Any vector $\u\in V$ can then be written as a linear combination
of the basis vectors, i.e.,

!bt
\begin{equation*} \u = \sum_{j=0}^N c_j\psib_j,\end{equation*}
!et
where $c_j\in\Real$ are scalar coefficients to be determined.

=== The least squares method ===

Now we want to find $c_0,\ldots,c_N$, such that $\u$ is the best
approximation to $\f$ in the sense that the distance (error)
$\e = \f - \u$ is minimized. Again, we define
the squared distance as a function of the free parameters
$c_0,\ldots,c_N$,

!bt
\begin{align}
E(c_0,\ldots,c_N) &= (\e,\e) = (\f -\sum_jc_j\psib_j,\f -\sum_jc_j\psib_j)
\nonumber\\
&= (\f,\f) - 2\sum_{j=0}^N c_j(\f,\psib_j) +
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\psib_p,\psib_q)\tp
label{fem:vec:genE}
\end{align}
!et
Minimizing this $E$ with respect to the independent variables
$c_0,\ldots,c_N$ is obtained by requiring

!bt
\begin{equation*}
\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N
\tp
\end{equation*}
!et
The first term in (ref{fem:vec:genE}) is independent of $c_i$, so its
derivative vanishes.
The second term in (ref{fem:vec:genE}) is differentiated as follows:

!bt
\begin{equation}
\frac{\partial}{\partial c_i}
2\sum_{j=0}^N c_j(\f,\psib_j) = 2(\f,\psib_i),
\end{equation}
!et
since the expression to be differentiated is a sum and only one term,
$c_i(\f,\psib_i)$,
contains $c_i$ (this term is linear in $c_i$).
To understand this differentiation in detail,
write out the sum specifically for,
e.g, $N=3$ and $i=1$.

The last term in (ref{fem:vec:genE})
is more tedious to differentiate. It can be wise to write out the
double sum for $N=1$ and perform differentiation with respect to
$c_0$ and $c_1$ to see the structure of the expression. Thereafter,
one can generalize to an arbitrary $N$ and observe that

!bt
\begin{equation}
\frac{\partial}{\partial c_i}
c_pc_q =
\left\lbrace\begin{array}{ll}
0, & \hbox{ if } p\neq i\hbox{ and } q\neq i,\\
c_q, & \hbox{ if } p=i\hbox{ and } q\neq i,\\
c_p, & \hbox{ if } p\neq i\hbox{ and } q=i,\\
2c_i, & \hbox{ if } p=q= i\tp\\
\end{array}\right.
\end{equation}
!et
Then

!bt
\begin{equation*} \frac{\partial}{\partial c_i}
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\psib_p,\psib_q)
= \sum_{p=0, p\neq i}^N c_p(\psib_p,\psib_i)
+ \sum_{q=0, q\neq i}^N c_q(\psib_i,\psib_q)
+2c_i(\psib_i,\psib_i)\tp  \end{equation*}
!et
Since each of the two sums is missing the term $c_i(\psib_i,\psib_i)$,
we may split the very last term in two, to get exactly that ``missing''
term for each sum. This idea allows us to write

!bt
\begin{equation}
\frac{\partial}{\partial c_i}
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\psib_p,\psib_q)
= 2\sum_{j=0}^N c_i(\psib_j,\psib_i)\tp
\end{equation}
!et
It then follows that setting

!bt
\begin{equation*}
\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N,\end{equation*}
!et
implies

!bt
\[ - 2(\f,\psib_i) + 2\sum_{j=0}^N c_i(\psib_j,\psib_i) = 0,\quad i=0,\ldots,N\tp\]
!et
Moving the first term to the right-hand side shows that the equation is
actually a *linear system* for the unknown parameters $c_0,\ldots,c_N$:

!bt
\begin{equation}
\sum_{j=0}^N A_{i,j} c_j = b_i, \quad i=0,\ldots,N,
label{fem:approx:vec:Np1dim:eqsys}
\end{equation}
!et
where

!bt
\begin{align}
A_{i,j} &= (\psib_i,\psib_j),\\
b_i &= (\psib_i, \f)\tp  \end{align}
!et
We have changed the order of the two vectors in the inner
product according to (ref{fem:vec:rule:symmetry}):


!bt
\[ A_{i,j} = (\psib_j,\psib_i) = (\psib_i,\psib_j),\]
!et
simply because the sequence $i$-$j$ looks more aesthetic.

=== The Galerkin or projection method ===

In analogy with the ``one-dimensional'' example in
Section ref{fem:approx:vec:plane}, it holds also here in the general
case that minimizing the distance
(error) $\e$ is equivalent to demanding that $\e$ is orthogonal to
all $\v\in V$:
idx{Galerkin method!vectors} idx{projection!vectors}

!bt
\begin{equation}
(\e,\v)=0,\quad \forall\v\in V\tp
label{fem:approx:vec:Np1dim:Galerkin}
\end{equation}
!et
Since any $\v\in V$ can be written as $\v =\sum_{i=0}^N c_i\psib_i$,
the statement (ref{fem:approx:vec:Np1dim:Galerkin}) is equivalent to
saying that

!bt
\begin{equation*} (\e, \sum_{i=0}^N c_i\psib_i) = 0,\end{equation*}
!et
for any choice of coefficients $c_0,\ldots,c_N$.
The latter equation can be rewritten as

!bt
\begin{equation*} \sum_{i=0}^N c_i (\e,\psib_i) =0\tp  \end{equation*}
!et
If this is to hold for arbitrary values of $c_0,\ldots,c_N$,
we must require that each term in the sum vanishes, which means that

!bt
\begin{equation}
(\e,\psib_i)=0,\quad i=0,\ldots,N\tp
label{fem:approx:vec:Np1dim:Galerkin0}
\end{equation}
!et
These $N+1$ equations result in the same linear system as
(ref{fem:approx:vec:Np1dim:eqsys}):

!bt
\[ (\f - \sum_{j=0}^N c_j\psib_j, \psib_i) = (\f, \psib_i) - \sum_{j=0}^N
(\psib_i,\psib_j)c_j = 0,\]
!et
and hence

!bt
\[ \sum_{j=0}^N (\psib_i,\psib_j)c_j = (\f, \psib_i),\quad i=0,\ldots, N
\tp
\]
!et
So, instead of differentiating the
$E(c_0,\ldots,c_N)$ function, we could simply use
(ref{fem:approx:vec:Np1dim:Galerkin}) as the principle for
determining $c_0,\ldots,c_N$, resulting in the $N+1$
equations (ref{fem:approx:vec:Np1dim:Galerkin0}).

The names *least squares method* or *least squares approximation*
are natural since the calculations consists of
minimizing $||\e||^2$, and $||\e||^2$ is a sum of squares
of differences between the components in $\f$ and $\u$.
We find $\u$ such that this sum of squares is minimized.

The principle (ref{fem:approx:vec:Np1dim:Galerkin}),
or the equivalent form (ref{fem:approx:vec:Np1dim:Galerkin0}),
is known as *projection*. Almost the same mathematical idea
was used by the Russian mathematician "Boris Galerkin":
"http://en.wikipedia.org/wiki/Boris_Galerkin" to solve
differential equations, resulting in what is widely known as
*Galerkin's method*.

!split
======= Approximation principles =======
label{fem:approx:global}
idx{approximation!of functions}

Let $V$ be a function space spanned by a set of *basis functions*
$\baspsi_0,\ldots,\baspsi_N$,

!bt
\begin{equation*} V = \hbox{span}\,\{\baspsi_0,\ldots,\baspsi_N\},\end{equation*}
!et
such that any function $u\in V$ can be written as a linear
combination of the basis functions:

!bt
\begin{equation}
label{fem:approx:ufem}
u = \sum_{j\in\If} c_j\baspsi_j\tp
\end{equation}
!et

That is, we consider functions as vectors in a vector space -- a
so-called function space -- and we have a finite set of basis
functions that span the space just as basis vectors or unit vectors
span a vector space.

The index set $\If$ is defined as $\If =\{0,\ldots,N\}$ and is
from now on used
both for compact notation and for flexibility in the numbering of
elements in sequences.

For now, in this introduction, we shall look at functions of a
single variable $x$:
$u=u(x)$, $\baspsi_j=\baspsi_j(x)$, $j\in\If$. Later, we will almost
trivially extend the mathematical details
to functions of two- or three-dimensional physical spaces.
The approximation (ref{fem:approx:ufem}) is typically used
to discretize a problem in space. Other methods, most notably
finite differences, are common for time discretization, although the
form (ref{fem:approx:ufem}) can be used in time as well.

===== The least squares method =====
label{fem:approx:LS}

Given a function $f(x)$, how can we determine its best approximation
$u(x)\in V$? A natural starting point is to apply the same reasoning
as we did for vectors in Section ref{fem:approx:vec:Np1dim}. That is,
we minimize the distance between $u$ and $f$. However, this requires
a norm for measuring distances, and a norm is most conveniently
defined through an
inner product. Viewing a function as a vector of infinitely
many point values, one for each value of $x$, the inner product of
two arbitrary functions $f(x)$ and $g(x)$ could
intuitively be defined as the usual summation of
pairwise ``components'' (values), with summation replaced by integration:

!bt
\begin{equation*}
(f,g) = \int f(x)g(x)\, \dx
\tp
\end{equation*}
!et
To fix the integration domain, we let $f(x)$ and $\baspsi_i(x)$
be defined for a domain $\Omega\subset\Real$.
The inner product of two functions $f(x)$ and $g(x)$ is then

!bt
\begin{equation}
(f,g) = \int_\Omega f(x)g(x)\, \dx
label{fem:approx:LS:innerprod}
\tp
\end{equation}
!et

The distance between $f$ and any function $u\in V$ is simply
$f-u$, and the squared norm of this distance is

!bt
\begin{equation}
E = (f(x)-\sum_{j\in\If} c_j\baspsi_j(x), f(x)-\sum_{j\in\If} c_j\baspsi_j(x))\tp
label{fem:approx:LS:E}
\end{equation}
!et
Note the analogy with (ref{fem:vec:genE}): the given function
$f$ plays the role of the given vector $\f$, and the basis function
$\baspsi_i$ plays the role of the basis vector $\psib_i$.
We can rewrite (ref{fem:approx:LS:E}),
through similar steps as used for the result
(ref{fem:vec:genE}), leading to

!bt
\begin{equation}
E(c_i, \ldots, c_N) = (f,f) -2\sum_{j\in\If} c_j(f,\baspsi_i)
+ \sum_{p\in\If}\sum_{q\in\If} c_pc_q(\baspsi_p,\baspsi_q)\tp  \end{equation}
!et
Minimizing this function of $N+1$ scalar variables
$\sequencei{c}$, requires differentiation
with respect to $c_i$, for all $i\in\If$. The resulting
equations are very similar to those we had in the vector case,
and we hence end up with a
linear system of the form (ref{fem:approx:vec:Np1dim:eqsys}), with
basically the same expressions:

!bt
\begin{align}
A_{i,j} &= (\baspsi_i,\baspsi_j),
label{fem:approx:Aij}\\
b_i &= (f,\baspsi_i)\tp
label{fem:approx:bi}
\end{align}
!et

The only difference from
(ref{fem:approx:vec:Np1dim:eqsys})
is that the inner product is defined in terms
of integration rather than summation.

===== The projection (or Galerkin) method =====

idx{Galerkin method!functions} idx{projection!functions}

As in Section ref{fem:approx:vec:Np1dim}, the minimization of $(e,e)$
is equivalent to

!bt
\begin{equation}
(e,v)=0,\quad\forall v\in V\tp
label{fem:approx:Galerkin}
\end{equation}
!et
This is known as a projection of a function $f$ onto the subspace $V$.
We may also call it a Galerkin method for approximating functions.
Using the same reasoning as
in
(ref{fem:approx:vec:Np1dim:Galerkin})-(ref{fem:approx:vec:Np1dim:Galerkin0}),
it follows that (ref{fem:approx:Galerkin}) is equivalent to

!bt
\begin{equation}
(e,\baspsi_i)=0,\quad i\in\If\tp
label{fem:approx:Galerkin0}
\end{equation}
!et
Inserting $e=f-u$ in this equation and ordering terms, as in the
multi-dimensional vector case, we end up with a linear
system with a coefficient matrix (ref{fem:approx:Aij}) and
right-hand side vector (ref{fem:approx:bi}).

Whether we work with vectors in the plane, general vectors, or
functions in function spaces, the least squares principle and
the projection or Galerkin method are equivalent.

===== Example on linear approximation =====
label{fem:approx:global:linear}

Let us apply the theory in the previous section to a simple problem:
given a parabola $f(x)=10(x-1)^2-1$ for $x\in\Omega=[1,2]$, find
the best approximation $u(x)$ in the space of all linear functions:

!bt
\begin{equation*} V = \hbox{span}\,\{1, x\}\tp  \end{equation*}
!et
With our notation, $\baspsi_0(x)=1$, $\baspsi_1(x)=x$, and $N=1$.
We seek

!bt
\begin{equation*} u=c_0\baspsi_0(x) + c_1\baspsi_1(x) = c_0 + c_1x,\end{equation*}
!et
where
$c_0$ and $c_1$ are found by solving a $2\times 2$ the linear system.
The coefficient matrix has elements

!bt
\begin{align}
A_{0,0} &= (\baspsi_0,\baspsi_0) = \int_1^21\cdot 1\, \dx = 1,\\
A_{0,1} &= (\baspsi_0,\baspsi_1) = \int_1^2 1\cdot x\, \dx = 3/2,\\
A_{1,0} &= A_{0,1} = 3/2,\\
A_{1,1} &= (\baspsi_1,\baspsi_1) = \int_1^2 x\cdot x\,\dx = 7/3\tp  \end{align}
!et
The corresponding right-hand side is

!bt
\begin{align}
b_1 &= (f,\baspsi_0) = \int_1^2 (10(x-1)^2 - 1)\cdot 1 \, \dx = 7/3,\\
b_2 &= (f,\baspsi_1) = \int_1^2 (10(x-1)^2 - 1)\cdot x\, \dx = 13/3\tp  \end{align}
!et
Solving the linear system results in

!bt
\begin{equation}
c_0 = -38/3,\quad c_1 = 10,
\end{equation}
!et
and consequently

!bt
\begin{equation}
u(x) = 10x - \frac{38}{3}\tp  \end{equation}
!et
Figure ref{fem:approx:global:fig:parabola:linear} displays the
parabola and its best approximation in the space of all linear functions.

FIGURE: [fig/parabola_ls_linear, width=400]  Best approximation of a parabola by a straight line.  label{fem:approx:global:fig:parabola:linear}

===== Implementation of the least squares method =====
label{fem:approx:global:LS:code}

=== Symbolic integration ===

The linear system can be computed either symbolically or
numerically (a numerical integration rule is needed in the latter case).
Let us first compute the system and its solution symbolically, i.e.,
using classical ``pen and paper'' mathematics with symbols.
The Python package `sympy` can greatly help with this type of
mathematics, and will therefore be frequently used in this text.
Some basic familiarity with `sympy` is assumed, typically
`symbols`, `integrate`, `diff`, `expand`, and `simplify`. Much can be learned
by studying the many applications of `sympy` that will be presented.

Below is a function for symbolic computation of the linear system,
where $f(x)$ is given as a `sympy` expression `f` involving
the symbol `x`, `psi` is a list of expressions for $\sequencei{\baspsi}$,
and `Omega` is a 2-tuple/list holding the limits of the domain $\Omega$:

!bc pycod
import sympy as sym

def least_squares(f, psi, Omega):
    N = len(psi) - 1
    A = sym.zeros(N+1, N+1)
    b = sym.zeros(N+1, 1)
    x = sym.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            A[i,j] = sym.integrate(psi[i]*psi[j],
                                  (x, Omega[0], Omega[1]))
            A[j,i] = A[i,j]
        b[i,0] = sym.integrate(psi[i]*f, (x, Omega[0], Omega[1]))
    c = A.LUsolve(b)
    # Note: c is a sympy Matrix object, solution is in c[:,0]
    u = 0
    for i in range(len(psi)):
        u += c[i,0]*psi[i]
    return u, c
!ec


Observe that we exploit the symmetry of the coefficient matrix:
only the upper triangular part is computed. Symbolic integration, also in
`sympy`, is often time consuming, and (roughly) halving the
work has noticeable effect on the waiting time for the computations to
finish.

!bnotice
We remark that the symbols in `sympy` are created and stored in
a symbol factory that is indexed by the expression used in the construction
and that repeated constructions from the same expression will not create
new objects. The following code illustrates the behavior of the
symbol factory:

!bc pyshell
>>> from sympy import *
>>> x0 = Symbol("x")
>>> x1 = Symbol("x")
>>> id(x0) ==id(x1)
True
>>> a0 = 3.0
>>> a1 = 3.0
>>> id(a0) ==id(a1)
False
!ec
!enotice


=== Fall back on numerical integration ===

Obviously, `sympy` may fail to successfully integrate
$\int_\Omega\baspsi_i\baspsi_j\dx$, and
especially $\int_\Omega f\baspsi_i\dx$, symbolically.
Therefore, we should extend
the `least_squares` function such that it falls back on
numerical integration if the symbolic integration is unsuccessful.
In the latter case, the returned value from `sympy`'s
`integrate` function is an object of type `Integral`.
We can test on this type and utilize the `mpmath` module
to perform numerical integration of high precision.
Even when `sympy` manages to integrate symbolically, it can
take an undesirable long time. We therefore include an
argument `symbolic` that governs whether or not to try
symbolic integration. Here is a complete and
improved version of the previous function `least_squares`:

!bc pycod
def least_squares(f, psi, Omega, symbolic=True):
    N = len(psi) - 1
    A = sym.zeros(N+1, N+1)
    b = sym.zeros(N+1, 1)
    x = sym.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            integrand = psi[i]*psi[j]
            if symbolic:
                I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if not symbolic or isinstance(I, sym.Integral):
                # Could not integrate symbolically, use numerical int.
                integrand = sym.lambdify([x], integrand, 'mpmath')
                I = mpmath.quad(integrand, [Omega[0], Omega[1]])
            A[i,j] = A[j,i] = I

        integrand = psi[i]*f
        if symbolic:
            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
        if not symbolic or isinstance(I, sym.Integral):
            # Could not integrate symbolically, use numerical int.
            integrand = sym.lambdify([x], integrand, 'mpmath')
            I = mpmath.quad(integrand, [Omega[0], Omega[1]])
        b[i,0] = I
    if symbolic:
        c = A.LUsolve(b)  # symbolic solve
        # c is a sympy Matrix object, numbers are in c[i,0]
        c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
    else:
        c = mpmath.lu_solve(A, b)  # numerical solve
        c = [c[i,0] for i in range(c.rows)]
    u = sum(c[i]*psi[i] for i in range(len(psi)))
    return u, c
!ec
The function is found in the file `approx1D.py`.

=== Plotting the approximation ===

Comparing the given $f(x)$ and the approximate $u(x)$ visually is done
by the following function, which utilizes `sympy`'s `lambdify` tool to
convert a `sympy` expression to a Python function for numerical
computations:

!bc pycod
def comparison_plot(f, u, Omega, filename='tmp.pdf'):
    x = sym.Symbol('x')
    f = sym.lambdify([x], f, modules="numpy")
    u = sym.lambdify([x], u, modules="numpy")
    resolution = 401  # no of points in plot
    xcoor  = linspace(Omega[0], Omega[1], resolution)
    exact  = f(xcoor)
    approx = u(xcoor)
    plot(xcoor, approx)
    hold('on')
    plot(xcoor, exact)
    legend(['approximation', 'exact'])
    savefig(filename)
!ec
The `modules='numpy'` argument to `lambdify` is important
if there are mathematical functions, such as `sin` or `exp`
in the symbolic expressions in `f` or `u`, and these
mathematical functions are to be used with vector arguments, like
`xcoor` above.

Both the `least_squares` and `comparison_plot` functions are found in
the file "`approx1D.py`": "${fem_src}/approx1D.py".  The
`comparison_plot` function in this file is more advanced and flexible
than the simplistic version shown above.  The file `ex_approx1D.py`
applies the `approx1D` module to accomplish the forthcoming examples.

!bnotice
We remind the reader that the code examples can be found in a
tarball at
URL: "http://hplgit.github.io/fem-book/doc/web/".
The following command shows a useful way to search for code

!bc shcod
Terminal> find . -name '*.py' -exec grep least_squares {} \; -print
!ec
Here `'.'` specifies the directory for the search, `-name '*.py'` that
files with suffix `*.py` should be searched through while

!bc
-exec grep least_squares {} \; -print
!ec
means that all lines
containing the text `least_squares` should be printed to the screen.
!enotice



===== Perfect approximation =====
label{fem:approx:global:exact1}

Let us use the code above to recompute the problem from
Section ref{fem:approx:global:linear} where we want to approximate
a parabola. What happens if we add an element $x^2$ to the basis and test what
the best approximation is if $V$ is the space of all parabolic functions?
The answer is quickly found by running

!bc pycod
>>> from approx1D import *
>>> x = sym.Symbol('x')
>>> f = 10*(x-1)**2-1
>>> u, c = least_squares(f=f, psi=[1, x, x**2], Omega=[1, 2])
>>> print(u)
10*x**2 - 20*x + 9
>>> print(sym.expand(f))
10*x**2 - 20*x + 9
!ec

Now, what if we use $\baspsi_i(x)=x^i$ for $i=0,1,\ldots,N=40$?
The output from `least_squares` gives $c_i=0$ for $i>2$, which
means that the method finds the perfect approximation.

In fact, we have a general result that
if $f\in V$, the least squares and projection/Galerkin methods compute
the exact solution $u=f$.
The proof is straightforward: if $f\in V$, $f$ can be expanded in
terms of the basis functions, $f=\sum_{j\in\If} d_j\baspsi_j$, for
some coefficients $\sequencej{d}$,
and the right-hand side then has entries

!bt
\begin{equation*} b_i = (f,\baspsi_i) = \sum_{j\in\If} d_j(\baspsi_j, \baspsi_i) = \sum_{j\in\If} d_jA_{i,j}
\tp  \end{equation*}
!et
The linear system $\sum_jA_{i,j}c_j = b_i$, $i\in\If$, is then

!bt
\begin{equation*} \sum_{j\in\If} c_jA_{i,j} = \sum_{j\in\If}d_jA_{i,j},
\quad i\in\If,\end{equation*}
!et
which implies that $c_i=d_i$ for $i\in\If$.

===== The regression method =====
label{fem:approx:global:regression}

So far, the function to be approximated has been known in terms of
a formula $f(x)$. Very often in applications, no formula is known, but
the function value is known at a set of points. If we use $N+1$ basis
functions and know exactly $N+1$ function values, we can determine the
coefficients $c_i$ by *interpolation* as explained in Section
ref{fem:approx:global:interp}. The approximating function will then
equal the $f$ values at the points where the $f$ values are sampled.

However, one normally has $f$ sampled at a lot of points, here denoted
by $\xno{0},\xno{1},\ldots,\xno{m}$, and we assume $m\gg N$. What can
we do then to determine the coefficients?  The answer is to find a
least squares approximation.  The resulting method is called
*regression* and is well known from statistics when fitting a simple
(usually polynomial) function to a set of data points.

=== Overdetermined equation system ===

Intuitively, we would demand $u$ to equal $f$ at all the
data points $\xno{i}$, $i0,1,\ldots,m$,

!bt
\begin{equation}
u(\xno{i}) = \sum_{j\in\If} c_j \baspsi_j(\xno{i}) = f(\xno{i}),
\quad i=0,1,\ldots,m\tp
\end{equation}
!et
The fundamental problem here is that we have more equations than
unknowns since there are $N+1$ unknowns and $m+1>N+1$ equations.
Such a system of equations is called an *overdetermined system*.
We can write it in matrix form as

!bt
\begin{equation}
\sum_{j\in\If} A_{i,j}c_j = b_i,\quad i=0,1,\ldots,m,
\end{equation}
!et
with coefficient matrix and right-hand side vector given by

!bt
\begin{align}
A_{i,j} &= \baspsi_j(\xno{i}),
label{fem:approx:global:regression:Aij}\\
b_i &= f(\xno{i})\tp
label{fem:approx:global:regression:bi}
\end{align}
!et
Note that the matrix is a *rectangular* $(m+1)\times(N+1)$
matrix since $i=0,\ldots,m$ and $j=0,\ldots,N$.

=== The normal equations derived from a least squares principle ===

The least squares method is a common technique for solving
overdetermined equations systems. Let us write the overdetermined
system $\sum_{j\in\If} A_{i,j}c_j = b_i$ more compactly in matrix form
as $Ac=b$.  Since we have more equations than unknowns, it is (in
general) impossible to find a vector $c$ that fulfills $Ac=b$. The
best we can do is to make the residual $r=b-Ac$ as small as
possible. That is, we can find $c$ such that it minimizes the norm
Euclidean norm of $r$: $||r||$.  The algebra simplifies significantly
by minimizing $||r||^2$ instead.  This principle corresponds to a
least squares method.


idx{normal equations}
idx{$A^TA=A^Tb$ (normal equations)}

The $i$-th component of $r$ reads $r_i = b_i -\sum_jA_{i,j}c_j$,
so $||r||^2 = \sum_ir_i^2$.
Minimizing $||r||^2$ with respect to the unknowns $c_0,\ldots,c_N$
implies that

!bt
\begin{equation}
\frac{\partial}{\partial c_k}||r||^2=0,\quad k=0,\ldots,N,
label{fem:approx:global:regression:r2min}
\end{equation}
!et
which leads to

!bt
\[ \frac{\partial}{\partial c_k}\sum_i r_i^2 =
\sum_i 2r_i\frac{\partial r_i}{\partial c_k}
=\sum_i 2r_i \frac{\partial}{\partial c_k}(b_i -\sum_jA_{i,j}c_j)
= 2\sum_i r_i(-A_{i,k}) = 0\tp\]
!et
By inserting $r_i = b_i -\sum_jA_{i,j}c_j$ in the last expression we
get

!bt
\[ \sum_i\left(b_i -\sum_jA_{i,j}c_j\right)\left(-A_{i,k}\right)
= -\sum_i b_iA_{i,k} + \sum_j (\sum_i A_{i,j}A_{i,k})c_j = 0\tp\]
!et
Introducing the transpose of $A$, $A^T$, we know that $A^T_{i,j}=A_{j,i}$.
Therefore, the expression $\sum_i A_{i,j}A_{i,k}$ can be written
as $\sum_i A^T_{k,i}A_{i,j}$ and be recognized as the formula for the
matrix-matrix product $A^TA$. Also, $\sum_i b_i A_{i,k}$ can be written
$\sum_i A^T_{k,i}b_i$ and recognized as the matrix-vector product
$A^Tb$. These observations imply that (ref{fem:approx:global:regression:r2min})
is equivalent to the linear system

!bt
\begin{equation}
\sum_j (\sum_i A^T_{k,i}A_{i,j})c_j=\sum_j(A^TA)_{k,j}
c_j = \sum_i  A^T_{k,i}b_i=(A^Tb)_k,\quad k=0,\ldots,N,
label{fem:approx:global:regression:normal1}
\end{equation}
!et
or in matrix form,

!bt
\begin{equation}
 A^TA c = A^Tb\tp
label{fem:approx:global:regression:normal2}
\end{equation}
!et
The equation system (ref{fem:approx:global:regression:normal1}) or
(ref{fem:approx:global:regression:normal2}) are known as the
*normal equations*.
With $A$ as an $(m+1)\times (N+1)$ matrix, $A^TA$ becomes an $(N+1)\times (N+1)$
matrix, and $A^Tb$ becomes a vector of length $N+1$. Often, $m\gg N$,
so $A^TA$ is much smaller than $A$.

Many prefer to write the linear system
(ref{fem:approx:global:regression:normal1}) on the standard form
$\sum_j B_{i,j}c_j=d_i$, $i=0,\ldots,N$.  We can easily do so by
exchanging the $i$ and $k$ index ($i\leftrightarrow k$), $\sum_i
A^T_{k,i}A_{i,j} = \sum_k A^T_{i,k}A_{k,j}$, and setting $B_{i,j}=\sum_k
A^T_{i,k}A_{k,j}$. Similarly, we exchange $i$ and $k$ in the right-hand
side expression and get $\sum_k A^T_{i,k}b_k = d_i$.  Expressing
$B_{i,j}$ and $d_i$ in terms of the $\baspsi_i$ and $\xno{i}$, using
(ref{fem:approx:global:regression:Aij}) and
(ref{fem:approx:global:regression:bi}), we end up with the formulas

!bt
\begin{align}
B_{i,j} &= \sum_k A^T_{i,k}A_{k,j} = \sum_k A_{k,i}A_{k,j}
=\sum_{k=0}^m\baspsi_i(\xno{k})\baspsi_j(\xno{k}),
label{fem:approx:global:regression:Bij}\\
d_i &=\sum_k A^T_{i,k}b_k = \sum_k A_{k,i}b_k =\sum_{k=0}^m
\baspsi_i(\xno{k})f(\xno{k})
label{fem:approx:global:regression:di}
\end{align}
!et

=== Implementation ===

The following function defines the matrix entries $B_{i,j}$ according
to (ref{fem:approx:global:regression:Bij}) and the right-hand side
entries $d_i$ according
(ref{fem:approx:global:regression:di}). Thereafter, it solves the
linear system $\sum_jB_{i,j}c_j=d_i$.  The input data `f` and `psi`
hold $f(x)$ and $\baspsi_i$, $i=0,\ldots,N$, as symbolic expression, but
since $m$ is thought to be much larger than $N$, and there are loops
from $0$ to $m$, we use numerical computing to speed up the
computations.

!bc pycod
def regression(f, psi, points):
    N = len(psi) - 1
    m = len(points)
    # Use numpy arrays and numerical computing
    B = np.zeros((N+1, N+1))
    d = np.zeros(N+1)
    # Wrap psi and f in Python functions rather than expressions
    # so that we can evaluate psi at points[i]
    x = sym.Symbol('x')
    psi_sym = psi  # save symbolic expression
    psi = [sym.lambdify([x], psi[i]) for i in range(N+1)]
    f = sym.lambdify([x], f)
    for i in range(N+1):
        for j in range(N+1):
            B[i,j] = 0
            for k in range(m+1):
                B[i,j] += psi[i](points[k])*psi[j](points[k])
        d[i] = 0
        for k in range(m+1):
            d[i] += psi[i](points[k])*f(points[k])
    c = np.linalg.solve(B, d)
    u = sum(c[i]*psi_sym[i] for i in range(N+1))
    return u, c
!ec

=== Example ===

We repeat the computational example from Section
ref{fem:approx:global:interp}, but this time with many more
points. The parabola $f(x)=10(x-1)^2-1$ is to be approximated by a
linear function on $\Omega=[1,2]$. We divide $\Omega$ into $m+2$
intervals and use the inner $m+1$ points:

!bc pycod
import sympy as sym
x = sym.Symbol('x')
f = 10*(x-1)**2 - 1
psi = [1, x]
Omega = [1, 2]
m_values = [2-1, 8-1, 64-1]
# Create m+3 points and use the inner m+1 points
for m in m_values:
    points = np.linspace(Omega[0], Omega[1], m+3)[1:-1]
    u, c = regression(f, psi, points)
    comparison_plot(
        f, u, Omega,
        filename='parabola_by_regression_%d' % (m+1),
        points=points,
        points_legend='%d interpolation points' % (m+1),
        legend_loc='upper left')
!ec
Figure ref{fem:approx:global:linear:regression:fig1} shows results for
$m+1=2$ (left), $m+1=8$ (middle), and $m+1=64$ (right) data points.
The approximating function is not so sensitive to the number of
points as long as they cover a significant part of the domain (the first 2 point approximation puts too much weight on the center,
while the 8 point approximation cover almost the entire
domain and produces a good approximation which is barely improved with 64 points):

!bt
\begin{align*}
u(x) &= 10x - 13.2,\quad 2\hbox{ points}\\
u(x) &= 10x - 12.7,\quad 8\hbox{ points}\\
u(x) &= 10x - 12.7,\quad 64\hbox{ points}
\end{align*}
!et

FIGURE: [fig/parabola_by_regression, width=800]  Approximation of a parabola by a regression method with varying number of points.  label{fem:approx:global:linear:regression:fig1}


To explicitly make the link to classical regression in statistics, we
consider $f=10(x-1)^2 - 1 + \epsilon$, where $\epsilon$ is a random,
normally distributed variable. The goal in classical regression is
to find the straight line that best fits the data points (in a least
squares sense). The only difference from the previous setup, is that
the $f(\xno{i})$ values are based on a function formula, here $10(x-1)^2-1$,
*plus* normally distributed noise.
Figure ref{fem:approx:global:linear:regression:fig2} shows three sets of
data points, along with the original $f(x)$ function without noise, and
the straight line that is a least squares approximation to the data points.
# python ex_approx1D.py run_noisy_parabola_by_linear_regression

FIGURE: [fig/noisy_parabola_by_linear_regression, width=800]  Approximation of a parabola with noise by a straight line.  label{fem:approx:global:linear:regression:fig2}

We can fit a parabola instead of a straight line, as done in
Figure ref{fem:approx:global:linear:regression:fig3}. When $m$ becomes large,
the fitted parabola and the original parabola without noise become very close.
# python ex_approx1D.py run_noisy_parabola_by_quadratic_regression

FIGURE: [fig/noisy_parabola_by_quadratic_regression, width=800]  Approximation of a parabola with noise by a parabola.  label{fem:approx:global:linear:regression:fig3}

The regression method is not much used for approximating differential
equations or a given function, but is central in uncertainty quantification
methods such as polynomial chaos expansions.

!bnotice The residual: an indirect but computationally cheap measure of the error
When attempting to  solve
a system $A c = b$, we may question how far off a start vector or a current approximation $c_0$ is.
The error is clearly the difference between $c$ and $c_0$, $e=c-c_0$, but since we do
not know the true solution $c$ we are unable to assess the error. 
However, the vector $c_0$ is the solution of the an alternative problem $A c_0 = b_0$. 
If the input, i.e., the right-hand sides $b_0$ and $b$ are close to each other then 
we expect the output of a solution process $c$ and $c_0$ to be close to each other.   
Furthermore, while $b_0$ in principle is unknown, it is easily computable as $b_0 = A c_0$
and does not require inversion of $A$.  
The vector $b - b_0$ is the so-called residual $r$ defined by  
!bt
\[
r = b - b_0 = b - A c_0 =  A c - A c_0 .
\]
!et
Clearly, the error and the residual are related by

!bt
\[
A e = r .
\]
!et
While the computation of the error requires inversion of $A$,
which may be computationally expensive, the residual
is easily computable and do only require a matrix-vector product and vector additions. 
!enotice



!split
======= Orthogonal basis functions =======

Approximation of a function via orthogonal functions, especially sinusoidal
functions or orthogonal polynomials, is a very popular and successful approach.
The finite element method does not make use of orthogonal functions, but
functions that are ``almost orthogonal''.


===== Ill-conditioning =====
label{fem:approx:global:illconditioning}

For basis functions that are not orthogonal the condition number
of the matrix may create problems during the solution process
due to for example round-off errors as will be illustrated in the
following.   The computational example in Section ref{fem:approx:global:exact1}
applies the `least_squares` function which invokes symbolic
methods to calculate and solve the linear system. The correct
solution $c_0=9, c_1=-20, c_2=10, c_i=0$ for $i\geq 3$ is perfectly
recovered.

Suppose we
convert the matrix and right-hand side to floating-point arrays
and then solve the system using finite-precision arithmetics, which
is what one will (almost) always do in real life. This time we
get astonishing results! Up to about $N=7$ we get a solution that
is reasonably close to the exact one. Increasing $N$ shows that
seriously wrong coefficients are computed.
Below is a table showing the solution of the linear system arising from
approximating a parabola
by functions on the form $u(x)=c_0 + c_1x + c_2x^2 + \cdots + c_{10}x^{10}$.
Analytically, we know that $c_j=0$ for $j>2$, but numerically we may get
$c_j\neq 0$ for $j>2$.

|-----------------------------------------|
| exact | `sympy` | `numpy32` | `numpy64` |
|---r--------r---------r-----------r------|
|9      | 9.62    | 5.57      |  8.98     |
|-20    | -23.39  | -7.65     | -19.93    |
|10     | 17.74   | -4.50     |  9.96     |
|0      | -9.19   | 4.13      | -0.26     |
|0      | 5.25    | 2.99      |  0.72     |
|0      | 0.18    | -1.21     | -0.93     |
|0      | -2.48   | -0.41     |  0.73     |
|0      | 1.81    | -0.013    | -0.36     |
|0      | -0.66   | 0.08      |  0.11     |
|0      | 0.12    | 0.04      | -0.02     |
|0      | -0.001  | -0.02     |  0.002    |
|-----------------------------------------|

The exact value of $c_j$, $j=0,1,\ldots,10$, appears in the first
column while the other columns correspond to results obtained
by three different methods:

  * Column 2: The matrix and vector are converted to
    the data structure  `mpmath.fp.matrix` and the
    `mpmath.fp.lu_solve` function is used to solve the system.
  * Column 3: The matrix and vector are converted to
    `numpy` arrays with data type `numpy.float32`
    (single precision floating-point number) and solved by
    the `numpy.linalg.solve` function.
  * Column 4: As column 3, but the data type is
    `numpy.float64` (double
    precision floating-point number).

We see from the numbers in the table that
double precision performs much better than single precision.
Nevertheless, when plotting all these solutions the curves cannot be
visually distinguished (!). This means that the approximations look
perfect, despite the partially very wrong values of the coefficients.

Increasing $N$ to 12 makes the numerical solver in `numpy`
abort with the message: "matrix is numerically singular".
A matrix has to be non-singular to be invertible, which is a requirement
when solving a linear system. Already when the matrix is close to
singular, it is *ill-conditioned*, which here implies that
the numerical solution algorithms are sensitive to round-off
errors and may produce (very) inaccurate results.

The reason why the coefficient matrix is nearly singular and
ill-conditioned is that our basis functions $\baspsi_i(x)=x^i$ are
nearly linearly dependent for large $i$.  That is, $x^i$ and $x^{i+1}$
are very close for $i$ not very small. This phenomenon is
illustrated in Figure ref{fem:approx:global:fig:illconditioning}.
There are 15 lines in this figure, but only half of them are
visually distinguishable.
Almost linearly dependent basis functions give rise to an
ill-conditioned and almost singular matrix.  This fact can be
illustrated by computing the determinant, which is indeed very close
to zero (recall that a zero determinant implies a singular and
non-invertible matrix): $10^{-65}$ for $N=10$ and $10^{-92}$ for
$N=12$. Already for $N=28$ the numerical determinant computation
returns a plain zero.

FIGURE: [fig/ill_conditioning, width=600] The 15 first basis functions $x^i$, $i=0,\ldots,14$. label{fem:approx:global:fig:illconditioning}

On the other hand, the double precision `numpy` solver does run for
$N=100$, resulting in answers that are not significantly worse than
those in the table above, and large powers are
associated with small coefficients (e.g., $c_j < 10^{-2}$ for $10\leq
j\leq 20$ and $c<10^{-5}$ for $j > 20$). Even for $N=100$ the
approximation still lies on top of the exact curve in a plot (!).

The conclusion is that visual inspection of the quality of the approximation
may not uncover fundamental numerical problems with the computations.
However, numerical analysts have studied approximations and ill-conditioning
for decades, and it is well known that the basis $\{1,x,x^2,x^3,\ldots,\}$
is a bad basis. The best basis from a matrix conditioning point of view
is to have orthogonal functions such that $(\psi_i,\psi_j)=0$ for
$i\neq j$. There are many known sets of orthogonal polynomials and
other functions.
The functions used in the finite element methods are almost orthogonal,
and this property helps to avoid problems with solving matrix systems.
Almost orthogonal is helpful, but not enough when it comes to
partial differential equations, and ill-conditioning
of the coefficient matrix is a theme when solving large-scale matrix
systems arising from finite element discretizations.


===== Fourier series =====
label{fem:approx:global:Fourier}
idx{approximation!by sines}

A set of sine functions is widely used for approximating functions
(note that the sines are orthogonal with respect to the $L_2$ inner product as can be easily
verified using `sympy`).  Let us take

!bt
\begin{equation*}
V = \hbox{span}\,\{ \sin \pi x, \sin 2\pi x,\ldots,\sin (N+1)\pi x\}
\tp  \end{equation*}
!et
That is,

!bt
\begin{equation*} \baspsi_i(x) = \sin ((i+1)\pi x),\quad i\in\If\tp \end{equation*}
!et
An approximation to the parabola $f(x)=10(x-1)^2-1$ for $x\in\Omega=[1,2]$ from
Section ref{fem:approx:global:linear} can then be computed by the
`least_squares` function from Section ref{fem:approx:global:LS:code}:

!bc pycod
N = 3
import sympy as sym
x = sym.Symbol('x')
psi = [sym.sin(sym.pi*(i+1)*x) for i in range(N+1)]
f = 10*(x-1)**2 - 1
Omega = [0, 1]
u, c = least_squares(f, psi, Omega)
comparison_plot(f, u, Omega)
!ec
Figure ref{fem:approx:global:fig:parabola:sine1} (left) shows the oscillatory approximation
of $\sum_{j=0}^Nc_j\sin ((j+1)\pi x)$ when $N=3$.
Changing $N$ to 11 improves the approximation considerably, see
Figure ref{fem:approx:global:fig:parabola:sine1} (right).

FIGURE: [fig/parabola_ls_sines4_12, width=800]  Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions.  label{fem:approx:global:fig:parabola:sine1}

There is an error $f(0)-u(0)=9$ at $x=0$ in Figure ref{fem:approx:global:fig:parabola:sine1} regardless of how large $N$ is, because all $\baspsi_i(0)=0$ and hence
$u(0)=0$. We may help the approximation to be correct at $x=0$ by
seeking

!bt
\begin{equation}
u(x) = f(0) + \sum_{j\in\If} c_j\baspsi_j(x)
\tp
\end{equation}
!et
However, this adjustment introduces a new problem at $x=1$ since
we now get an error $f(1)-u(1)=f(1)-0=-1$ at this point. A more
clever adjustment is to replace the $f(0)$ term by a term that
is $f(0)$ at $x=0$ and $f(1)$ at $x=1$. A simple linear combination
$f(0)(1-x) + xf(1)$ does the job:
!bt
\begin{equation}
u(x) = f(0)(1-x) + xf(1) + \sum_{j\in\If} c_j\baspsi_j(x)
\tp
\end{equation}
!et
This adjustment of $u$ alters the linear system slightly. In the general
case, we set

!bt
\[ u(x) = B(x) +  \sum_{j\in\If} c_j\baspsi_j(x),\]
!et
and the linear system becomes

!bt
\[ \sum_{j\in\If}(\baspsi_i,\baspsi_j)c_j = (f-B,\baspsi_i),\quad i\in\If\tp\]
!et
The calculations can still utilize the `least_squares` or
`least_squares_orth` functions, but solve for $u-b$:

!bc pycod
f0 = 0;  f1 = -1
B = f0*(1-x) + x*f1
u_sum, c = least_squares_orth(f-b, psi, Omega)
u = B + u_sum
!ec

Figure ref{fem:approx:global:fig:parabola:sine2} shows the result
of the technique for
ensuring right boundary values. Even 3 sines can now adjust the
$f(0)(1-x) + xf(1)$ term such that $u$ approximates the parabola really
well, at least visually.

FIGURE: [fig/parabola_ls_sines4_12_wfterm, width=800]  Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions with a boundary term.  label{fem:approx:global:fig:parabola:sine2}


===== Orthogonal basis functions =====
label{fem:approx:global:orth}

The choice of sine functions $\baspsi_i(x)=\sin ((i+1)\pi x)$ has a great
computational advantage: on $\Omega=[0,1]$ these basis functions are
*orthogonal*, implying that $A_{i,j}=0$ if $i\neq j$. This
result is realized by trying

!bc pycod
integrate(sin(j*pi*x)*sin(k*pi*x), x, 0, 1)
!ec
in "WolframAlpha": "http://wolframalpha.com"
(avoid `i` in the integrand as this symbol means
the imaginary unit $\sqrt{-1}$).
Asking WolframAlpha also
about $\int_0^1\sin^2 (j\pi x) \dx$, we find that it equals
1/2.
With a diagonal matrix we can easily solve for the coefficients
by hand:

!bt
\begin{equation}
c_i = 2\int_0^1 f(x)\sin ((i+1)\pi x) \dx,\quad i\in\If,
\end{equation}
!et
which is nothing but the classical formula for the coefficients of
the Fourier sine series of $f(x)$ on $[0,1]$. In fact, when
$V$ contains the basic functions used in a Fourier series expansion,
the approximation method derived in Section ref{fem:approx:global}
results in the classical Fourier series for $f(x)$ (see Exercise ref{fem:approx:exer:Fourier}
for details).

With orthogonal basis functions we can make the
`least_squares` function (much) more efficient since we know that
the matrix is diagonal and only the diagonal elements need to be computed:

!bc pycod
def least_squares_orth(f, psi, Omega):
    N = len(psi) - 1
    A = [0]*(N+1)
    b = [0]*(N+1)
    x = sym.Symbol('x')
    for i in range(N+1):
        A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))
        b[i] = sym.integrate(psi[i]*f,  (x, Omega[0], Omega[1]))
    c = [b[i]/A[i] for i in range(len(b))]
    u = 0
    for i in range(len(psi)):
        u += c[i]*psi[i]
    return u, c
!ec

As mentioned in Section ref{fem:approx:global:LS:code}, symbolic integration
may fail or take a very long time. It is therefore natural to extend the
implementation above with a version where we can choose between symbolic
and numerical integration and fall back on the latter if the former
fails:

!bc pycod
def least_squares_orth(f, psi, Omega, symbolic=True):
    N = len(psi) - 1
    A = [0]*(N+1)       # plain list to hold symbolic expressions
    b = [0]*(N+1)
    x = sym.Symbol('x')
    for i in range(N+1):
        # Diagonal matrix term
        A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))

        # Right-hand side term
        integrand = psi[i]*f
        if symbolic:
            I = sym.integrate(integrand,  (x, Omega[0], Omega[1]))
        if not symbolic or isinstance(I, sym.Integral):
            print('numerical integration of', integrand)
            integrand = sym.lambdify([x], integrand, 'mpmath')
            I = mpmath.quad(integrand, [Omega[0], Omega[1]])
        b[i] = I
    c = [b[i]/A[i] for i in range(len(b))]
    u = 0
    u = sum(c[i]*psi[i] for i in range(len(psi)))
    return u, c
!ec
This function is found in the file `approx1D.py`. Observe that
we here assume that
$\int_\Omega\basphi_i^2\dx$ can always be symbolically computed,
which is not an unreasonable assumption
when the basis functions are orthogonal, but there is no guarantee,
so an improved version of the function above would implement
numerical integration also for the `A[i,i]` term.

===== Numerical computations =====

Sometimes the basis functions $\baspsi_i$ and/or the function $f$
have a nature that makes symbolic integration CPU-time
consuming or impossible.
Even though we implemented a fallback on numerical integration
of $\int f\basphi_i \dx$, considerable time might still be required
by `sympy` just by *attempting* to integrate symbolically.
Therefore, it will be handy to have function for fast
*numerical integration and numerical solution
of the linear system*. Below is such a method. It requires
Python functions `f(x)` and `psi(x,i)` for $f(x)$ and $\baspsi_i(x)$
as input. The output is a mesh function
with values `u` on the mesh with points in the array `x`.
Three numerical integration methods are offered:
`scipy.integrate.quad` (precision set to $10^{-8}$),
`mpmath.quad` (about machine precision), and a Trapezoidal
rule based on the points in `x` (unknown accuracy, but
increasing with the number of mesh points in `x`).

!bc pycod
def least_squares_numerical(f, psi, N, x,
                            integration_method='scipy',
                            orthogonal_basis=False):
    import scipy.integrate
    A = np.zeros((N+1, N+1))
    b = np.zeros(N+1)
    Omega = [x[0], x[-1]]
    dx = x[1] - x[0]       # assume uniform partition

    for i in range(N+1):
        j_limit = i+1 if orthogonal_basis else N+1
        for j in range(i, j_limit):
            print('(%d,%d)' % (i, j))
            if integration_method == 'scipy':
                A_ij = scipy.integrate.quad(
                    lambda x: psi(x,i)*psi(x,j),
                    Omega[0], Omega[1], epsabs=1E-9, epsrel=1E-9)[0]
            elif integration_method == 'sympy':
                A_ij = mpmath.quad(
                    lambda x: psi(x,i)*psi(x,j),
                    [Omega[0], Omega[1]])
            else:
                values = psi(x,i)*psi(x,j)
                A_ij = trapezoidal(values, dx)
            A[i,j] = A[j,i] = A_ij

        if integration_method == 'scipy':
            b_i = scipy.integrate.quad(
                lambda x: f(x)*psi(x,i), Omega[0], Omega[1],
                epsabs=1E-9, epsrel=1E-9)[0]
        elif integration_method == 'sympy':
            b_i = mpmath.quad(
                lambda x: f(x)*psi(x,i), [Omega[0], Omega[1]])
        else:
            values = f(x)*psi(x,i)
            b_i = trapezoidal(values, dx)
        b[i] = b_i

    c = b/np.diag(A) if orthogonal_basis else np.linalg.solve(A, b)
    u = sum(c[i]*psi(x, i) for i in range(N+1))
    return u, c

def trapezoidal(values, dx):
    """Integrate values by the Trapezoidal rule (mesh size dx)."""
    return dx*(np.sum(values) - 0.5*values[0] - 0.5*values[-1])
!ec
Here is an example on calling the function:

!bc pycod
from numpy import linspace, tanh, pi

def psi(x, i):
    return sin((i+1)*x)

x = linspace(0, 2*pi, 501)
N = 20
u, c = least_squares_numerical(lambda x: tanh(x-pi), psi, N, x,
                               orthogonal_basis=True)
!ec

__Remark.__
The `scipy.integrate.quad` integrator is usually much faster than
`mpmath.quad`.

!split
======= Interpolation =======

===== The interpolation (or collocation) principle =====
label{fem:approx:global:interp}

idx{collocation method (approximation)}
idx{approximation!collocation}
idx{interpolation method (approximation)}
idx{approximation!interpolation}

The principle of minimizing the distance between $u$ and $f$ is
an intuitive way of computing a best approximation $u\in V$ to $f$.
However, there are other approaches as well.
One is to demand that $u(\xno{i}) = f(\xno{i})$ at some selected points
$\xno{i}$, $i\in\If$:

!bt
\begin{equation}
u(\xno{i}) = \sum_{j\in\If} c_j \baspsi_j(\xno{i}) = f(\xno{i}),
\quad i\in\If\tp \end{equation}
!et
We recognize that the equation $\sum_j c_j \baspsi_j(\xno{i}) = f(\xno{i})$
is actually a linear system with $N+1$ unknown coefficients $\sequencej{c}$:

!bt
\begin{equation}
\sum_{j\in\If} A_{i,j}c_j = b_i,\quad i\in\If,
\end{equation}
!et
with coefficient matrix and right-hand side vector given by

!bt
\begin{align}
A_{i,j} &= \baspsi_j(\xno{i}),\\
b_i &= f(\xno{i})\tp  \end{align}
!et
This time the coefficient matrix is not symmetric because
$\baspsi_j(\xno{i})\neq \baspsi_i(\xno{j})$ in general.
The method is often referred to as an *interpolation method*
since some point values of $f$ are given ($f(\xno{i})$) and we
fit a continuous function $u$ that goes through the $f(\xno{i})$ points.
In this case the $\xno{i}$ points are called *interpolation points*.
When the same approach is used to approximate differential equations,
one usually applies the name *collocation method* and
$\xno{i}$ are known as *collocation points*.

Given $f$  as a `sympy` symbolic expression `f`, $\sequencei{\baspsi}$
as a list `psi`, and a set of points $\sequencei{x}$  as a list or array
`points`, the following Python function sets up and solves the matrix system
for the coefficients $\sequencei{c}$:

!bc pycod
def interpolation(f, psi, points):
    N = len(psi) - 1
    A = sym.zeros(N+1, N+1)
    b = sym.zeros(N+1, 1)
    psi_sym = psi  # save symbolic expression
    x = sym.Symbol('x')
    psi = [sym.lambdify([x], psi[i], 'mpmath') for i in range(N+1)]
    f = sym.lambdify([x], f, 'mpmath')
    for i in range(N+1):
        for j in range(N+1):
            A[i,j] = psi[j](points[i])
        b[i,0] = f(points[i])
    c = A.LUsolve(b)
    # c is a sympy Matrix object, turn to list
    c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
    u = sym.simplify(sum(c[i]*psi_sym[i] for i in range(N+1)))
    return u, c
!ec
The `interpolation` function is a part of the `approx1D`
module.


We found it convenient in the above function to turn the expressions `f` and
`psi` into ordinary Python functions of `x`, which can be called with
`float` values in the list `points` when building the matrix and
the right-hand side. The alternative is to use the `subs` method
to substitute the `x` variable in an expression by an element from
the `points` list. The following session illustrates both approaches
in a simple setting:

!bc ipy
>>> import sympy as sym
>>> x = sym.Symbol('x')
>>> e = x**2              # symbolic expression involving x
>>> p = 0.5               # a value of x
>>> v = e.subs(x, p)      # evaluate e for x=p
>>> v
0.250000000000000
>>> type(v)
sympy.core.numbers.Float
>>> e = lambdify([x], e)  # make Python function of e
>>> type(e)
>>> function
>>> v = e(p)              # evaluate e(x) for x=p
>>> v
0.25
>>> type(v)
float
!ec

A nice feature of the interpolation or collocation method is that it
avoids computing integrals. However, one has to decide on the location
of the $\xno{i}$ points.  A simple, yet common choice, is to
distribute them uniformly throughout the unit interval.

=== Example ===

Let us illustrate the interpolation method by approximating
our parabola $f(x)=10(x-1)^2-1$ by a linear function on $\Omega=[1,2]$,
using two collocation points $x_0=1+1/3$ and $x_1=1+2/3$:

!bc pycod
import sympy as sym
x = sym.Symbol('x')
f = 10*(x-1)**2 - 1
psi = [1, x]
Omega = [1, 2]
points = [1 + sym.Rational(1,3), 1 + sym.Rational(2,3)]
u, c = interpolation(f, psi, points)
comparison_plot(f, u, Omega)
!ec
The resulting linear system becomes

!bt
\begin{equation*}
\left(\begin{array}{ll}
1 & 4/3\\
1 & 5/3\\
\end{array}\right)
\left(\begin{array}{l}
c_0\\
c_1\\
\end{array}\right)
=
\left(\begin{array}{l}
1/9\\
31/9\\
\end{array}\right)
\end{equation*}
!et
with solution $c_0=-119/9$ and $c_1=10$.
Figure ref{fem:approx:global:linear:interp:fig1} (left) shows the resulting
approximation $u=-119/9 + 10x$.
We can easily test other interpolation points, say $x_0=1$ and $x_1=2$.
This changes the line quite significantly, see
Figure ref{fem:approx:global:linear:interp:fig1} (right).

FIGURE: [fig/parabola_inter, width=800]  Approximation of a parabola by linear functions computed by two interpolation points: 4/3 and 5/3 (left) versus 1 and 2 (right).  label{fem:approx:global:linear:interp:fig1}


===== Lagrange polynomials =====
label{fem:approx:global:Lagrange}
idx{Lagrange (interpolating) polynomial}

In Section ref{fem:approx:global:Fourier} we explained the advantage
of having a diagonal matrix: formulas for the coefficients
$\sequencei{c}$ can then be derived by hand. For an interpolation (or
collocation) method a diagonal matrix implies that $\baspsi_j(\xno{i})
= 0$ if $i\neq j$. One set of basis functions $\baspsi_i(x)$ with this
property is the *Lagrange interpolating polynomials*, or just
*Lagrange polynomials*. (Although the functions are named after
Lagrange, they were first discovered by Waring in 1779, rediscovered
by Euler in 1783, and published by Lagrange in 1795.)  Lagrange
polynomials are key building blocks in the finite element method, so
familiarity with these polynomials will be required anyway.

A Lagrange polynomial can be written as

!bt
\begin{equation}
\baspsi_i(x) =
\prod_{j=0,j\neq i}^N
\frac{x-\xno{j}}{\xno{i}-\xno{j}}
= \frac{x-x_0}{\xno{i}-x_0}\cdots\frac{x-\xno{i-1}}{\xno{i}-\xno{i-1}}\frac{x-\xno{i+1}}{\xno{i}-\xno{i+1}}
\cdots\frac{x-x_N}{\xno{i}-x_N},
label{fem:approx:global:Lagrange:poly}
\end{equation}
!et
for $i\in\If$.
We see from (ref{fem:approx:global:Lagrange:poly}) that all the $\baspsi_i$
functions are polynomials of degree $N$ which have the property

idx{Kronecker delta}

!bt
\begin{equation}
\baspsi_i(x_s) = \delta_{is},\quad \delta_{is} =
\left\lbrace\begin{array}{ll}
1, & i=s,\\
0, & i\neq s,
\end{array}\right.
label{fem:inter:prop}
\end{equation}
!et
when $x_s$ is an interpolation (collocation) point.
Here we have used the *Kronecker delta* symbol $\delta_{is}$.
This property implies that $A$ is a diagonal matrix, that is
that  $A_{i,j}=0$ for $i\neq j$ and
$A_{i,j}=1$ when $i=j$. The solution of the linear system is
then simply

!bt
\begin{equation}
c_i = f(\xno{i}),\quad i\in\If,
\end{equation}
!et
and

!bt
\begin{equation}
u(x) = \sum_{j\in\If} c_i \baspsi_i(x) = \sum_{j\in\If} f(\xno{i})\baspsi_i(x)\tp  \end{equation}
!et
We remark however that (ref{fem:inter:prop}) does not necessary imply that the matrix
obtained by the least squares or project methods are diagonal.


The following function computes the Lagrange interpolating polynomial
$\baspsi_i(x)$ on the unit interval (0,1), given the interpolation points $\xno{0},\ldots,\xno{N}$ in
the list or array `points`:

!bc pycod
def Lagrange_polynomial(x, i, points):
    p = 1
    for k in range(len(points)):
        if k != i:
            p *= (x - points[k])/(points[i] - points[k])
    return p
!ec
The next function computes a complete basis, $\baspsi_0,\ldots,\baspsi_N$, using equidistant points throughout
$\Omega$:

!bc pycod
def Lagrange_polynomials_01(x, N):
    if isinstance(x, sym.Symbol):
        h = sym.Rational(1, N-1)
    else:
        h = 1.0/(N-1)
    points = [i*h for i in range(N)]
    psi = [Lagrange_polynomial(x, i, points) for i in range(N)]
    return psi, points
!ec
When `x` is an `sym.Symbol` object, we let the spacing between the
interpolation points, `h`, be a `sympy` rational number, so that we
get nice end results in the formulas for $\baspsi_i$.  The other case,
when `x` is a plain Python `float`, signifies numerical computing, and
then we let `h` be a floating-point number.  Observe that the
`Lagrange_polynomial` function works equally well in the symbolic and
numerical case - just think of `x` being an `sym.Symbol` object or a
Python `float`.  A little interactive session illustrates the
difference between symbolic and numerical computing of the basis
functions and points:

!bc ipy
>>> import sympy as sym
>>> x = sym.Symbol('x')
>>> psi, points = Lagrange_polynomials_01(x, N=2)
>>> points
[0, 1/2, 1]
>>> psi
[(1 - x)*(1 - 2*x), 2*x*(2 - 2*x), -x*(1 - 2*x)]

>>> x = 0.5  # numerical computing
>>> psi, points = Lagrange_polynomials_01(x, N=2)
>>> points
[0.0, 0.5, 1.0]
>>> psi
[-0.0, 1.0, 0.0]
!ec

That is, when used symbolically, the `Lagrange_polynomials_01`
function returns the symbolic expression for the Lagrange functions
while when `x` is a numerical valued the function returns the value of
the basis function evaluate in `x`.  In the present example only the
second basis function should be 1 in the mid-point while the others
are zero according to (ref{fem:inter:prop}).


=== Approximation of a polynomial ===

The Galerkin or least squares methods lead to an exact approximation
if $f$ lies in the space spanned by the basis functions. It could be
of interest to see how the interpolation method with Lagrange
polynomials as basis is able to approximate a polynomial, e.g., a
parabola. Running

!bc pycod
for N in 2, 4, 5, 6, 8, 10, 12:
    f = x**2
    psi, points = Lagrange_polynomials_01(x, N)
    u = interpolation(f, psi, points)
!ec
shows the result that up to `N=4` we achieve an exact approximation,
and then round-off errors start to grow, such that
`N=15` leads to a 15-degree polynomial for $u$ where
the coefficients in front of $x^r$ for $r>2$ are
of size $10^{-5}$ and smaller. As the matrix is ill-conditioned
and we use floating-point arithmetic, we do not obtain the exact
solution. Still, we get  a solution that is visually identical to the
exact solution. The reason is that the ill-conditioning causes
the system to have many solutions very close to the true solution.
While we are lucky for `N=15` and obtain a solution that is
visually identical to the true solution, ill-conditioning may also
result in completely wrong results. As we continue with higher values,  `N=20` reveals that the
procedure is starting to fall apart as the approximate solution is around 0.9 at $x=1.0$,
where it should have
been $1.0$. At `N=30` the approximate solution is around $5\cdot10^8 $ at $x=1$.


=== Successful example ===

Trying out the Lagrange polynomial basis for approximating
$f(x)=\sin 2\pi x$ on $\Omega =[0,1]$ with the least squares
and the interpolation techniques can be done by

!bc pycod
x = sym.Symbol('x')
f = sym.sin(2*sym.pi*x)
psi, points = Lagrange_polynomials_01(x, N)
Omega=[0, 1]
u, c = least_squares(f, psi, Omega)
comparison_plot(f, u, Omega)
u, c = interpolation(f, psi, points)
comparison_plot(f, u, Omega)
!ec
Figure ref{fem:approx:global:Lagrange:fig:sine:ls:colloc} shows the results.
There is a difference between the least squares and the interpolation
technique but the difference decreases rapidly with  Increasing $N$.

FIGURE: [fig/Lagrange_ls_interp_sin_4, width=800]  Approximation via least squares (left) and interpolation (right) of a sine function by Lagrange interpolating polynomials of degree 3. label{fem:approx:global:Lagrange:fig:sine:ls:colloc}

idx{Runge's phenomenon}

=== Less successful example ===

The next example concerns interpolating $f(x)=|1-2x|$ on $\Omega
=[0,1]$ using Lagrange polynomials. Figure
ref{fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14} shows a peculiar
effect: the approximation starts to oscillate more and more as $N$
grows. This numerical artifact is not surprising when looking at the
individual Lagrange polynomials. Figure
ref{fem:approx:global:Lagrange:fig:abs:Lag:unif:osc} shows two such
polynomials, $\psi_2(x)$ and $\psi_7(x)$, both of degree 11 and
computed from uniformly spaced points $\xno{i}=i/11$,
$i=0,\ldots,11$, marked with circles.  We clearly see the property of
Lagrange polynomials: $\psi_2(\xno{i})=0$ and $\psi_7(\xno{i})=0$ for
all $i$, except $\psi_2(\xno{2})=1$ and $\psi_7(\xno{7})=1$.  The most
striking feature, however, is the dominating oscillation near the
boundary where $\psi_2>5$ and $\psi_7=-10$ in some points. The reason is easy to understand: since we force the
functions to be zero at so many points, a polynomial of high degree is
forced to oscillate between the points.  The phenomenon is named
*Runge's phenomenon* and you can read a more detailed explanation on
"Wikipedia": "http://en.wikipedia.org/wiki/Runge%27s_phenomenon".

idx{Chebyshev nodes}

=== Remedy for strong oscillations ===

The oscillations can be reduced by a more clever choice of
interpolation points, called the *Chebyshev nodes*:

!bt
\begin{equation}
\xno{i} = \half (a+b) + \half(b-a)\cos\left( \frac{2i+1}{2(N+1)}pi\right),\quad i=0\ldots,N,
\end{equation}
!et
on the interval $\Omega = [a,b]$.
Here is a flexible version of the `Lagrange_polynomials_01` function above,
valid for any interval $\Omega =[a,b]$ and with the possibility to generate
both uniformly distributed points and Chebyshev nodes:

!bc pycod
def Lagrange_polynomials(x, N, Omega, point_distribution='uniform'):
    if point_distribution == 'uniform':
        if isinstance(x, sym.Symbol):
            h = sym.Rational(Omega[1] - Omega[0], N)
        else:
            h = (Omega[1] - Omega[0])/float(N)
        points = [Omega[0] + i*h for i in range(N+1)]
    elif point_distribution == 'Chebyshev':
        points = Chebyshev_nodes(Omega[0], Omega[1], N)
    psi = [Lagrange_polynomial(x, i, points) for i in range(N+1)]
    return psi, points

def Chebyshev_nodes(a, b, N):
    from math import cos, pi
    return [0.5*(a+b) + 0.5*(b-a)*cos(float(2*i+1)/(2*N+1))*pi) \
            for i in range(N+1)]
!ec
All the functions computing Lagrange polynomials listed
above are found in the module file `Lagrange.py`.

Figure ref{fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14} shows the
improvement of using Chebyshev nodes, compared with the equidistant
points in Figure
ref{fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14}.  The reason for
this improvement is that the corresponding Lagrange polynomials have
much smaller oscillations, which can be seen by comparing Figure
ref{fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc} (Chebyshev
points) with Figure
ref{fem:approx:global:Lagrange:fig:abs:Lag:unif:osc} (equidistant
points). Note the different scale on the vertical axes in the two
figures and also that the Chebyshev points tend to cluster
more around the element boundaries.


Another cure for undesired oscillations of higher-degree interpolating
polynomials is to use lower-degree Lagrange polynomials on many small
patches of the domain. This is actually the idea pursued in the finite
element method. For instance, linear Lagrange polynomials on $[0,1/2]$
and $[1/2,1]$ would yield a perfect approximation to $f(x)=|1-2x|$ on
$\Omega = [0,1]$ since $f$ is piecewise linear.

FIGURE: [fig/Lagrange_interp_abs_8_15, width=800]  Interpolation of an absolute value function by Lagrange polynomials and uniformly distributed interpolation points: degree 7 (left) and 14 (right).  label{fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14}

FIGURE: [fig/Lagrange_basis_12, width=400]  Illustration of the oscillatory behavior of two Lagrange polynomials based on 12 uniformly spaced points (marked by circles).  label{fem:approx:global:Lagrange:fig:abs:Lag:unif:osc}


FIGURE: [fig/Lagrange_interp_abs_Cheb_8_15, width=800]  Interpolation of an absolute value function by Lagrange polynomials and Chebyshev nodes as interpolation points: degree 7 (left) and 14 (right).  label{fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14}

FIGURE: [fig/Lagrange_basis_Cheb_12, width=400]  Illustration of the less oscillatory behavior of two Lagrange polynomials based on 12 Chebyshev points (marked by circles).  label{fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc}

How does the least squares or projection methods work with Lagrange
polynomials?
We can just call the `least_squares` function, but
`sympy` has problems integrating the $f(x)=|1-2x|$
function times a polynomial, so we need to fall back on numerical
integration.

!bc pycod
def least_squares(f, psi, Omega):
    N = len(psi) - 1
    A = sym.zeros(N+1, N+1)
    b = sym.zeros(N+1, 1)
    x = sym.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            integrand = psi[i]*psi[j]
            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if isinstance(I, sym.Integral):
                # Could not integrate symbolically, fall back
                # on numerical integration with mpmath.quad
                integrand = sym.lambdify([x], integrand, 'mpmath')
                I = mpmath.quad(integrand, [Omega[0], Omega[1]])
            A[i,j] = A[j,i] = I
        integrand = psi[i]*f
        I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
        if isinstance(I, sym.Integral):
            integrand = sym.lambdify([x], integrand, 'mpmath')
            I = mpmath.quad(integrand, [Omega[0], Omega[1]])
        b[i,0] = I
    c = A.LUsolve(b)
    c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]
    u = sum(c[i]*psi[i] for i in range(len(psi)))
    return u, c
!ec


# Convergence of Lagrange polynomials.

# #ifdef 2DO
===== Non-symbolic implementation of the least squares method =====

Aim: drop sympy and work with `numpy.poly1d` instead, do numerical integration
(need a library with composite integration rules, maybe the one from
class Integrator is ok, or scipy.integrate.quad is also fine).
`poly1d` objects can be made from `scipy.special.orthogonal`!

Can take the Integrator hierarchy and extend: give a set of intervals
(uniform by default) and run the method on each interval.
A memoize function can be used to avoid two function evaluations
at the same point :-) One can then reuse a 1D lib of [-1,1]
rules by a simple linear transformation of the points to any
interval [x0,x1]. Should be in scitools? Should all the software
for this book be packages in scitools? Or independent?
Exercise: avoid memoize and derive special formulas
and modify the code accordingly.

# #endif



FIGURE: [fig/Lagrange_ls_abs_12, width=400]  Illustration of an approximation of the absolute value function using the least square method .  label{fem:approx:global:Lagrange:fig:abs:Lag:unif:ls}




===== Bernstein polynomials =====
label{fem:approx:global:Bernstein}
idx{Bernstein(interpolating) polynomial}

An alternative to the Taylor and Lagrange families of polynomials
are the Bernstein polynomials.
These polynomials are popular in visualization and we include a
presentation of them for completeness. Furthermore, as we
will demonstrate, the choice of basis functions may matter
in terms of accuracy and efficiency.
In fact, in finite element methods,
a main challenge, from a numerical analysis point of view,
is to determine appropriate basis functions for
a particular purpose or equation.

On the unit interval, the Bernstein
polynomials are defined in terms of powers of $x$ and $1-x$
(the barycentric coordinates of the unit interval) as
!bt
\begin{equation}
B_{i,n} = \binom{n}{i} x^i (1-x)^{n-i}, \quad i=0, \ldots, n .
label{bernstein:basis}
\end{equation}
!et

FIGURE: [fig/Bernstein_basis8] The nine functions of a Bernstein basis of order 8. label{Bernstein_basis_8}

FIGURE: [fig/Lagrange_basis8] The nine functions of a Lagrange basis of order 8. label{Lagrange_basis_8}



The Figure ref{Bernstein_basis_8} shows the  basis functions of a Bernstein basis of order 8.
This figure should be compared against Figure ref{Lagrange_basis_8}, which
shows the corresponding Lagrange basis of order 8.
The Lagrange basis is convenient because it is a nodal basis, that is; the basis functions are 1 in their nodal points and zero at all other nodal points as described by (ref{fem:inter:prop}).
However, looking at Figure ref{Lagrange_basis_8}
we also notice that the basis function are oscillatory and have absolute
values that are significantly larger than 1 between the nodal points.
Consider for instance the basis function represented by the purple color.
It is 1 in $x=0.5$ and 0 at all other nodal points
and hence this basis function represents the value at the mid-point.
However, this function also has strong
negative contributions close to the element boundaries where
it takes negative values less than $-2$.
For the Bernstein basis, all  functions are positive and
all functions output values in $[0,1]$. Therefore there is no oscillatory behavior.
The main disadvantage of the Bernstein basis is that the basis is not
a nodal basis. In fact, all functions contribute everywhere except $x=0$ and $x=1$.


Both Lagrange and Bernstein polynomials take larger values towards the element boundaries than in
the middle of the element, but the Bernstein polynomials always remain less or equal to 1.

We  remark that the Bernstein basis is easily extended to polygons in 2D and
3D in terms of the barycentric coordinates. For example, consider the reference triangle in
2D consisting of the faces $x=0$, $y=0$, and $x+y=1$. The barycentric coordinates
are $b_1(x,y)=x$, $b_2(x,y)$, and $b_3(x,y)=1-x-y$ and the Bernstein basis functions
of order $n$ is of the form

!bt
\[
B_{i,j,k} = \frac{n!}{i! j! k!} x^i y^j (1-x-y)^k, \quad \mbox{ for }  i+j+k = n \tp
\]
!et

!bnotice 
We have considered approximation with a sinusoidal basis and with  Lagrange or Bernstein polynomials, 
all of which are frequently used for scientific computing. The Lagrange polynomials (of various
order) are extensively used in finite element methods, while the Bernstein polynomials are more used
in computational geometry. However, we mention a few recent efforts in finite element computations that   
explore the combination of symbolic and numerical evaluation for finite element methods 
and have demonstrated that the Bernstein basis
enables fast computations through their explicit representation (of both the basis functions and their derivatives)
cite{alnaes2010efficiency,kirby2011fast}. The Lagrange and the Bernstein families are, however, but a few in the jungle of polynomials
spaces used for finite element computations and their efficiency and accuracy can very quite substantially.  
Furthermore, while a method may be efficient and accurate for one type of PDE it might not even converge for
another type of PDE.  The development and analysis of finite element methods for different purposes is currently an intense research field
and has been so for several decades.
Some structure in this vast jungle of methods can be found in cite{arnold2014periodic}.  
FEniCS has implemented a wide range of polynomial spaces cite{kirby2012common} and has a general
framework for implementing new elements cite{kirby2012constructing}. While finite element methods
explore different families of polynomials, the so-called spectral methods explore the use
of sinusoidal functions or polynomials with high order. From an abstract point of view, the different methods
can all be obtained simply by changing the basis for the trial and test functions. However, their
efficiency and accuracy may vary substantially, as we will also see in the following.    
     
!enotice 

======= Approximation properties and convergence rates =======

We will now compare the different approximation methods in terms of
accuracy and efficiency.  We consider four different series for
generating approximations: Taylor, Lagrange, sinusoidal, and
Bernstein. For all families we expect that the approximations improve
as we increase the number of basis functions in our
representations. We also expect that the computational complexity
increases. Let us therefore try to quantify the accuracy and
efficiency of the different methods in terms of the number of basis
functions $N$. In the present example we consider the least square
method.

Let us consider the approximation of a Gaussian bell function, i.e.,
that the exact solution is

!bt
\[
u_e = \exp(-(x-0.5)^2) - \exp(-0.5^2)
\]
!et
We remark that $u_e$ is zero in $x=0$ and $x=1$ and that
we have chosen the bell function because it cannot be expressed
as a finite sum of either polynomials or sines.
We may therefore study the behavior as $N\rightarrow\infty$.

To quantify the behavior of the error as well as the
complexity of the computations we  compute the approximation
with increasing number of basis functions and time
the computations by using `time.clock` (returning the CPU time so far
in the program). A code example goes as
follows:

!bc pycod
def convergence_rate_analysis(series_type, func):
    N_values = [2, 4, 8, 16]
    norms = []
    cpu_times = []
    for N in N_values:

        psi = series(series_type, N)
        t0 = time.clock()
        u, c = least_squares_non_verbose(
	       gauss_bell, psi, Omega, False)
        t1 = time.clock()

        error2 = sym.lambdify([x], (func - u)**2)
        L2_norm = scipy.integrate.quad(error2, Omega[0], Omega[1])
        L2_norm = scipy.sqrt(L2_norm)
        norms.append(L2_norm[0])
        cpu_times.append(t1-t0)

    return N_values, norms, cpu_times
!ec

We run the analysis as follows

!bc pycod
Omega = [0, 1]
x = sym.Symbol("x")
gaussian_bell = sym.exp(-(x-0.5)**2) - sym.exp(-0.5**2)
step = sym.Piecewise((1, 0.25 < x), (0, True)) - \
       sym.Piecewise((1, 0.75 < x), (0, True))
func = gaussian_bell

import pylab as plt
series_types = ["Taylor", "Sinusoidal", "Bernstein", "Lagrange"]
for series_type in series_types:
    N_values, norms, cpu_times = \
        convergence_rate_analysis(series_type, func)
    plt.loglog(N_values, norms)
plt.show()
!ec
and the different families of basis functions are: 
!bc pycod
def Lagrange_series(N): 
  psi = []
  h = 1.0/N
  points = [i*h for i in range(N+1)]
  for i in range(len(points)): 
    p = 1 
    for k in range(len(points)): 
      if k != i:
        p *= (x - points[k])/(points[i] - points[k])
    psi.append(p)
  return psi

def Bernstein_series(N): 
  psi = []
  for k in range(0,N+1): 
    psi_k = sym.binomial(N, k)*x**k*(1-x)**(N-k)  
    psi.append(psi_k)
  return psi

def Sinusoidal_series(N): 
  psi = []
  for k in range(1,N): 
    psi_k = sym.sin(sym.pi*k*x)
    psi.append(psi_k)
  return psi

def Taylor_series(N): 
  psi = []
  for k in range(1,N): 
    psi_k = x**k 
    psi.append(psi_k)
  return psi

def series(series_type, N): 
  if   series_type== "Taylor"     : return Taylor_series(N)
  elif series_type== "Sinusoidal" : return Sinusoidal_series(N)
  elif series_type== "Bernstein"  : return Bernstein_series(N)
  elif series_type== "Lagrange"   : return Lagrange_series(N)
  else: print("series type unknown ") 
!ec




Below we list the numerical error for different $N$  when approximating the Gaussian bell function.

|------------------------------------------------------------------|
|  N          |  2        |  4          |  8         |  16         |
|--c----------|--c--------|--c----------|--c---------|--c----------|
|  Taylor     |  9.83e-02   |  2.63e-03   |  7.83e-07  |  3.57e-10   |
|  sine       |  2.70e-03   |  6.10e-04   |  1.20e-04  |  2.17e-05   |
|  Bernstein  |  2.10e-03   |  4.45e-05   |  8.73e-09  |  4.49e-15   |
|  Lagrange   |  2.10e-03   |  4.45e-05   |  8.73e-09  |  2.45e-12   |
|------------------------------------------------------------------|

It is quite clear that the different methods have different
properties.  For example, the Lagrange basis for $N=16$ is 145 times
more accurate than the Taylor basis. However, Bernstein is actually
more than 500 times more accurate than the Lagrange basis! The
approximations obtained by sines are far behind the polynomial
approximations for $N>4$.

The corresponding CPU time of the required computations also vary quite a bit:

|------------------------------------------------------------------|
|  N          |  2        |  4          |  8         |  16         |
|--c----------|--c--------|--c----------|--c---------|--c----------|
|  Taylor     |  0.0123   |  0.0325    |  0.108   |  0.441   |
|  sine       |  0.0113   |  0.0383    |  0.229   |  1.107   |
|  Bernstein  |  0.0384   |  0.1100    |  0.3368  |  1.187   |
|  Lagrange   |  0.0807   |  0.3820    |  2.5233  |  26.52   |
|------------------------------------------------------------------|

Here, the timings are in seconds.  The Taylor basis is the most
efficient and is in fact more than 60 times faster than the Lagrange
basis for $N=16$ (with our naive implementation of basic formulas).

In order to get a more precise idea of how the approximation methods
behave as $N$ increases, we investigate two simple data models which
may be used in a regression analysis.  These two are the polynomial
and exponential model defined by

!bt
\begin{align}
label{sec:approx:pol:model}
E_{1}(N) &= \alpha_{1} N^{\beta_{1}}, \\
label{sec:approx:exp:model}
E_{2}(N) &= \alpha_{2} \exp(\beta_2 N)
\end{align}
!et
Taking the logarithm of (ref{sec:approx:pol:model}) we
obtain

!bt
\[
\log (E_1(N)) = \beta_1 \log(N) + log(\alpha_1)
\]
!et
Hence, letting $x=\log(N)$ be the independent variable and $y=\log
(E_1(N))$ the dependent one, we simply have the straight line $y = a x
+ b$ with $a=\beta_1$ and $b= log(\alpha_1)$.  Then, we may perform a
regression analysis as earlier with respect to the basis functions
$(1,x)$ and obtain an estimate of the order of convergence in terms of
$\beta_1$ . For the second model (ref{sec:approx:exp:model}), we take
the natural logarithm and obtain

!bt
\[
\ln (E_2(N)) = \beta_2 N + \ln(\alpha_2)
\]
!et
Again, regression analysis provides the means to estimate the convergence,
but here we let $x=N$ be the independent variable,
$y=\ln (E_2(N))$, $a=\beta_2$ and $b= \ln(\alpha_2)$.
To summarize, the polynomial model should have the data around a straight
line in a log-log plot, while the exponential model has its date around
a straight line in a log plot with the logarithmic scale on the $y$ axis.

Before we perform the regression analysis, a good rule is to inspect
the behavior visually in log and log-log plots. Figure
ref{fem:approx:bell:loglogfig} shows a log-log plot of the error with
respect to $N$ for the various methods. Clearly, the sinusoidal basis
seems to have a polynomial convergence rate as the log-log plot is a
linear line. The Bernstein, Lagrange, and Taylor methods appear to
have convergence that is faster than polynomial. It is then
interesting to consider a log plot and see if the behavior is
exponential. Figure ref{fem:approx:bell:semilogfig} is a log
plot. Here, the Bernstein approximation appears to be a linear line
which suggests that the convergence is exponential.


FIGURE: [fig/Bell_convergence_loglog] Convergence of least square approximation using basis function in terms of the Taylor, sinusoidal, Bernstein and Lagrange basis in a log-log plot.  label{fem:approx:bell:loglogfig}

FIGURE: [fig/Bell_convergence_semilogy] Convergence of least square approximation using basis function in terms of the Taylor, sinusoidal, Bernstein and Lagrange basis in a log plot.  label{fem:approx:bell:semilogfig}

The following program computes the order of convergence for
the sines using the polynomial model (ref{sec:approx:pol:model})
while the Bernstein approximation is estimates
in terms of model (ref{sec:approx:exp:model}). We avoid to
compute estimates for the Taylor and Lagrange approximations as neither
the log-log plot nor the log plot demonstrated linear behavior.

!bc pycod
N_values = [2, 4, 8, 16, 32]
Taylor     = [0.0983, 0.00263,  7.83e-07, 3.57e-10]
Sinusoidal = [0.0027, 0.00061,  0.00012,  2.17e-05]
Bernstein  = [0.0021, 4.45e-05, 8.73e-09, 4.49e-15]
Lagrange   = [0.0021, 4.45e-05, 8.73e-09, 2.45e-12]

x = sym.Symbol('x')
psi = [1, x]

u, c = regression_with_noise(log2(Sinusoidal), psi, log2(N_values))
print("estimated model for sine: %3.2e*N**(%3.2e)" % \
      (2**(c[0]), c[1]))

# Check the numbers estimated by the model by manual inspection
for N in N_values:
    print(2**c[0] * N**c[1])

u, c = regression_with_noise(log(Bernstein), psi, N_values)
print("estimated model for Bernstein: %3.2e*exp(%3.2e*N)" % \
      (exp(c[0]), c[1]))

# Check the numbers estimated by the model by manual inspection
for N in N_values:
    print(exp(c[0]) * exp(N * c[1]))
!ec

The program estimates the sinusoidal approximation convergences as
$1.4 10^{-2} N^{-2.3}$, which means that the convergence is slightly
above second order.  The Bernstein approximation on the other hand is
$8.01 10^{-2} \exp(-1.9 N)$. Considering now that
we have $N=100$ then we can estimate that the sinusoidal approximation 
would give us an error of $\approx 3.6 10^{-7}$ while the estimate for
the Bernstein polynomials amounts to $\approx 3.3 10^{-85}$ and is hence 
vastly superior.  
We remark here that floating point errors likely will be an issue, but libraries
with arbitrary precision is available in Python.  


The CPU time in this example here would be significantly faster if the
algorithms were implemented in a compiled language like C/C++ or
Fortran and we should not be careful in drawing conclusion about the
efficiency of the different methods based on this example
alone. However, for completeness we include a log-log plot in Figure
ref{fem:comp:bell:loglogfig} to illustrate the polynomial increase in
CPU time with respect to N.
It seems that the efficiency of both the Taylor and Bernstein approximations can be estimated to be of the order of $N^2$,
but the sinusoidal and Lagrange approximations seem to grow faster.  

FIGURE: [fig/Bell_computations_loglog] CPU timings of the approximation with the difference basis in a log-log plot.  label{fem:comp:bell:loglogfig}


The complete code can be found in
"`convergence_rate_local.py`": "${fem_src}/convergence_rate_local.py".

The code for the "`regression\_with\_noise`" is as follows: 

@@@CODE src/approx1D.py fromto: def regression_with_noise@    return  

!split
======= Approximation of functions in higher dimensions =======
label{fem:approx:2D}

All the concepts and algorithms developed for approximation of 1D functions
$f(x)$ can readily be extended to 2D functions $f(x,y)$ and 3D functions
$f(x,y,z)$. Basically, the extensions consist of defining basis functions
$\baspsi_i(x,y)$ or $\baspsi_i(x,y,z)$ over some domain $\Omega$, and
for the least squares and Galerkin methods, the integration is done over
$\Omega$.

As in 1D, the least squares and projection/Galerkin methods
lead to linear systems

!bt
\begin{align*}
\sum_{j\in\If} A_{i,j}c_j &= b_i,\quad i\in\If,\\
A_{i,j} &= (\baspsi_i,\baspsi_j),\\
b_i &= (f,\baspsi_i),
\end{align*}
!et
where the inner product of two functions $f(x,y)$ and $g(x,y)$ is defined
completely analogously to the 1D case (ref{fem:approx:LS:innerprod}):
!bt
\begin{equation}
(f,g) = \int_\Omega f(x,y)g(x,y) dx dy
\end{equation}
!et


===== 2D basis functions as tensor products of 1D functions =====
label{fem:approx:2D:global}


idx{tensor product}

One straightforward way to construct a basis in 2D is to combine 1D
basis functions. Say we have the 1D vector space

!bt
\begin{equation}
V_x = \mbox{span}\{ \hat\baspsi_0(x),\ldots,\hat\baspsi_{N_x}(x)\}
label{fem:approx:2D:Vx}
\tp
\end{equation}
!et
A similar space for a function's variation in $y$ can be defined,


!bt
\begin{equation}
V_y = \mbox{span}\{ \hat\baspsi_0(y),\ldots,\hat\baspsi_{N_y}(y)\}
label{fem:approx:2D:Vy}
\tp
\end{equation}
!et
We can then form 2D basis functions as *tensor products* of 1D basis functions.

!bnotice Tensor products
Given two vectors $a=(a_0,\ldots,a_M)$ and $b=(b_0,\ldots,b_N)$,
their *outer tensor product*, also called the *dyadic product*,
is $p=a\otimes b$, defined through

!bt
\[ p_{i,j}=a_ib_j,\quad i=0,\ldots,M,\ j=0,\ldots,N\tp\]
!et
In the tensor terminology,
$a$ and $b$ are first-order tensors (vectors with one index, also termed
rank-1 tensors), and then their outer
tensor product is a second-order tensor (matrix with two indices, also
termed rank-2 tensor). The
corresponding *inner tensor product* is the well-known scalar or dot
product of two vectors: $p=a\cdot b = \sum_{j=0}^N a_jb_j$. Now,
$p$ is a rank-0 tensor.


Tensors are typically represented by arrays in computer code.
In the above example, $a$ and $b$ are represented by
one-dimensional arrays of length
$M$ and $N$, respectively, while $p=a\otimes b$ must be represented
by a two-dimensional array of size $M\times N$.

"Tensor products": "http://en.wikipedia.org/wiki/Tensor_product" can
be used in a variety of contexts.
!enotice


# The following is from http://en.wikipedia.org/wiki/Tensor_product,
# Notation and examples
Given the vector spaces $V_x$ and $V_y$ as defined
in (ref{fem:approx:2D:Vx}) and (ref{fem:approx:2D:Vy}), the
tensor product space $V=V_x\otimes V_y$ has a basis formed
as the tensor product of the basis for $V_x$ and $V_y$.
That is, if $\left\{ \baspsi_i(x) \right\}_{i\in\Ix}$
and $\left\{ \baspsi_i(y) \right\}_{i\in \Iy}$ are basis for
$V_x$ and $V_y$, respectively, the elements in the basis for $V$ arise
from the tensor product:
$\left\{ \baspsi_i(x)\baspsi_j(y) \right\}_{i\in \Ix,j\in \Iy}$.
The index sets are $I_x=\{0,\ldots,N_x\}$ and $I_y=\{0,\ldots,N_y\}$.

The notation for a basis function in 2D can employ a double index as in

!bt
\[ \baspsi_{p,q}(x,y) = \hat\baspsi_p(x)\hat\baspsi_q(y),
\quad p\in\Ix,q\in\Iy\tp
\]
!et
The expansion for $u$ is then written as a double sum

!bt
\[ u = \sum_{p\in\Ix}\sum_{q\in\Iy} c_{p,q}\baspsi_{p,q}(x,y)\tp
\]
!et
Alternatively, we may employ a single index,

!bt
\[
\baspsi_i(x,y) = \hat\baspsi_p(x)\hat\baspsi_q(y),
\]
!et
and use the standard form for $u$,

!bt
\[ u = \sum_{j\in\If} c_j\baspsi_j(x,y)\tp\]
!et
The single index can be expressed in terms of the double index through
$i=p (N_y+1) + q$ or $i=q (N_x+1) + p$.

===== Example on polynomial basis in 2D =====

Let us again consider approximation with the least squares method, but now
with basis functions in 2D.
Suppose we choose $\hat\baspsi_p(x)=x^p$, and try an approximation with
$N_x=N_y=1$:

!bt
\[ \baspsi_{0,0}=1,\quad \baspsi_{1,0}=x, \quad \baspsi_{0,1}=y,
\quad \baspsi_{1,1}=xy
\tp
\]
!et
Using a mapping to one index like $i=q (N_x+1) + p$, we get

!bt
\[ \baspsi_0=1,\quad \baspsi_1=x, \quad \baspsi_2=y,\quad\baspsi_3 =xy
\tp
\]
!et

With the specific choice $f(x,y) = (1+x^2)(1+2y^2)$ on
$\Omega = [0,L_x]\times [0,L_y]$, we can perform actual calculations:

!bt
\begin{align*}
A_{0,0} &= (\baspsi_0,\baspsi_0) = \int_0^{L_y}\int_{0}^{L_x} 1 dxdy = L_{x} L_{y}, \\
A_{0,1} &= (\baspsi_0,\baspsi_1) = \int_0^{L_y}\int_{0}^{L_x} x dxdy = \frac{L_{x}^{2} L_{y}}{2}, \\
A_{0,2} &= (\baspsi_0,\baspsi_2) = \int_0^{L_y}\int_{0}^{L_x} y dxdy = \frac{L_{x} L_{y}^{2}}{2}, \\
A_{0,3} &= (\baspsi_0,\baspsi_3) = \int_0^{L_y}\int_{0}^{L_x} x y dxdy = \frac{L_{x}^{2} L_{y}^{2}}{4}, \\
A_{1,0} &= (\baspsi_1,\baspsi_0) = \int_0^{L_y}\int_{0}^{L_x} x dxdy = \frac{L_{x}^{2} L_{y}}{2}, \\
A_{1,1} &= (\baspsi_1,\baspsi_1) = \int_0^{L_y}\int_{0}^{L_x} x^{2} dxdy = \frac{L_{x}^{3} L_{y}}{3}, \\
A_{1,2} &= (\baspsi_1,\baspsi_2) = \int_0^{L_y}\int_{0}^{L_x} x y dxdy = \frac{L_{x}^{2} L_{y}^{2}}{4}, \\
A_{1,3} &= (\baspsi_1,\baspsi_3) = \int_0^{L_y}\int_{0}^{L_x} x^{2} y dxdy = \frac{L_{x}^{3} L_{y}^{2}}{6}, \\
A_{2,0} &= (\baspsi_2,\baspsi_0) = \int_0^{L_y}\int_{0}^{L_x} y dxdy = \frac{L_{x} L_{y}^{2}}{2}, \\
A_{2,1} &= (\baspsi_2,\baspsi_1) = \int_0^{L_y}\int_{0}^{L_x} x y dxdy = \frac{L_{x}^{2} L_{y}^{2}}{4}, \\
A_{2,2} &= (\baspsi_2,\baspsi_2) = \int_0^{L_y}\int_{0}^{L_x} y^{2} dxdy = \frac{L_{x} L_{y}^{3}}{3}, \\
A_{2,3} &= (\baspsi_2,\baspsi_3) = \int_0^{L_y}\int_{0}^{L_x} x y^{2} dxdy = \frac{L_{x}^{2} L_{y}^{3}}{6}, \\
A_{3,0} &= (\baspsi_3,\baspsi_0) = \int_0^{L_y}\int_{0}^{L_x} x y dxdy = \frac{L_{x}^{2} L_{y}^{2}}{4}, \\
A_{3,1} &= (\baspsi_3,\baspsi_1) = \int_0^{L_y}\int_{0}^{L_x} x^{2} y dxdy = \frac{L_{x}^{3} L_{y}^{2}}{6}, \\
A_{3,2} &= (\baspsi_3,\baspsi_2) = \int_0^{L_y}\int_{0}^{L_x} x y^{2} dxdy = \frac{L_{x}^{2} L_{y}^{3}}{6}, \\
A_{3,3} &= (\baspsi_3,\baspsi_3) = \int_0^{L_y}\int_{0}^{L_x} x^{2} y^{2} dxdy = \frac{L_{x}^{3} L_{y}^{3}}{9}
\tp
\end{align*}
!et
The right-hand side vector has the entries

!bt
\begin{align*}
b_{0} &= (\baspsi_0,f) = \int_0^{L_y}\int_{0}^{L_x}1\cdot (1+x^2)(1+2y^2) dxdy\\
&= \int_0^{L_y}(1+2y^2)dy \int_{0}^{L_x} (1+x^2)dx
= (L_y + \frac{2}{3}L_y^3)(L_x + \frac{1}{3}L_x^3)\\
b_{1} &= (\baspsi_1,f) = \int_0^{L_y}\int_{0}^{L_x} x(1+x^2)(1+2y^2) dxdy\\
&=\int_0^{L_y}(1+2y^2)dy \int_{0}^{L_x} x(1+x^2)dx
= (L_y + \frac{2}{3}L_y^3)({\half}L_x^2 + \frac{1}{4}L_x^4)\\
b_{2} &= (\baspsi_2,f) = \int_0^{L_y}\int_{0}^{L_x} y(1+x^2)(1+2y^2) dxdy\\
&= \int_0^{L_y}y(1+2y^2)dy \int_{0}^{L_x} (1+x^2)dx
= ({\half}L_y^2 + {\half}L_y^4)(L_x + \frac{1}{3}L_x^3)\\
b_{3} &= (\baspsi_3,f) = \int_0^{L_y}\int_{0}^{L_x} xy(1+x^2)(1+2y^2) dxdy\\
&= \int_0^{L_y}y(1+2y^2)dy \int_{0}^{L_x} x(1+x^2)dx
= ({\half}L_y^2 + {\half}L_y^4)({\half}L_x^2 + \frac{1}{4}L_x^4)
\tp
\end{align*}
!et

There is a general pattern in these calculations that we can explore.
An arbitrary matrix entry has the formula

!bt
\begin{align*}
A_{i,j} &= (\baspsi_i,\baspsi_j) = \int_0^{L_y}\int_{0}^{L_x}
\baspsi_i\baspsi_j dx dy \\
&= \int_0^{L_y}\int_{0}^{L_x}
\baspsi_{p,q}\baspsi_{r,s} dx dy
= \int_0^{L_y}\int_{0}^{L_x}
\hat\baspsi_p(x)\hat\baspsi_q(y)\hat\baspsi_r(x)\hat\baspsi_s(y) dx dy\\
&= \int_0^{L_y} \hat\baspsi_q(y)\hat\baspsi_s(y)dy
\int_{0}^{L_x} \hat\baspsi_p(x) \hat\baspsi_r(x) dx\\
&= \hat A^{(x)}_{p,r}\hat A^{(y)}_{q,s},
\end{align*}
!et
where

!bt
\[ \hat A^{(x)}_{p,r} = \int_{0}^{L_x} \hat\baspsi_p(x) \hat\baspsi_r(x) dx,
\quad
\hat A^{(y)}_{q,s} = \int_0^{L_y} \hat\baspsi_q(y)\hat\baspsi_s(y)dy,
\]
!et
are matrix entries for one-dimensional approximations. Moreover,
$i=p N_x+q$ and $j=s N_y+r$.


With $\hat\baspsi_p(x)=x^p$ we have

!bt
\[ \hat A^{(x)}_{p,r} = \frac{1}{p+r+1}L_x^{p+r+1},\quad
\hat A^{(y)}_{q,s} = \frac{1}{q+s+1}L_y^{q+s+1},
\]
!et
and

!bt
\[ A_{i,j} = \hat A^{(x)}_{p,r} \hat A^{(y)}_{q,s} =
\frac{1}{p+r+1}L_x^{p+r+1} \frac{1}{q+s+1}L_y^{q+s+1},
\]
!et
for $p,r\in\Ix$ and $q,s\in\Iy$.

Corresponding reasoning for the right-hand side leads to

!bt
\begin{align*}
b_i &= (\baspsi_i,f) = \int_0^{L_y}\int_{0}^{L_x}\baspsi_i f\,dxdx\\
&= \int_0^{L_y}\int_{0}^{L_x}\hat\baspsi_p(x)\hat\baspsi_q(y) f\,dxdx\\
&= \int_0^{L_y}\hat\baspsi_q(y) (1+2y^2)dy
\int_0^{L_y}\hat\baspsi_p(x) (1+x^2)dx\\
&= \int_0^{L_y} y^q (1+2y^2)dy
\int_0^{L_y}x^p (1+x^2)dx\\
&= (\frac{1}{q+1} L_y^{q+1} + \frac{2}{q+3}L_y^{q+3})
(\frac{1}{p+1} L_x^{p+1} + \frac{1}{p+3}L_x^{p+3})
\end{align*}
!et

Choosing $L_x=L_y=2$, we have

!bt
\[
A =
\left[\begin{array}{cccc}
4 & 4 & 4 & 4\\
4 & \frac{16}{3} & 4 & \frac{16}{3}\\
4 & 4 & \frac{16}{3} & \frac{16}{3}\\
4 & \frac{16}{3} & \frac{16}{3} & \frac{64}{9}
\end{array}\right],\quad
b = \left[\begin{array}{c}
\frac{308}{9}\\\frac{140}{3}\\44\\60\end{array}\right],
\quad c = \left[
\begin{array}{r}
-\frac{1}{9}\\
- \frac{2}{3}
\frac{4}{3} \\
 8
\end{array}\right]
\tp
\]
!et
Figure ref{fem:approx:fe:2D:fig:ubilinear} illustrates the result.

FIGURE: [fig/approx2D_bilinear, width=800] Approximation of a 2D quadratic function (left) by a 2D bilinear function (right) using the Galerkin or least squares method. label{fem:approx:fe:2D:fig:ubilinear}


The calculations can also be done using  `sympy`. The following code computes the matrix of our example
!bc pycod
import sympy as sym

x, y, Lx, Ly = sym.symbols("x y L_x L_y")

def integral(integrand):
  Ix = sym.integrate(integrand, (x, 0, Lx))
  I = sym.integrate(Ix, (y, 0, Ly))
  return I

basis = [1, x, y, x*y]
A = sym.Matrix(sym.zeros(4,4))

for i in range(len(basis)):
  psi_i = basis[i]
  for j in range(len(basis)):
    psi_j = basis[j]
    A[i,j] = integral(psi_i*psi_j)
!ec
We remark that `sympy` may even write the output in LaTeX or C++ format
using the functions  `latex` and  `ccode`.

===== Implementation =====
label{fem:approx:2D:global:code}

The `least_squares` function from
Section ref{fem:approx:global:orth} and/or the
file "`approx1D.py`": "${fem_src}/fe_approx1D.py"
can with very small modifications solve 2D approximation problems.
First, let `Omega` now be a list of the intervals in $x$ and $y$ direction.
For example, $\Omega = [0,L_x]\times [0,L_y]$ can be represented
by `Omega = [[0, L_x], [0, L_y]]`.

Second, the symbolic integration must be extended to 2D:

!bc pycod
import sympy as sym

integrand = psi[i]*psi[j]
I = sym.integrate(integrand,
                 (x, Omega[0][0], Omega[0][1]),
                 (y, Omega[1][0], Omega[1][1]))
!ec
provided `integrand` is an expression involving the `sympy` symbols `x`
and `y`.
The 2D version of numerical integration becomes

!bc pycod
if isinstance(I, sym.Integral):
    integrand = sym.lambdify([x,y], integrand, 'mpmath')
    I = mpmath.quad(integrand,
                    [Omega[0][0], Omega[0][1]],
                    [Omega[1][0], Omega[1][1]])
!ec
The right-hand side integrals are modified in a similar way.
(We should add that `mpmath.quad` is sufficiently fast
even in 2D, but `scipy.integrate.nquad` is much faster.)

Third, we must construct a list of 2D basis functions. Here are two
examples based on tensor products of 1D "Taylor-style" polynomials $x^i$
and 1D sine functions $\sin((i+1)\pi x)$:

!bc pycod
def taylor(x, y, Nx, Ny):
    return [x**i*y**j for i in range(Nx+1) for j in range(Ny+1)]

def sines(x, y, Nx, Ny):
    return [sym.sin(sym.pi*(i+1)*x)*sym.sin(sym.pi*(j+1)*y)
            for i in range(Nx+1) for j in range(Ny+1)]
!ec
The complete code appears in
"`approx2D.py`": "${fem_src}/fe_approx2D.py".

The previous hand calculation where a quadratic $f$ was approximated by
a bilinear function can be computed symbolically by

!bc ipy
>>> from approx2D import *
>>> f = (1+x**2)*(1+2*y**2)
>>> psi = taylor(x, y, 1, 1)
>>> Omega = [[0, 2], [0, 2]]
>>> u, c = least_squares(f, psi, Omega)
>>> print(u)
8*x*y - 2*x/3 + 4*y/3 - 1/9
>>> print(sym.expand(f))
2*x**2*y**2 + x**2 + 2*y**2 + 1
!ec
We may continue with adding higher powers to the basis:

!bc ipy
>>> psi = taylor(x, y, 2, 2)
>>> u, c = least_squares(f, psi, Omega)
>>> print(u)
2*x**2*y**2 + x**2 + 2*y**2 + 1
>>> print(u-f)
0
!ec
For $N_x\geq 2$ and $N_y\geq 2$ we recover the exact function $f$, as
expected, since in that case $f\in V$, see
Section ref{fem:approx:global:exact1}.

===== Extension to 3D =====
label{fem:approx:3D:global}

Extension to 3D is in principle straightforward once the 2D extension
is understood. The only major difference is that we need the
repeated outer tensor product,

!bt
\[ V = V_x\otimes V_y\otimes V_z\tp\]
!et
In general, given vectors (first-order tensors)
$a^{(q)} = (a^{(q)}_0,\ldots,a^{(q)}_{N_q})$, $q=0,\ldots,m$,
the tensor product $p=a^{(0)}\otimes\cdots\otimes a^{m}$ has
elements

!bt
\[ p_{i_0,i_1,\ldots,i_m} = a^{(0)}_{i_1}a^{(1)}_{i_1}\cdots a^{(m)}_{i_m}\tp\]
!et
The basis functions in 3D are then

!bt
\[ \baspsi_{p,q,r}(x,y,z) = \hat\baspsi_p(x)\hat\baspsi_q(y)\hat\baspsi_r(z),\]
!et
with $p\in\Ix$, $q\in\Iy$, $r\in\Iz$. The expansion of $u$ becomes

!bt
\[ u(x,y,z) = \sum_{p\in\Ix}\sum_{q\in\Iy}\sum_{r\in\Iz} c_{p,q,r}
\baspsi_{p,q,r}(x,y,z)\tp\]
!et
A single index can be introduced also here, e.g., $i=rN_xN_y + qN_x + p$,
$u=\sum_i c_i\baspsi_i(x,y,z)$.

!bnotice Use of tensor product spaces
Constructing a multi-dimensional space and basis from tensor products
of 1D spaces is a standard technique when working with global basis
functions. In the world of finite elements, constructing basis functions
by tensor products is much used on quadrilateral and hexahedra cell
shapes, but not on triangles and tetrahedra. Also, the global
finite element basis functions are almost exclusively denoted by a single
index and not by the natural tuple of indices that arises from
tensor products.
!enotice

!split
======= Exercises =======

===== Problem: Linear algebra refresher =====
label{fem:approx:exer:linalg1}
file=linalg1

Look up the topic of *vector space* in your favorite linear algebra
book or search for the term at Wikipedia.

!bsubex
Prove that vectors in the plane spanned by the vector $(a,b)$  form a vector space
by showing that all the axioms of a vector space are satisfied.


!bsol
The axioms of a vector space go as follows (see
"Wikipedia": "https://en.wikipedia.org/wiki/Vector_space", but
we use slightly different notation below):

  o The sum of $u$ and $v$, denoted by $u + v$, is in V.
  o $u + v = v + u$.
  o $(u + v) + w = u + (v + w)$.
  o There is a *zero* vector $0$ in V such that $u + 0 = u$.
  o For each $u$ in V, there is a vector $-u$ in V such that $u + (-u) = 0$.
  o The scalar multiple of $u$ by $\gamma$, denoted by $\gamma u$, is in V.
  o $\gamma (u + v) = \gamma u + \gamma v$.
  o $(\gamma + \delta)u = \gamma u + \delta u$ for scalar $\gamma$ and $\delta$.
  o $\gamma(\delta u) = \gamma\delta u$.
  o $1u = u$.

We must show that each axiom is fulfilled by planar vectors and their
mathematical rules. Let $u=(a,b)$ and $v=(c,d)$.

Axiom 1:

!bt
\[ u+v = (a,b)+(c,d)=(a+c, b+d),\]
!et
is a planar vector and therefore in $V$.

Axiom 2:

!bt
\[ u+v = (a,b)+(c,d)=(a+c, b+d) = (c+a, d+b) = v + u\tp\]
!et

Axiom 3:

!bt
\[ (u+v) + w = ((a,b) + (c,d)) + (e,f) = (a,b) + ((c,d) + (e,f)) = u + (v+w)\tp\]
!et

Axiom 4: The $(0,0)$ vector is the 0 element,

!bt
\[ u + 0 = (a,b) + (0,0) = (a+0, b+0) = (a,b) = u\tp\]
!et

Axiom 5: Let $-u$ element is $(-a,-b)$, so

!bt
\[ u + (-u) = (a,b) + (-a,-b) = (0,0) = 0\tp\]
!et

Axiom 6:

!bt
\[ \gamma u = \gamma\cdot(a,b)= (\gamma a, \gamma b),\]
!et
is also a planar vector and therefore in $V$.

Axiom 7:

!bt
\[ \gamma (u + v) = \gamma \cdot((a,b) + (c,d) = \gamma (a,b) + \gamma (c,d)\tp\]
!et

Axiom 8:

!bt
\[ (\gamma + \delta)u = (\gamma + \delta)(a,b) = \gamma (a,b) + \delta (a,b) = \gamma u + \delta u\tp\]
!et

Axiom 9:

!bt
\[ \gamma (\delta u) = \gamma (\delta\cdot (a,b)) = \gamma (\delta a, \delta b) = (\gamma\delta a, \gamma\delta b) = \gamma\delta u\tp\]
!et

Axiom 10:

!bt
\[ 1u = 1(a,b) = (1\cdot a, 1\cdot b) = (a,b) = u\tp\]
!et

!esol
!esubex

!bsubex
Prove that all linear functions of the form $ax+b$ constitute a vector space,
$a,b\in\Real$.

!bsol
Let $u=ax+b$ and $v=cx+d$. We verify each axiom.

Axiom 1:

!bt
\[ u+v = ax + b + cx + d = (a+c)x + (b + d),\]
!et
is also a linear function and therefore in $V$.

Axiom 2:

!bt
\[ u+v = ax+b + cx+ d = cx+d + ax+ b = v + u\tp\]
!et

Axiom 3:

!bt
\[ (u+v) + w = (ax+b + cx+ d) + ex + f = ax+b + cx+ d + ex + f
ax+b + (cx+ d + ex + f) = u + (v+w)\tp\]
!et

Axiom 4: The $0x+0=0$ function is the 0 element,

!bt
\[ u + 0 = ax + b + 0 = ax + b = u\tp\]
!et

Axiom 5: Let $-u$ element is $-ax-b$, so

!bt
\[ u + (-u) = ax + b + (-ax - b) = 0\tp\]
!et

Axiom 6:

!bt
\[ \gamma u = \gamma(ax +b)= \gamma ax + \gamma b,\]
!et
is also a linear function and therefore in $V$.

Axiom 7:

!bt
\[ \gamma (u + v) = \gamma (ax+b + cx+ d) = \gamma ax + \gamma b + \gamma cx + \gamma d = \gamma(ax + b) + \gamma(cd + d) = \gamma u + \gamma v\tp\]
!et

Axiom 8:

!bt
\[ (\gamma + \delta)u = (\gamma + \delta)(ax + b) = \gamma (ax+b) + \delta (ax+b) = \gamma u + \delta u\tp\]
!et

Axiom 9:

!bt
\[ \gamma (\delta u) = \gamma (\delta(ax+b))) = \gamma\delta u\tp\]
!et

Axiom 10:

!bt
\[ 1u = 1(ax+b) = ax+b = u\tp\]
!et
!esol
!esubex

!bsubex
Show that all quadratic functions of the form $1 + ax^2 + bx$ *do not*
constitute a vector space.

!bsol
Let $u=ax^2+bx + 1$ and $v=cx^2 + dx + 1$. We try to verify each axiom.

Axiom 1:

!bt
\[ u+v = ax^2 + bx + 1 + cx^2 + dx + 1 = (a+c)x^2 + (b+d)x + 2,\]
!et
but this quadratic function is not in $V$ because the constant term is 2
and not 1. Consequently, quadratic functions of the particular
form $1 + ax^2 + bx$ do not constitute a vector space.
The more general form  $ax^2 + bx +c$ for arbitrary constants $a$, $b$, and
$c$ makes functions that span a function space.

!esol
!esubex

!bsubex
Check out
the topic of *inner product spaces*. Suggest a possible inner product
for the space of all linear functions of the form $ax+b$, $a,b\in\Real$,
defined on some interval $\Omega=[A,B]$.
Show that this particular inner product satisfies the
general requirements of an inner product in a vector space.

!bsol
According to "Wikipedia": "https://en.wikipedia.org/wiki/Inner_product_space",
an inner product space, with an inner product $(u,v)$, has three axioms
(we drop the possibility of complex numbers and assume everything is real):

 o Symmetry: $(u,v)$ = $(v,u)$
 o Linearity in the first argument: $\gamma u, v) = \gamma (u,v)$
 o Positive-definiteness: $(u,u)\geq 0$ and $(u,u)=0$ implies $u=0$

A possible inner product for linear functions on a domain $\Omega$ is

!bt
\[ (u,v) = \int_A^B uv\dx = \int_A^B (ax+b)(cx+d)\dx\tp\]
!et
Symmetry is obvious since $uv=vu$:

!bt
\[ (u,v) = \int_A^B uv\dx = \int_A^B vu d = (v,u)\tp\]
!et
Linearity in the first argument:

!bt
\[  (\gamma u,v) = \gamma\int_A^B(\gamma u)v\dx = \gamma\int_A^B uv\dx = \gamma (u,v)\tp\]
!et
Positive-definiteness:

!bt
\[ (u,u) = \int_A^B(ax+b)^2\dx\geq 0,\]
!et
since the integral of a function $f(x)\geq$ must be greater than or equal to
zero, and in particular,

!bt
\[ (u,u)=0\quad\Rightarrow\quad \int_A^B(ax+b)^2\dx = 0,\]
!et
implies $ax+b=0$.
!esol
!esubex


===== Problem: Approximate a three-dimensional vector in a plane =====
label{fem:approx:exer:vec:3Dby2D}
file=vec111_approx

Given $\f = (1,1,1)$ in $\Real^3$, find the best approximation vector
$\u$ in the plane spanned by the unit vectors $(1,0)$ and $(0,1)$.
Repeat the calculations using the vectors $(2,1)$ and $(1,2)$.

!bsol
We have the vector $\pmb{f} = (1,1,1)$.
Our aim is to approximate this with a vector in the vector-space spanned by the unit vectors $\pmb{\phi_0}$ and $\pmb{\phi_1}$. We seek a solution $u = c_0 \phi_0 + c_1 \phi_1$. To do this we use the least-square-method and solve the equation $\pmb{A c} = \pmb{b}$. Or written out:

!bt
\[
\left[\begin{array}{cc}
A_{0,0} & A_{0,1} \\A_{1,0} & A_{1,1}
\end{array}\right]
\left[ \begin{array}{c} c_0 \\ c_1 \end{array} \right]
=
\left[ \begin{array}{c} b_0 \\ b_1 \end{array} \right]
\]
!et
where $A_{i,j} = ( \pmb{\phi_i}, \pmb{\phi_j} )$ and $b_i = ( \pmb{\phi_i}, \pmb{f} )$.

We start with $\pmb{\phi_0} = (0,1)$, $\pmb{\phi_1} = (1,0)$.
Calculations give

!bt
\begin{align*}
A_{0,0} &= ([1,0],[1,0]) = 1,\\
A_{0,1} &= ([1,0],[0,1]) = 0,
A_{1,0} &= ([0,1],[1,0]) = 0,
A_{1,1} &= ([0,1],[0,1]) = 1,
b_0 &= ([1,0],[1,1,1]) = 1,
b_0 &= ([0,1],[1,1,1]) = 1.
\end{align*}
!et
The result becomes $ c_0 = c_1 =1$, and hence

!bt
\[ u = 1\cdot\pmb{\phi_0}  + 1\cdot\pmb{\phi_1}.\]
!et

Then we proceed with
$\pmb{\phi_0} = (2,1)$, $\pmb{\phi_1} = (1,2)$.
Calculations give

!bt
\begin{align*}
A_{0,0} &= ([2,1],[2,1]) = 2\cdot 2 + 1\cdot 1 = 5,
A_{0,1} &= ([2,1],[1,2]) = 2\cdot 1 + 1\cdot 2 = 4,
A_{1,0} &= ([1,2],[2,1]) = 1\cdot 2 + 2\cdot 1 = 4,
A_{1,1} &= ([1,2],[1,2]) = 1\cdot 1 + 2\cdot 2 = 5,
b_0 &= ([2,1],[1,1,1]) = 2\cdot 1 + 1\cdot 1 = 3,
b_0 &= ([1,2],[1,1,1]) = 1\cdot 1 + 2\cdot 1 = 3.
\end{align*}
!et
Solving for the $c_i$ values results in $ c_0 = c_1 =\frac{1}{3}$ and
hence

!bt
\[ u = \frac{1}{3}\cdot \pmb{\phi_0}  + \frac{1}{3}\cdot \pmb{\phi_1} .\]
!et
!esol

===== Problem: Approximate a parabola by a sine =====
label{fem:approx:exer:parabola_sine}
file=parabola_sin

Given the function $f(x)=1 + 2x(1-x)$ on $\Omega=[0,1]$, we want to
find an approximation in the function space

!bt
\[ V = \hbox{span}\{1, \sin(\pi x)\}\tp\]
!et

!bsubex
Sketch or plot $f(x)$. Think intuitively how an expansion in terms
of the basis functions of $V$, $\baspsi_0(x)=1$, $\baspsi_1(x)=\sin(\pi x)$,
can be construction to yield a best approximation to $f$. Or
phrased differently, see if you can guess the coefficients $c_0$ and $c_1$
in the expansion

!bt
\[ u(x) = c_0\baspsi_0 + c_1\baspsi_1 = c_0 + c_1\sin (\pi x)\tp\]
!et
Compute the $L^2$ error $||f-u||_{L^2}=(\int_0^1(f-u)^2\dx)^{1/2}$.

!bhint
If you make a mesh function `e` of the error
on some mesh with uniformly spaced coordinates in
the array `xc`, the integral can be approximated as `np.sqrt(dx*np.sum(e**2))`,
where `dx=xc[0]-xc[1]` is the mesh spacing and `np` is an alias for the
`numpy` module in Python.
!ehint

!bsol
The function $\baspsi_0=1$ can be used to ``take care of'' of the
constant 1 in $f$, while $\baspsi_1=\sin(\pi x)$ can approximate
the parabola. The maximum of $f$ is 3/2, so we should use
$u(x) = 1\cdot\baspsi_0 + \frac{1}{2}\baspsi_1$.

Plotting and the computation of the $L^2$ error can be done by

@@@CODE exer/parabola_sin.py fromto: import numpy@plt.savefig\('tmp1
The $L^2$ error becomes $0.0179$.

FIGURE: [fig/parabola_sin_a, width=500 frac=0.7]

!esol
!esubex

!bsubex
Perform the hand calculations for a least squares approximation.

!bsol
The least squares method ends up with a linear system where the
coefficient matrix has entries $A_{i,j}=\int_0^1\baspsi_i\baspsi_j\dx$
and the right-hand side has entries $b_i=\int_0^1\baspsi_i\dx$.
We can use `sympy` do carry out the integrals:

@@@CODE exer/parabola_sin.py fromto: # Do the calculat@print\(sym\.latex
Looking at the results, we get the linear system

!bt
\[
\left(\begin{matrix}1 & \frac{2}{\pi}\\\frac{2}{\pi} & \frac{1}{2}\end{matrix}\right)
\left(\begin{bmatrix}
\frac{- 24 \pi^{2} - 96 + 4 \pi^{4}}{3 \pi^{2} \left(-8 + \pi^{2}\right)}\\
\frac{- 4 \pi^{2} + 48}{3 \pi \left(-8 + \pi^{2}\right)}
\end{bmatrix}\right) =
\left(\begin{matrix}
\frac{4}{3}\\
\frac{8}{\pi^{3}} + \frac{2}{\pi}
\end{matrix}\right)
\]
!et
The resulting best approximation reads

!bt
\[ u(x) = \frac{4 \left(- \pi^{2} + 12\right) \sin{\left (\pi x \right )}}{3 \pi \left(-8 + \pi^{2}\right)} + \frac{- 24 \pi^{2} - 96 + 4 \pi^{4}}{3 \pi^{2} \left(-8 + \pi^{2}\right)}\approx 1.025 + 0.484\sin(\pi x)\]
!et
The $L^2$ error turns out to be $0.00876$.
To conclude, the least squares method is slightly better than the intuition
in this case.

FIGURE: [fig/parabola_sin_b, width=500 frac=0.7]

!esol
!esubex

===== Problem: Approximate the exponential function by power functions =====
label{fem:approx:exer:exp:powers}
file=exp_powers

Let $V$ be a function space with basis functions $x^i$,
$i=0,1,\ldots,N$.  Find the best approximation to $f(x)=\exp(-x)$ on
$\Omega =[0,8]$ among all functions in $V$ for $N=2,4,6$. Illustrate
the three approximations in three separate plots.

!bhint
Apply the `lest_squares` and
`comparison_plot` functions in the `approx1D.py` module as these
make the exercise easier to solve.
!ehint

!bsol
A suitable code is

@@@CODE exer/exp_powers.py fromto: from approx1D@
For the case $N=2$ the program prints the following, here edited for clearer
reading:

!bc
A:
Matrix([[8, 32, 512/3], [32, 512/3, 1024], [512/3, 1024, 32768/5]])
b:
Matrix([[-exp(-8) + 1], [-9*exp(-8) + 1], [-82*exp(-8) + 2]])

f: exp(-x)
u: x**2*(-465*exp(-8)/4096 + 105/4096) + x*(-141/512 +
   405*exp(-8)/512) - 111*exp(-8)/128 + 87/128
!ec

FIGURE: [fig/exp_powers_N2, width=500 frac=0.8]
!esol

# Taylor: these polynomials go so far off on [0,8] that it is not a
# good idea to add them.

===== Problem: Approximate the sine function by power functions =====
label{fem:approx:exer:sin:powers}
file=sin_powers

In this exercise we want to approximate the sine function by polynomials
of order $N+1$. Consider two bases:

!bt
\begin{align*}
V_1 &= \{x, x^3, x^5, \ldots, x^{N-2}, x^N \}, \\
V_2 &= \{1,x,x^2,x^3,\ldots, x^N\}\tp
\end{align*}
!et
The basis $V_1$ is motivated by the fact that the Taylor polynomial
approximation to the sine function has only odd powers, while $V_2$
is motivated by the assumption that also the even powers could
improve the approximation in a least-squares setting.

Compute the best approximation to $f(x)=\sin(x)$ among all functions
in $V_1$ and $V_2$ on two domains of increasing sizes: $\Omega_{1,k} =
[0, k\pi]$, $k=2,3\ldots,6$ and $\Omega_{2,k} = [-k\pi /2, k\pi/2]$,
$k=2,3,4,5$.  Make plots for all combinations of $V_1$, $V_2$,
$\Omega_1$, $\Omega_2$, $k=2,3,\ldots,6$.

Add a plot of the $N$-th degree Taylor polynomial approximation of
$\sin(x)$ around $x=0$.

!bhint
You can make a loop over $V_1$ and $V_2$, a loop over
$\Omega_1$ and $\Omega_2$, and a loop over $k$. Inside the loops,
call the functions `least_squares` and
`comparison_plot` from the `approx1D` module.
$N=7$ is a suggested value.
!ehint

## With [-L,L] the even powers have no effect because the function f
## is odd around x=0 and the even powers are even and probably get
## c_i=0 for these terms

# Solveig explanations based on f-B approx

!bsol
Suitable code is

@@@CODE exer/sin_powers.py fromto: import sympy@

The odd powers ($V_1$ space) behave not so good on $\Omega_{1,k}$,
but better on $\Omega_{2,k}$:

FIGURE: [fig/sin_powers_N7_V1_Omega1, width=800 frac=1] $V_1$ space, $\Omega_{1,k}$ domain.

FIGURE: [fig/sin_powers_N7_V1_Omega2, width=800 frac=1] $V_1$ space, $\Omega_{2,k}$ domain.

Including also even powers ($V_2$ space) is clearly much better:

FIGURE: [fig/sin_powers_N7_V2_Omega1, width=800 frac=1] $V_2$ space, $\Omega_{1,k}$ domain.

FIGURE: [fig/sin_powers_N7_V2_Omega2, width=800 frac=1] $V_2$ space, $\Omega_{2,k}$ domain.

Comparison with a standard Taylor series shows that it is very inferior
as an approximation over the entire domain, but much more accurate
close to the origin (as expected, since the Taylor series is constructed
with this property, while the least squares method tries to find a good
approximation over the entire domain).

FIGURE: [fig/sin_taylor7, width=500 frac=0.8]

!esol

===== Problem: Approximate a steep function by sines =====
label{fem:approx:exer:tanh:sine1}
file=tanh_sines

Find the best approximation of $f(x) = \tanh (s(x-\pi))$ on
$[0, 2\pi]$ in the space $V$ with basis
$\baspsi_i(x) = \sin((2i+1)x)$, $i\in\If = \{0,\ldots,N\}$.
Make a movie showing how $u=\sum_{j\in\If}c_j\baspsi_j(x)$
approximates $f(x)$ as $N$ grows. Choose $s$ such that $f$ is
steep ($s=20$ is appropriate).

!bhint
One may naively call the `least_squares_orth` and `comparison_plot`
from the `approx1D` module in a loop and extend the basis with
one new element in each pass. This approach
implies a lot of recomputations.
A more efficient strategy is to let `least_squares_orth`
compute with only one basis function at a time and accumulate
the corresponding `u` in the total solution.
!ehint

!bhint
`ffmpeg` or `avconv` may skip frames when plot files are combined to
a movie. Since there are few files and we want to see each of them,
use `convert` to make an animated GIF file (`-delay 200` is
suitable).
!ehint

!bremarks
Approximation of a discontinuous (or steep) $f(x)$ by sines,
results in slow convergence and oscillatory behavior of the
approximation close to the abrupt changes in $f$.
This is known as the "Gibb's phenomenon": "http://en.wikipedia.org/wiki/Gibbs_phenomenon".
!eremarks

!bsol
The code may read

@@@CODE exer/tanh_sines.py fromto: import sympy@

# #if FORMAT in ('html', 'sphinx')
FIGURE: [mov/tanh_sines.gif, width=500]
# #else
FIGURE: [fig/tanh_sines, width=800 frac=1]
# #endif
!esol

===== Problem: Approximate a steep function by sines with boundary adjustment =====
label{fem:approx:exer:tanh:sine3}
file=tanh_sines_boundary_term

We study the same approximation problem as in
Problem ref{fem:approx:exer:tanh:sine1}. Since $\baspsi_i(0)=\baspsi_i(2\pi)=0$
for all $i$, $u=0$ at the boundary points $x=0$ and $x=2\pi$, while
$f(0)=-1$ and $f(2\pi)=1$. This discrepancy at the boundary can be
removed by adding a boundary function $B(x)$:

!bt
\[
u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_j(x),
\]
!et
where $B(x)$ has the right boundary values: $B(x_L)=f(x_L)$ and
$B(x_R)=f(x_R)$, with $x_L=0$ and $x_R=2\pi$ as the boundary points.
A linear choice of $B(x)$ is

!bt
\[ B(x) = \frac{(x_R-x)f(x_L) + (x-x_L)f(x_R)}{x_R-x_L}\tp\]
!et

!bsubex
Use the basis
$\baspsi_i(x) = \sin((i+1)x)$, $i\in\If = \{0,\ldots,N\}$
and plot $u$ and $f$ for $N=16$. (It suffices to make plots for even $i$.)

!bsol
With a boundary term $B(x)$ we call `least_squares_orth` with
`f-B` as right-hand side function, and we must remember to add $B$ to $u$.

We can extend the code from Exercise ref{fem:approx:exer:tanh:sine1} and
let the function `efficient` handle different choices of basis.
Appropriate code for all three subexercises is

@@@CODE exer/tanh_sines_boundary_term.py fromto: import sympy@

FIGURE: [fig/tanh_sines_boundary_term_a, width=800 frac=1]

!esol
!esubex

!bsubex
Use the basis from Exercise ref{fem:approx:exer:tanh:sine1},
$\baspsi_i(x) = \sin((2i+1)x)$, $i\in\If = \{0,\ldots,N\}$.
(It suffices to make plots for even $i$.)
Observe that the approximation converges to a piecewise
linear function!

!bsol

# #if FORMAT in ('html', 'sphinx')
FIGURE: [mov/tanh_sines_boundary_term_b.gif, width=500]
# #else
FIGURE: [fig/tanh_sines_boundary_term_b, width=800 frac=1]
# #endif

!esol
!esubex

!bsubex
Use the basis
$\baspsi_i(x) = \sin(2(i+1)x)$, $i\in\If = \{0,\ldots,N\}$,
and observe that the approximation converges to a piecewise
constant function.

!bsol

# #if FORMAT in ('html', 'sphinx')
FIGURE: [mov/tanh_sines_boundary_term_c.gif, width=500]
# #else
FIGURE: [fig/tanh_sines_boundary_term_c, width=800 frac=1]
# #endif

!esol
!esubex

!bremarks
The strange results in b) and c) are due to the choice of
basis. In b), $\basphi_i(x)$ is an odd function around
$x=\pi/2$ and $x=3\pi/2$. No combination of basis functions
is able to approximate the flat regions of $f$.
All basis functions in c) are even around $x=\pi/2$ and $x=3\pi/2$,
but odd at $x=0,\pi,2\pi$. With all the sines represented, as in a),
the approximation is not constrained with a particular symmetry
behavior.
!eremarks

## Note: interesting with
##  with/without boundary term
##  all three basis
##  s=0.5 and s=20





===== Exercise: Fourier series as a least squares approximation =====
label{fem:approx:exer:Fourier}
file=Fourier_ls

!bsubex
Given a function $f(x)$ on an interval $[0,L]$, look up the formula
for the coefficients $a_j$ and $b_j$ in the Fourier series of $f$:

!bt
\begin{equation*}
f(x) = \frac{1}{2}a_0 +
\sum_{j=1}^\infty a_j\cos \left(j\frac{2\pi x}{L}\right)
+ \sum_{j=1}^\infty b_j\sin \left(j\frac{2\pi x}{L}\right)\tp
\end{equation*}
!et

!bsol
From "Wikipedia": "https://en.wikipedia.org/wiki/Fourier_series" we
have

!bt
\begin{align*}
a_j &= \frac{2}{L}\int_0^P f(x)\cos\left(j\frac{2\pi x}{L}\right) \dx,\\
b_j &= \frac{2}{L}\int_0^P f(x)\sin\left(j\frac{2\pi x}{L}\right) \dx\tp
\end{align*}
!et
!esol
!esubex

!bsubex
Let an infinite-dimensional vector space $V$ have the basis functions
$\cos j\frac{2\pi x}{L}$ for $j=0,1,\dots,\infty$ and
$\sin j\frac{2\pi x}{L}$ for $j=1,\dots,\infty$.  Show that the least squares
approximation method from Section ref{fem:approx:global} leads to a
linear system whose solution coincides with the standard formulas for
the coefficients in a Fourier series of $f(x)$ (see also
Section ref{fem:approx:global:Fourier}).

!bhint
You may choose

!bt
\begin{equation}
\baspsi_{2i} = \cos\left( i\frac{2\pi}{L}x\right),\quad
\baspsi_{2i+1} = \sin\left( i\frac{2\pi}{L}x\right),
label{fem:approx:exer:Fourier:basis}
\end{equation}
!et
for $i=0,1,\ldots,N\rightarrow\infty$.
!ehint

!bsol
The entries in the linear system arising from the least squares method
are $A_{i,j}=\int_0^L\baspsi_i\baspsi_j\dx$ and $b_i=\int_0^L
f(x)\baspsi_i \dx$. To avoid name clash between the right-hand side
components of the linear system and the $b_i$ coefficients in the
Fourier series, we use the symbol $q_i$ for the former.
With the basis functions in
(ref{fem:approx:exer:Fourier:basis}) we get four different types
of integrals:

!bt
\begin{align*}
A_{2i,2j} &= \int_0^L \cos\left( i\frac{2\pi}{L}x\right)
\cos\left( j\frac{2\pi}{L}x\right)\dx = A_{2j,2i},\\
A_{2i, 2j+1} &= \int_0^L \cos\left( i\frac{2\pi}{L}x\right)
\sin\left( j\frac{2\pi}{L}x\right)\dx,\\
A_{2i+1,2j} &= \int_0^L \sin\left( i\frac{2\pi}{L}x\right)
\cos\left( j\frac{2\pi}{L}x\right)\dx,\\
A_{2i+1, 2j+1} &= \int_0^L \sin\left( i\frac{2\pi}{L}x\right)
\sin\left( j\frac{2\pi}{L}x\right)dx,\\
q_{2i} &= \int_0^L f(x)\cos\left( i\frac{2\pi}{L}x\right)\dx,\\
q_{2i+1} &= \int_0^L f(x)\sin\left( i\frac{2\pi}{L}x\right)\dx\tp
\end{align*}
!et
Now, the sine and cosine basis functions are orthogonal on $[0,L]$.
We have in general

!bt
\begin{align*}
\int_0^L
\cos\left( i\frac{2\pi}{L}x\right) \cos\left( j\frac{2\pi}{L}x\right)\dx
&=0,\quad i\neq j,\\
\int_0^L
\cos\left( i\frac{2\pi}{L}x\right) \cos\left( j\frac{2\pi}{L}x\right)\dx
&=\frac{L}{2},\quad i= j\neq 0,\\
\int_0^L
\cos\left( i\frac{2\pi}{L}x\right) \cos\left( j\frac{2\pi}{L}x\right)\dx
&=L,\quad i= j= 0,\\
\int_0^L
\sin\left( i\frac{2\pi}{L}x\right) \sin\left( j\frac{2\pi}{L}x\right)\dx
&=0,\quad i\neq j,\\
\int_0^L
\sin\left( i\frac{2\pi}{L}x\right) \sin\left( j\frac{2\pi}{L}x\right)\dx
&=\frac{L}{2},\quad i= j,\\
\int_0^L
\cos\left( i\frac{2\pi}{L}x\right) \sin\left( j\frac{2\pi}{L}x\right)\dx
&=0\tp
\end{align*}
!et
These results imply that only diagonal terms in the coefficient matrix
are different from zero. We have

!bt
\begin{align*}
A_{0,0} &= L,\\
A_{2i,2i} &= \frac{L}{2},\quad i>0,\\
A_{2i+1,2i+1} &= \frac{L}{2}\tp
\end{align*}
!et
The unknown vector with components $c_i$ must be arranged as

!bt
\[ (a_0, b_1, a_1, b_2, a_2, b_3, \ldots)\tp \]
!et
We then get

!bt
\[ A_{0,0}a_0=q_0,\quad A_{1,1}b_1=q_1,\quad A_{2,2}a_1=q_2,\quad A_{3,3}b_2=q_3,\ldots\]
!et
These equations lead to the formulas

!bt
\begin{align*}
a_0 &= \frac{1}{L}\int_0^P f(x)\dx,\\
b_1 &= \frac{2}{L}\int_0^P f(x)\sin\left( \frac{2\pi}{L}x\right)\dx,\\
a_1 &= \frac{2}{L}\int_0^P f(x)\cos\left( \frac{2\pi}{L}x\right)\dx,\\
b_2 &= \frac{2}{L}\int_0^P f(x)\sin\left( 2\frac{2\pi}{L}x\right)\dx,\\
a_2 &= \frac{2}{L}\int_0^P f(x)\cos\left( 2\frac{2\pi}{L}x\right)\dx\tp
\end{align*}
!et
which can be generalized to

!bt
\begin{align*}
a_0 &= \frac{1}{L}\int_0^P f(x)\dx,\\
a_j &= \frac{2}{L}\int_0^P f(x)\cos\left( j\frac{2\pi}{L}x\right)\dx,\ j>0,\\
b_j &= \frac{2}{L}\int_0^P f(x)\sin\left( j\frac{2\pi}{L}x\right)\dx,\ j>0,
\end{align*}
!et
and these are the standard formulas for the Fourier coefficients in a)
if we recognize that the $a_0$ above is twice the $a_0$ in the
expressions in a).
!esol
!esubex

!bsubex
Choose $f(x) = H(x-\half)$ on $\Omega=[0,1]$, where $H$ is the
Heaviside function: $H(x)=0$ for $x<0$, $H(x)=1$ for $x>0$ and
$H(0)=\half$. Find the coefficients $a_j$ and $b_j$ in the
Fourier series for $f(x)$. Plot the sum for $j=2N+1$, where $N=5$ and
$N=100$.

!bsol
The formulas give

!bt
\begin{align*}
a_0 &= 2\int_0^1 f(x)\dx = 2\int_{\half}^1 \dx,\\
a_j &= 2\int_0^1 f(x)\cos\left( 2j\pi x\right)\dx
= 2\int_{\half}^1 \cos\left( 2j\pi x\right)\dx,\\
b_j &= 2\int_{\half}^1 \sin\left( 2j\pi x\right)\dx\tp
\end{align*}
!et
The integrals are readily computed by `sympy`:

!bc pyshell
>>> import sympy as sym
>>> j = sym.symbols('k', integer=True)
>>> x = sym.symbols('x', real=True)
>>> I = integrate(cos(2*j*pi*x), (x,Rational(1,2),1))
>>> I
0
>>> I = integrate(cos(2*0*pi*x), (x,Rational(1,2),1))
>>> I
1/2
>>> I = integrate(sin(2*j*pi*x), (x,Rational(1,2),1))
>>> I
(-1)**j/(2*pi*j) - 1/(2*pi*j)
!ec
This means that we have the series

!bt
\[ u(x) = \frac{1}{2} + 2\sum_{j=1}^\infty \frac{(-1)^j - 1}{2\pi j}
\sin\left( 2j\pi x\right)\tp\]
!et
We only get a nonzero coefficient for $j$ odd:

!bt
\[ u(x) = \frac{1}{2} -2\sum_{k=1}^\infty \frac{1}{(2k+1)\pi}
\sin\left( 2(2k+1)\pi x\right)\tp\]
!et

Appropriate computer code for visualizing the series goes like

@@@CODE exer/Fourier_ls.py

FIGURE: [fig/Fourier_Heaviside, width=800 frac=1]

We clearly see the Gibbs' phenomenon: oscillations and overshoot around
the point of discontinuity in the function we try to approximate.
!esol
!esubex


===== Problem: Approximate a steep function by Lagrange polynomials =====
label{fem:approx:exer:tanh:Lagrange}
file=tanh_Lagrange

Use interpolation with uniformly distributed
points and Chebychev nodes to approximate

!bt
\begin{equation*} f(x) = -\tanh(s(x-\half)),\quad x\in [0,1],\end{equation*}
!et
by Lagrange polynomials for $s=5$ and $s=20$, and $N=3,7,11,15$.
Combine $2\times 2$ plots of the approximation for the four
$N$ values, and create such figures for the four combinations of
$s$ values and point types.

!bsol
The following code does the work (symbolically):

@@@CODE exer/tanh_Lagrange.py

For a smooth function ($s=5$), the difference between uniform points and
Chebyshev nodes is not substantial:

FIGURE: [fig/tanh_Lagrange_uniform_s5, width=800 frac=1]

FIGURE: [fig/tanh_Lagrange_Chebyshev_s5, width=800 frac=1]

However, for a steep function ($s=20$) the overshoot and oscillations
associated with uniform points must be considered unacceptable for
larger $N$ values:

FIGURE: [fig/tanh_Lagrange_uniform_s20, width=800 frac=1]

Switching to Chebyshev points does give a great improvement, but
we still have oscillatory approximations:

FIGURE: [fig/tanh_Lagrange_Chebyshev_s20, width=800 frac=1]

!esol

===== Problem: Approximate a steep function by Lagrange polynomials and regression =====
label{fem:approx:exer:tanh:Lagrange:regression}
file=tanh_Lagrange_regression

Redo Problem ref{fem:approx:exer:tanh:Lagrange}, but apply a regression
method with $N$-degree Lagrange polynomials and $2N+1$
data points. Recall that
Problem ref{fem:approx:exer:tanh:Lagrange} applies $N+1$ points and
the resulting approximation interpolates $f$ at these points, while
a regression method with more points does not interpolate $f$ at the
data points.
Do more points and a regression method help reduce
the oscillatory behavior of Lagrange polynomial approximations?

!bsol
We start out with the program from
Problem ref{fem:approx:exer:tanh:Lagrange}. This time we need
to call `Lagrange_polynomails` twice: first to compute the $\baspsi(x)$
functions (of degree $N$) and then to compute the data points corresponding
to a uniform or Chebyshev distribution of $2N+1$ nodes.

@@@CODE exer/tanh_Lagrange_regression.py
An important point is to convert `points` to a `numpy` array using
`dtype=float`. Leaving out this second argument makes an array
of objects of symbolic expressions, and we cannot apply `tanh` to it.

The oscillatory behavior is much reduced using more points and a
regression method, and the difference between uniform and Chebyshev
points is minor, even in the steep case $s=20$:

FIGURE: [fig/tanh_Lagrange_regr_uniform_s20, width=800 frac=1]

FIGURE: [fig/tanh_Lagrange_regr_Chebyshev_s20, width=800 frac=1]

!esol

!split
========= Function approximation by finite elements =========
label{ch:approx:fe}

The purpose of this chapter is to use the ideas from the previous
chapter to approximate functions, but the basis functions are now
of finite element type. 

======= Finite element basis functions =======
label{fem:approx:fe}

The specific basis functions exemplified in Section
ref{fem:approx:global} are in general nonzero on the entire domain
$\Omega$, as can be seen in Figure ref{fem:approx:fe:fig:u:sin}, where
we plot two sinusoidal basis functions $\psi_0(x)=\sin\half\pi x$ and
$\psi_1(x)=\sin 2\pi x$ together with the sum $u(x)=4\psi_0(x) -
\half\psi_1(x)$. We shall now turn our attention to basis functions
that have *compact support*, meaning that they are nonzero on a small
portion of $\Omega$ only. Moreover, we shall restrict the functions to
be *piecewise polynomials*. This means that the domain is split into
subdomains and each basis function is a polynomial on one or more of
these subdomains, see Figure ref{fem:approx:fe:fig:u:fe} for a sketch
involving locally defined hat functions that make
$u=\sum_jc_j\baspsi_j$ piecewise linear. At the boundaries between
subdomains, one normally just forces continuity of $u$, so that when
connecting two polynomials from two subdomains, the derivative becomes
discontinuous. This type of basis functions is fundamental in the
finite element method.  (One may wonder why continuity of derivatives
is not desired, and it is, but it turns out to be mathematically
challenging in 2D and 3D, and it is not strictly needed.)

FIGURE: [fig/u_example_sin, width=600] A function resulting from a weighted sum of two sine basis functions. label{fem:approx:fe:fig:u:sin}

FIGURE: [fig/u_example_P1, width=600] A function resulting from a weighted sum of three local piecewise linear (hat) functions. label{fem:approx:fe:fig:u:fe}

We first introduce the concepts of elements and nodes in a simplistic fashion.
Later, we shall generalize the concept
of an element, which is a necessary step before treating a wider class of
approximations within the family of finite element methods.
The generalization is also compatible with
the concepts used in the "FEniCS": "http://fenicsproject.org" finite
element software.

===== Elements and nodes =====
label{fem:approx:fe:def:elements:nodes}

Let $u$ and $f$ be defined on an interval $\Omega$. We divide $\Omega$
into $N_e$ non-overlapping subintervals $\Omega^{(e)}$,
$e=0,\ldots,N_e-1$:

!bt
\begin{equation}
\Omega = \Omega^{(0)}\cup \cdots \cup \Omega^{(N_e)}\tp  \end{equation}
!et
We shall for now refer to $\Omega^{(e)}$ as an *element*, identified
by the unique number $e$.  On each element we introduce a set of
points called *nodes*.  For now we assume that the nodes are uniformly
spaced throughout the element and that the boundary points of the
elements are also nodes.  The nodes are given numbers both within an
element and in the global domain. These are referred to as *local* and
*global* node numbers, respectively.  Local nodes are numbered with an
index $r=0,\ldots,d$, while the $N_n$ global nodes are numbered as
$i=0,\ldots,N_n-1$.  Figure
ref{fem:approx:fe:def:elements:nodes:fig:P1} shows nodes as small
circular disks and element boundaries as small vertical lines.  Global
node numbers appear under the nodes, but local node numbers are not
shown. Since there are two nodes in each elements, the local nodes are
numbered 0 (left) and 1 (right) in each element.

FIGURE: [fig/fe_mesh1D_P1, width=500 frac=0.7] Finite element mesh with 5 elements and 6 nodes. label{fem:approx:fe:def:elements:nodes:fig:P1}

idx{finite element mesh}idx{mesh!finite elements}

Nodes and elements uniquely define a *finite element mesh*, which is
our discrete representation of the domain in the computations.  A
common special case is that of a *uniformly partitioned mesh* where
each element has the same length and the distance between nodes is
constant.  Figure ref{fem:approx:fe:def:elements:nodes:fig:P1} shows
an example on a uniformly partitioned mesh. The strength of the finite
element method (in contrast to the finite difference method) is that
it is equally easy to work with a non-uniformly partitioned mesh as a
uniformly partitioned one.

=== Example ===

On $\Omega =[0,1]$ we may introduce two elements,
$\Omega^{(0)}=[0,0.4]$ and $\Omega^{(1)}=[0.4,1]$. Furthermore, let us
introduce three nodes per element, equally spaced within each element.
Figure ref{fem:approx:fe:def:elements:nodes:fig:P2} shows the mesh
with $N_e=2$ elements and $N_n=2N_e+1=5$ nodes.  A node's coordinate
is denoted by $x_i$, where $i$ is either a global node number or a
local one. In the latter case we also need to know the element number
to uniquely define the node.

The three nodes in element number 0 are $x_0=0$, $x_1=0.2$, and
$x_2=0.4$.  The local and global node numbers are here equal.  In
element number 1, we have the local nodes $x_0=0.4$, $x_1=0.7$, and
$x_2=1$ and the corresponding global nodes $x_2=0.4$, $x_3=0.7$, and
$x_4=1$. Note that the global node $x_2=0.4$ is shared by the two
elements.

FIGURE: [fig/fe_mesh1D_P2, width=500 frac=0.7] Finite element mesh with 2 elements and 5 nodes. label{fem:approx:fe:def:elements:nodes:fig:P2}

For the purpose of implementation, we introduce two lists or arrays:
`nodes` for storing the coordinates of the nodes, with the global node
numbers as indices, and `elements` for holding the global node numbers
in each element. By defining `elements` as a list of lists, where each
sublist contains the global node numbers of one particular element,
the indices of each sublist will correspond to local node numbers for
that element.  The `nodes` and `elements` lists for the sample mesh
above take the form

!bc pycod
nodes = [0, 0.2, 0.4, 0.7, 1]
elements = [[0, 1, 2], [2, 3, 4]]
!ec
Looking up the coordinate of, e.g., local node number 2 in element 1,
is done by `nodes[elements[1][2]]` (recall that nodes and
elements start their numbering at 0). The corresponding global node number
is 4, so we could alternatively look up the coordinate as `nodes[4]`.

The numbering of elements and nodes does not need to be regular.
Figure ref{fem:approx:fe:def:elements:nodes:fig:P1:irregular} shows
and example corresponding to

!bc pycod
nodes = [1.5, 5.5, 4.2, 0.3, 2.2, 3.1]
elements = [[2, 1], [4, 5], [0, 4], [3, 0], [5, 2]]
!ec

FIGURE: [fig/fe_mesh1D_random_numbering, width=500 frac=0.7] Example on irregular numbering of elements and nodes. label{fem:approx:fe:def:elements:nodes:fig:P1:irregular}

===== The basis functions =====

=== Construction principles ===

Finite element basis functions are in this text recognized by
the notation $\basphi_i(x)$, where the index (now in the beginning)
corresponds to
a global node number. Since $\baspsi_i$ is the symbol for basis
functions in general in this text, the particular choice of
finite element basis functions means that we take
$\baspsi_i = \basphi_i$.


idx{internal node}
idx{shared node}

Let $i$ be the global node number corresponding to local node $r$ in
element number $e$ with $d+1$ local nodes. We distinguish between
*internal* nodes in an element and *shared* nodes. The latter are
nodes that are shared with the neighboring elements.
The finite element basis functions $\basphi_i$
are now defined as follows.

  * For an internal node, with global number $i$ and local number $r$,
    take $\basphi_i(x)$ to be the Lagrange
    polynomial that is 1 at the local node $r$ and zero
    at all other nodes in the element.
    The degree of the polynomial is $d$, according to (ref{fem:approx:global:Lagrange:poly}).
    On all other elements, $\basphi_i=0$.
  * For a shared node,
    let $\basphi_i$ be made up of the Lagrange polynomial on this element
    that is 1 at node $i$, combined with the Lagrange polynomial over
    the neighboring element that is also 1 at node $i$.
    On all other elements, $\basphi_i=0$.

A visual impression of three such basis functions is given in
Figure ref{fem:approx:fe:fig:P2}. When solving differential equations,
we need the derivatives of these basis functions as well, and the
corresponding derivatives are shown in Figure refref{fem:approx:fe:fig:dP2}.
Note that the derivatives are highly discontinuous!
In these figures,
the domain $\Omega = [0,1]$ is divided into four equal-sized elements,
each having three local nodes.
The element boundaries are
marked by vertical dashed lines and the nodes by small circles.
The function $\basphi_2(x)$
is composed of a quadratic Lagrange polynomial over element 0 and 1,
$\basphi_3(x)$ corresponds to an internal node in element 1 and
is therefore nonzero on this element only, while $\basphi_4(x)$
is like $\basphi_2(x)$ composed to two Lagrange polynomials over two
elements. Also observe that the basis function $\basphi_i$ is zero at
all nodes, except at global node number $i$.
We also remark that
the shape of a basis function over an element is completely determined
by the coordinates of the local nodes in the element.

#Sometimes we refer to a Lagrange polynomial on an element $e$, which
#means the basis function $\basphi_i(x)$ when $x\in\Omega^{(e)}$, and
#$\basphi_i(x)=0$ when $x\notin\Omega^{(e)}$.


FIGURE: [fig/phi/mpl_fe_basis_p2_4e_lab, width=600 frac=1]  Illustration of the piecewise quadratic basis functions associated with nodes in an element.  label{fem:approx:fe:fig:P2}

FIGURE: [fig/phi/mpl_fe_dbasis_p2_4e_lab, width=600 frac=1]  Illustration of the derivatives of piecewise quadratic basis functions associated with nodes in an element.  label{fem:approx:fe:fig:dP2}

=== Properties of $\basphi_i$ ===

The construction of basis functions according to the principles above
lead to two important properties of $\basphi_i(x)$. First,

idx{Kronecker delta}

!bt
\begin{equation}
\basphi_i(\xno{j}) =\delta_{ij},\quad \delta_{ij} =
\left\lbrace\begin{array}{ll}
1, & i=j,\\
0, & i\neq j,
\end{array}\right.
label{fem:approx:fe:phi:prop1}
\end{equation}
!et
when $\xno{j}$ is a node in the mesh with global node number $j$.
The
result $\basphi_i(\xno{j}) =\delta_{ij}$ is obtained as
the Lagrange polynomials are constructed to have
exactly this property.
The property also implies a convenient interpretation of $c_i$
as the value of $u$ at node $i$. To show this, we expand $u$
in the usual way as $\sum_jc_j\baspsi_j$ and choose $\baspsi_i = \basphi_i$:

!bt
\[
u(\xno{i}) = \sum_{j\in\If} c_j \baspsi_j (\xno{i}) =
\sum_{j\in\If} c_j \basphi_j (\xno{i}) = c_i \basphi_i (\xno{i}) = c_i
\tp
\]
!et
Because of this interpretation,
the coefficient $c_i$ is by many named $u_i$ or $U_i$.

# 2DO: switch to U_j?

Second,
$\basphi_i(x)$ is mostly zero throughout the domain:

 * $\basphi_i(x) \neq 0$ only on those elements that contain global node $i$,
 * $\basphi_i(x)\basphi_j(x) \neq 0$ if and only if $i$ and $j$ are global node
   numbers in the same element.

Since $A_{i,j}$ is the integral of
$\basphi_i\basphi_j$ it means that
*most of the elements in the coefficient matrix will be zero*.
We will come back to these properties and use
them actively in computations to save memory and CPU time.

In our example so far, each element has $d+1$ nodes, resulting in
local Lagrange polynomials of degree $d$ (according to Section
ref{fem:approx:global:Lagrange}), but it is not a requirement to have
the same $d$ value in each element.

===== Example on quadratic finite element functions =====

Let us set up the `nodes` and `elements` lists corresponding to the
mesh implied by Figure ref{fem:approx:fe:fig:P2}.
Figure ref{fem:approx:fe:fig:P2:mesh} sketches the mesh and the
numbering. We have

!bc pycod
nodes = [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]
elements = [[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]]
!ec


FIGURE: [fig/fe_mesh1D_P2_4e, width=500 frac=0.7]  Sketch of mesh with 4 elements and 3 nodes per element.  label{fem:approx:fe:fig:P2:mesh}


Let us explain mathematically how the basis functions are constructed
according to the principles.
Consider element number 1 in Figure ref{fem:approx:fe:fig:P2:mesh},
$\Omega^{(1)}=[0.25, 0.5]$, with local nodes
0, 1, and 2 corresponding to global nodes 2, 3, and 4.
The coordinates of these nodes are
$0.25$, $0.375$, and $0.5$, respectively.
We define three Lagrange
polynomials on this element:

 o The polynomial that is 1 at local node 1
   (global node 3) makes up the basis function
   $\basphi_3(x)$ over this element,
   with $\basphi_3(x)=0$ outside the element.
 o The polynomial that is 1 at local node 0 (global node 2) is the ``right
   part'' of the global basis function
   $\basphi_2(x)$. The ``left part'' of $\basphi_2(x)$ consists of
   a Lagrange polynomial associated with local node 2 in
   the neighboring element $\Omega^{(0)}=[0, 0.25]$.
 o Finally, the polynomial that is 1 at local node 2 (global node 4)
   is the ``left part'' of the global basis function $\basphi_4(x)$.
   The ``right part'' comes from the Lagrange polynomial that is 1 at
   local node 0 in the neighboring element $\Omega^{(2)}=[0.5, 0.75]$.

The specific mathematical form of the polynomials *over element* 1 is
given by the formula (ref{fem:approx:global:Lagrange:poly}):

!bt
\begin{align*}
\basphi_3(x) &= \frac{(x-0.25)(x-0.5)}{(0.375-0.25)(0.375-0.5)},\quad x\in\Omega^{(1)}\\
\basphi_2(x) &= \frac{(x-0.375)(x-0.5)}{(0.25-0.375)(0.25-0.5)},\quad x\in\Omega^{(1)}\\
\basphi_4(x) &= \frac{(x-0.25)(x-0.375)}{(0.5-0.25)(0.5-0.25)},\quad x\in\Omega^{(1)}
\end{align*}
!et

As mentioned earlier, any global basis function $\basphi_i(x)$ is zero
on elements that do not contain the node with global node number $i$.
Clearly, the property (ref{fem:approx:fe:phi:prop1}) is easily
verified, see for instance that $\basphi_3(0.375) = 1$ while
$\basphi_3(0.25) = 0$ and $\basphi_3(0.5) = 0$.

The other global functions associated with internal nodes,
$\basphi_1$, $\basphi_5$, and $\basphi_7$, are all of the same shape
as the drawn $\basphi_3$ in Figure ref{fem:approx:fe:fig:P2}, while
the global basis functions associated with shared nodes have the same
shape as shown $\basphi_2$ and $\basphi_4$. If the elements were of
different length, the basis functions would be stretched according to
the element size and hence be different.

#This was difficult to follow:
#The basis function $\basphi_2(x)$, corresponding to a node on the
#boundary of element 0 and 1, is made up of two pieces: (i) the Lagrange
#polynomial on element 1 that is 1 at local node 0 (global node 2)
#and zero at all other nodes in element 1, and (ii)
#the Lagrange
#polynomial on element 1 that is 1 at local node 2 (global node 2)
#and zero at all other nodes in element 0. Outside the elements that
#share global node 2, $\basphi_2(x)=0$. The same reasoning is applied to
#the construction of $\basphi_4(x)$ and $\basphi_6(x)$.


===== Example on linear finite element functions =====


Figure ref{fem:approx:fe:fig:P1} shows
piecewise linear basis functions ($d=1$) (with derivatives in
Figure ref{fem:approx:fe:fig:dP1}). These are mathematically
simpler than the quadratic functions in the previous section, and one would
therefore think that it is easier to understand the linear functions
first. However, linear basis functions do not involve internal nodes
and are therefore a special case of the general situation. That is why
we think it is better to understand the construction of quadratic functions
first, which easily generalize to any $d > 2$, and then look at the
special case $d=1$.

FIGURE: [fig/phi/mpl_fe_basis_p1_4e_lab, width=600 frac=1]  Illustration of the piecewise linear basis functions associated with nodes in an element.  label{fem:approx:fe:fig:P1}

FIGURE: [fig/phi/mpl_fe_dbasis_p1_4e_lab, width=600 frac=1]  Illustration of the derivatives of piecewise linear basis functions associated with nodes in an element.  label{fem:approx:fe:fig:dP1}


We have the same four elements on $\Omega = [0,1]$.  Now there are no
internal nodes in the elements so that all basis functions are
associated with shared nodes and hence made up of two Lagrange
polynomials, one from each of the two neighboring elements.  For
example, $\basphi_1(x)$ results from the Lagrange polynomial in
element 0 that is 1 at local node 1 and 0 at local node 0, combined
with the Lagrange polynomial in element 1 that is 1 at local node 0
and 0 at local node 1.  The other basis functions are constructed
similarly.

Explicit mathematical formulas are needed for $\basphi_i(x)$ in
computations.  In the piecewise linear case, the formula
(ref{fem:approx:global:Lagrange:poly}) leads to

!bt
\begin{equation}
\basphi_i(x) = \left\lbrace\begin{array}{ll}
0, & x < \xno{i-1},\\
(x - \xno{i-1})/(\xno{i} - \xno{i-1}),
& \xno{i-1} \leq x < \xno{i},\\
1 -
(x - x_{i})/(\xno{i+1} - x_{i}),
& \xno{i} \leq x < \xno{i+1},\\
0, & x\geq \xno{i+1}\tp  \end{array}
\right.
label{fem:approx:fe:phi:1:formula1}
\end{equation}
!et
Here, $\xno{j}$, $j=i-1,i,i+1$, denotes the coordinate of node $j$.
For elements of equal length $h$ the formulas can be simplified to

!bt
\begin{equation}
\basphi_i(x) = \left\lbrace\begin{array}{ll}
0, & x < \xno{i-1},\\
(x - \xno{i-1})/h,
& \xno{i-1} \leq x < \xno{i},\\
1 -
(x - x_{i})/h,
& \xno{i} \leq x < \xno{i+1},\\
0, & x\geq \xno{i+1}
\end{array}
\right.
label{fem:approx:fe:phi:1:formula2}
\end{equation}
!et


===== Example on cubic finite element functions =====

Piecewise cubic basis functions can be defined by introducing four
nodes per element. Figure ref{fem:approx:fe:fig:P3} shows
examples on $\basphi_i(x)$, $i=3,4,5,6$, associated with element number 1.
Note that $\basphi_4$ and $\basphi_5$ are nonzero only on element number 1,
while
$\basphi_3$ and $\basphi_6$ are made up of Lagrange polynomials on two
neighboring elements.

FIGURE: [fig/phi/mpl_fe_basis_p3_4e_lab, width=600 frac=1]  Illustration of the piecewise cubic basis functions associated with nodes in an element.  label{fem:approx:fe:fig:P3}

idx{chapeau function} idx{hat function} idx{finite element basis function}

We see that all the piecewise linear basis functions have the same
``hat'' shape. They are naturally referred to as *hat functions*,
also called *chapeau functions*.
The piecewise quadratic functions in Figure ref{fem:approx:fe:fig:P2}
are seen to be of two types. ``Rounded hats'' associated with internal
nodes in the elements and some more ``sombrero'' shaped hats associated
with element boundary nodes. Higher-order basis functions also have
hat-like shapes, but the functions have pronounced oscillations in addition,
as illustrated in Figure ref{fem:approx:fe:fig:P3}.

idx{linear elements} idx{quadratic elements} idx{P1 element} idx{P2 element}

A common terminology is to speak about *linear elements* as
elements with two local nodes associated with
piecewise linear basis functions. Similarly, *quadratic elements* and
*cubic elements* refer to piecewise quadratic or cubic functions
over elements with three or four local nodes, respectively.
Alternative names, frequently used in the following, are P1 elements for linear
elements, P2 for quadratic elements, and so forth: P$d$ signifies
degree $d$ of the polynomial basis functions.


===== Calculating the linear system =====
label{fem:approx:global:linearsystem}

The elements in the coefficient matrix and right-hand side are given
by the formulas (ref{fem:approx:Aij}) and (ref{fem:approx:bi}), but
now the choice of $\baspsi_i$ is $\basphi_i$.  Consider P1 elements
where $\basphi_i(x)$ is piecewise linear.  Nodes and elements numbered
consecutively from left to right in a uniformly partitioned mesh imply
the nodes

!bt
\[ x_i=i h,\quad i=0,\ldots,N_n-1,\]
!et
and the elements

!bt
\begin{equation}
\Omega^{(i)} = [\xno{i},\xno{i+1}] = [i h, (i+1)h],\quad
i=0,\ldots,N_e-1
\tp
\end{equation}
!et
We have in this case $N_e$ elements and $N_n=N_e+1$ nodes.
The parameter $N$ denotes the number of unknowns in the expansion
for $u$, and with the P1 elements, $N=N_n$.
The domain is $\Omega=[\xno{0},\xno{N}]$.
The formula for $\basphi_i(x)$ is given by
(ref{fem:approx:fe:phi:1:formula2}) and a graphical illustration is
provided in Figures ref{fem:approx:fe:fig:P1} and
ref{fem:approx:fe:fig:phi:i:im1}.


#We clearly see
#from the figures the very important property
#$\basphi_i(x)\basphi_j(x)\neq 0$ if and only if $j=i-1$, $j=i$, or
#$j=i+1$, or alternatively expressed, if and only if $i$ and $j$ are
#nodes in the same element. Otherwise, $\basphi_i$ and $\basphi_j$ are
#too distant to have an overlap and consequently their product vanishes.

FIGURE: [fig/fe_mesh1D_phi_2_3, width=500 frac=0.7]  Illustration of the piecewise linear basis functions corresponding to global node 2 and 3.  label{fem:approx:fe:fig:phi:2:3}

=== Calculating specific matrix entries ===

Let us calculate the specific matrix entry $A_{2,3} = \int_\Omega
\basphi_2\basphi_3\dx$. Figure ref{fem:approx:fe:fig:phi:2:3}
shows what $\basphi_2$ and $\basphi_3$ look like. We realize
from this figure that the product $\basphi_2\basphi_3\neq 0$
only over element 2, which contains node 2 and 3.
The particular formulas for $\basphi_{2}(x)$ and $\basphi_3(x)$ on
$[\xno{2},\xno{3}]$ are found from (ref{fem:approx:fe:phi:1:formula2}).
The function
$\basphi_3$ has positive slope over $[\xno{2},\xno{3}]$ and corresponds
to the interval $[\xno{i-1},\xno{i}]$ in
(ref{fem:approx:fe:phi:1:formula2}). With $i=3$ we get

!bt
\[ \basphi_3(x) = (x-x_2)/h,\]
!et
while $\basphi_2(x)$ has negative slope over $[\xno{2},\xno{3}]$
and corresponds to setting $i=2$ in (ref{fem:approx:fe:phi:1:formula2}),

!bt
\[ \basphi_2(x) = 1- (x-x_2)/h\tp\]
!et
We can now easily integrate,

!bt
\[
A_{2,3} = \int_\Omega \basphi_2\basphi_{3}\dx =
\int_{\xno{2}}^{\xno{3}}
\left(1 - \frac{x - \xno{2}}{h}\right) \frac{x - x_{2}}{h}
 \dx = \frac{h}{6}\tp
\]
!et

The diagonal entry in the coefficient matrix becomes

!bt
\[ A_{2,2} =
\int_{\xno{1}}^{\xno{2}}
\left(\frac{x - \xno{1}}{h}\right)^2\dx +
\int_{\xno{2}}^{\xno{3}}
\left(1 - \frac{x - \xno{2}}{h}\right)^2\dx
= \frac{2h}{3}\tp
\]
!et
The entry $A_{2,1}$ has an
integral that is geometrically similar to the situation in
Figure ref{fem:approx:fe:fig:phi:2:3}, so we get
$A_{2,1}=h/6$.


=== Calculating a general row in the matrix ===

We can now generalize the calculation of matrix entries to
a general row number $i$. The entry
$A_{i,i-1}=\int_\Omega\basphi_i\basphi_{i-1}\dx$ involves
hat functions as depicted in
Figure ref{fem:approx:fe:fig:phi:i:im1}. Since the integral
is geometrically identical to the situation with specific nodes
2 and 3, we realize that $A_{i,i-1}=A_{i,i+1}=h/6$ and
$A_{i,i}=2h/3$. However, we can compute the integral directly
too:

!bt
\begin{align*}
A_{i,i-1} &= \int_\Omega \basphi_i\basphi_{i-1}\dx\\
&=
\underbrace{\int_{\xno{i-2}}^{\xno{i-1}} \basphi_i\basphi_{i-1}\dx}_{\basphi_i=0} +
\int_{\xno{i-1}}^{\xno{i}} \basphi_i\basphi_{i-1}\dx +
\underbrace{\int_{\xno{i}}^{\xno{i+1}} \basphi_i\basphi_{i-1}\dx}_{\basphi_{i-1}=0}\\
&= \int_{\xno{i-1}}^{\xno{i}}
\underbrace{\left(\frac{x - x_{i}}{h}\right)}_{\basphi_i(x)}
\underbrace{\left(1 - \frac{x - \xno{i-1}}{h}\right)}_{\basphi_{i-1}(x)} \dx =
\frac{h}{6}
\tp
\end{align*}
!et
The particular formulas for $\basphi_{i-1}(x)$ and $\basphi_i(x)$ on
$[\xno{i-1},\xno{i}]$ are found from (ref{fem:approx:fe:phi:1:formula2}):
$\basphi_i$ is the linear function with positive slope, corresponding
to the interval $[\xno{i-1},\xno{i}]$ in
(ref{fem:approx:fe:phi:1:formula2}), while $\phi_{i-1}$ has a
negative slope so the definition in interval
$[\xno{i},\xno{i+1}]$ in (ref{fem:approx:fe:phi:1:formula2}) must be
used.

FIGURE: [fig/fe_mesh1D_phi_i_im1, width=500 frac=0.7]  Illustration of two neighboring linear (hat) functions with general node numbers.  label{fem:approx:fe:fig:phi:i:im1}


The first and last row of the coefficient matrix lead to slightly
different integrals:

!bt
\[ A_{0,0} = \int_\Omega \basphi_0^2\dx = \int_{\xno{0}}^{\xno{1}}
\left(1 - \frac{x-x_0}{h}\right)^2\dx = \frac{h}{3}\tp
\]
!et
Similarly, $A_{N,N}$ involves an integral over only one element
and hence equals $h/3$.

FIGURE: [fig/fe_mesh1D_phi_i_f, width=500 frac=0.7]  Right-hand side integral with the product of a basis function and the given function to approximate.  label{fem:approx:fe:fig:phi:i:f}


The general formula for $b_i$,
see Figure ref{fem:approx:fe:fig:phi:i:f}, is now easy to set up

!bt
\begin{equation}
b_i = \int_\Omega\basphi_i(x)f(x)\dx
= \int_{\xno{i-1}}^{\xno{i}} \frac{x - \xno{i-1}}{h} f(x)\dx
+ \int_{x_{i}}^{\xno{i+1}} \left(1 - \frac{x - x_{i}}{h}\right) f(x)
\dx\tp
label{fem:approx:fe:bi:formula1}
\end{equation}
!et
We remark that the above formula applies to internal nodes (living at the interface between two elements)
and that for the nodes on the boundaries only one integral needs to be computed.

We need a specific $f(x)$ function to compute these integrals.
With $f(x)=x(1-x)$ and
two equal-sized elements in $\Omega=[0,1]$, one gets

!bt
\begin{equation*}
A = \frac{h}{6}\left(\begin{array}{ccc}
2 & 1 & 0\\
1 & 4 & 1\\
0 & 1 & 2
\end{array}\right),\quad
b = \frac{h^2}{12}\left(\begin{array}{c}
2 - h\\
12 - 14h\\
10 -17h
\end{array}\right)\tp
\end{equation*}
!et
The solution becomes

!bt
\begin{equation*} c_0 = \frac{h^2}{6},\quad c_1 = h - \frac{5}{6}h^2,\quad
c_2 = 2h - \frac{23}{6}h^2\tp  \end{equation*}
!et
The resulting function

!bt
\begin{equation*} u(x)=c_0\basphi_0(x) + c_1\basphi_1(x) + c_2\basphi_2(x)\end{equation*}
!et
is displayed in Figure ref{fem:approx:fe:fig:ls:P1:2:4} (left).
Doubling the number of elements to four leads to the improved
approximation in the right part of Figure ref{fem:approx:fe:fig:ls:P1:2:4}.

FIGURE: [fig/fe_p1_x2_2e_4e, width=800 frac=1.0]  Least squares approximation of a parabola using 2 (left) and 4 (right) P1 elements. label{fem:approx:fe:fig:ls:P1:2:4}



===== Assembly of elementwise computations =====
label{fem:approx:fe:elementwise}

Our integral computations so far have been straightforward. However,
with higher-degree polynomials and in higher dimensions (2D and 3D),
integrating in the physical domain gets increasingly complicated. Instead,
integrating over one element at a time, and transforming each element
to a common standardized geometry in a new reference coordinate system,
is technically easier. Almost all computer codes employ a finite element
algorithm that calculates the linear system by integrating over one
element at a time. We shall therefore explain this algorithm next.
The amount of details might be overwhelming during a first reading, but
once all those details are done right, one has a general
finite element algorithm that can be applied to all sorts of elements,
in any space dimension, no matter how geometrically complicated the domain
is.

idx{element matrix}

=== The element matrix ===

We start by splitting
the integral over $\Omega$ into a sum of contributions from
each element:

!bt
\begin{equation}
A_{i,j} = \int_\Omega\basphi_i\basphi_j \dx = \sum_{e} A^{(e)}_{i,j},\quad
A^{(e)}_{i,j}=\int_{\Omega^{(e)}} \basphi_i\basphi_j \dx
\tp
label{fem:approx:fe:elementwise:Asplit}
\end{equation}
!et
Now, $A^{(e)}_{i,j}\neq 0$, if and only if, $i$ and $j$ are nodes in element
$e$ (look at Figure ref{fem:approx:fe:fig:phi:i:im1} to realize this
property, but the result also holds for all types of elements).
Introduce $i=q(e,r)$ as the mapping of local node number $r$ in element
$e$ to the global node number $i$. This is just a short mathematical notation
for the expression `i=elements[e][r]` in a program.
Let $r$ and $s$ be the local node numbers corresponding to the global
node numbers $i=q(e,r)$ and
$j=q(e,s)$. With $d+1$ nodes per element, all the nonzero matrix entries
in $A^{(e)}_{i,j}$ arise from the integrals involving basis functions with
indices corresponding to the global node numbers in element number $e$:


!bt
\begin{equation*}
\int_{\Omega^{(e)}}\basphi_{q(e,r)}\basphi_{q(e,s)} \dx,
\quad r,s=0,\ldots, d\tp
\end{equation*}
!et
These contributions can be collected in a $(d+1)\times (d+1)$ matrix known as
the *element matrix*. Let $\Ifd=\{0,\ldots,d\}$ be the valid indices
of $r$ and $s$.
We introduce the notation

!bt
\begin{equation*}
\tilde A^{(e)} = \{ \tilde A^{(e)}_{r,s}\},\quad
r,s\in\Ifd,
\end{equation*}
!et
for the element matrix. For P1 elements ($d=2$) we have

!bt
\begin{equation*}
\tilde A^{(e)} = \left\lbrack\begin{array}{ll}
\tilde A^{(e)}_{0,0} & \tilde A^{(e)}_{0,1}\\
\tilde A^{(e)}_{1,0} & \tilde A^{(e)}_{1,1}
\end{array}\right\rbrack
\tp
\end{equation*}
!et
while P2 elements have a $3\times 3$ element matrix:

!bt
\begin{equation*}
\tilde A^{(e)} = \left\lbrack\begin{array}{lll}
\tilde A^{(e)}_{0,0} & \tilde A^{(e)}_{0,1} & \tilde A^{(e)}_{0,2}\\
\tilde A^{(e)}_{1,0} & \tilde A^{(e)}_{1,1} & \tilde A^{(e)}_{1,2}\\
\tilde A^{(e)}_{2,0} & \tilde A^{(e)}_{2,1} & \tilde A^{(e)}_{2,2}
\end{array}\right\rbrack
\tp
\end{equation*}
!et

=== Assembly of element matrices ===

Given the numbers $\tilde A^{(e)}_{r,s}$,
we should, according to (ref{fem:approx:fe:elementwise:Asplit}),
add the contributions to the global coefficient matrix by

idx{assembly}

!bt
\begin{equation}
 A_{q(e,r),q(e,s)} := A_{q(e,r),q(e,s)} + \tilde A^{(e)}_{r,s},\quad
r,s\in\Ifd\tp
\end{equation}
!et
This process of adding in elementwise contributions to the global matrix
is called *finite element assembly* or simply *assembly*.

Figure ref{fem:approx:fe:fig:assembly:2x2} illustrates how element matrices
for elements with two nodes are added into the global matrix.
More specifically, the figure shows how the element matrix associated with
elements 1 and 2 assembled, assuming that global nodes are numbered
from left to right in the domain. With regularly numbered P3 elements, where
the element matrices have size $4\times 4$, the assembly of elements 1 and 2
are sketched in Figure ref{fem:approx:fe:fig:assembly:4x4}.

FIGURE: [mov/fe_assembly_regular_2x2/fe_assembly_regular_2x2, width=500 frac=0.6]  Illustration of matrix assembly: regularly numbered P1 elements.  label{fem:approx:fe:fig:assembly:2x2}

FIGURE: [mov/fe_assembly_regular_4x4/fe_assembly_regular_4x4, width=500 frac=0.6]  Illustration of matrix assembly: regularly numbered P3 elements.  label{fem:approx:fe:fig:assembly:4x4}

=== Assembly of irregularly numbered elements and nodes ===

After assembly of element matrices corresponding to regularly numbered elements
and nodes are understood, it is wise to study the assembly process for
irregularly numbered elements and nodes. Figure ref{fem:approx:fe:def:elements:nodes:fig:P1:irregular} shows a mesh where the `elements` array, or $q(e,r)$
mapping in mathematical notation, is given as

!bc pycod
elements = [[2, 1], [4, 5], [0, 4], [3, 0], [5, 2]]
!ec
The associated assembly of element matrices 1 and 2 is sketched in
Figure ref{fem:approx:fe:fig:assembly:irr2x2}.

We have created "animations":
"${fem_doc}/mov/fe_assembly.html" to illustrate the assembly of
P1 and P3 elements with regular numbering as well as P1 elements with
irregular numbering. The reader is encouraged to develop a
``geometric'' understanding of how element matrix entries are added to
the global matrix. This understanding is crucial for hand
computations with the finite element method.

#"P1 assembly movie": "${fem_doc}/mov/fe_assembly_regular_2x2/index.html".
#"P3 assembly movie": "${fem_doc}/mov/fe_assembly_regular_4x4/index.html".
#"P1 irregular numbering": "${fem_doc}/mov/fe_assembly_irregular/index.html".


FIGURE: [mov/fe_assembly_irregular/fe_assembly_irregular, width=500 frac=0.6]  Illustration of matrix assembly: irregularly numbered P1 elements.  label{fem:approx:fe:fig:assembly:irr2x2}

#old:
#FIGURE: [fig/matrix-assembly, width=600]  Illustration of matrix assembly.  label{fem:approx:fe:fig:assembly}

=== The element vector ===

The right-hand side of the linear system is also computed elementwise:

!bt
\begin{equation}
b_i = \int_\Omega f(x)\basphi_i(x) \dx = \sum_{e} b^{(e)}_{i},\quad
b^{(e)}_{i}=\int_{\Omega^{(e)}} f(x)\basphi_i(x)\dx
\tp  \end{equation}
!et
We observe that
$b_i^{(e)}\neq 0$ if and only if global node $i$ is a node in element $e$
(look at Figure ref{fem:approx:fe:fig:phi:i:f} to realize this property).
With $d$ nodes per element we can collect the $d+1$ nonzero contributions
$b_i^{(e)}$, for $i=q(e,r)$, $r\in\Ifd$, in an *element vector*

!bt
\begin{equation*}
\tilde b_r^{(e)}=\{ \tilde b_r^{(e)}\},\quad r\in\Ifd\tp
\end{equation*}
!et
These contributions are added to the
global right-hand side by an assembly process similar to that for the
element matrices:

!bt
\begin{equation}
b_{q(e,r)} := b_{q(e,r)} + \tilde b^{(e)}_{r},\quad
r\in\Ifd\tp  \end{equation}
!et


===== Mapping to a reference element =====
label{fem:approx:fe:mapping}

idx{affine mapping} idx{mapping of reference cells!affine mapping}

Instead of computing the integrals

!bt
\begin{equation*} \tilde A^{(e)}_{r,s} = \int_{\Omega^{(e)}}\basphi_{q(e,r)}(x)\basphi_{q(e,s)}(x)\dx\end{equation*}
!et
over some element
$\Omega^{(e)} = [x_L, x_R]$ in the physical coordinate system,
it turns out that it is considerably easier and more convenient
to map the element domain $[x_L, x_R]$
to a standardized reference element domain $[-1,1]$ and compute all
integrals over the same domain $[-1,1]$.
We have now introduced
$x_L$ and $x_R$ as the left and right boundary points of an arbitrary element.
With a natural, regular numbering of nodes and elements from left to right
through the domain, we have $x_L=\xno{e}$ and $x_R=\xno{e+1}$ for P1 elements.

=== The coordinate transformation ===

Let $X\in [-1,1]$ be the coordinate
in the reference element. A linear mapping, also known as an affine mapping,
from $X$ to $x$ can be written

!bt
\begin{equation}
x = \half (x_L + x_R) + \half (x_R - x_L)X\tp
label{fem:approx:fe:affine:mapping}
\end{equation}
!et
This relation can alternatively be expressed as

!bt
\begin{equation}
x = x_m + {\half}hX,
label{fem:approx:fe:affine:mapping2}
\end{equation}
!et
where we have introduced the element midpoint $x_m=(x_L+x_R)/2$ and
the element length $h=x_R-x_L$.

=== Formulas for the element matrix and vector entries ===

Integrating on the reference element is a matter of just changing the
integration variable from $x$ to $X$. Let

!bt
\begin{equation}
\refphi_r(X) = \basphi_{q(e,r)}(x(X))
\end{equation}
!et
be the basis function associated with local node number $r$ in the
reference element. Switching from $x$ to $X$ as integration variable,
using the rules from calculus, results in

!bt
\begin{equation}
\tilde A^{(e)}_{r,s} =
\int_{\Omega^{(e)}}\basphi_{q(e,r)}(x)\basphi_{q(e,s)}(x)\dx
= \int_{-1}^1 \refphi_r(X)\refphi_s(X)\frac{\dx}{\dX}\dX
\tp
\end{equation}
!et

In 2D and 3D, $\dx$ is transformed to $\hbox{det} J\dX$, where $J$ is
the Jacobian of the mapping from $x$ to $X$. In 1D,
$\hbox{det} J\dX = \dx/\dX = h/2$. To obtain a uniform
notation for 1D, 2D, and 3D problems we therefore replace
$\dx/\dX$ by $\det J$ already now.
The integration over the reference element is then written as

!bt
\begin{equation}
\tilde A^{(e)}_{r,s}
= \int_{-1}^1 \refphi_r(X)\refphi_s(X)\det J\,\dX
label{fem:approx:fe:mapping:Ae}
\tp
\end{equation}
!et
The corresponding formula for the element vector entries becomes

!bt
\begin{equation}
\tilde b^{(e)}_{r} = \int_{\Omega^{(e)}}f(x)\basphi_{q(e,r)}(x)\dx
= \int_{-1}^1 f(x(X))\refphi_r(X)\det J\,\dX
label{fem:approx:fe:mapping:be}
\tp
\end{equation}
!et

!bnotice Why reference elements?
The great advantage of using reference elements is that
the formulas for the basis functions, $\refphi_r(X)$, are the
same for all elements and independent of the element geometry
(length and location in the mesh). The geometric information
is ``factored out'' in the simple mapping formula and the associated
$\det J$ quantity. Also, the integration domain is the same for
all elements. All these features contribute to simplify computer
codes and make them more general.
!enotice

#Since we from now on will work in the reference
#element, we need explicit mathematical formulas for the basis
#functions $\basphi_i(x)$ in the reference element only, i.e., we only need
#to specify formulas for $\refphi_r(X)$.
#This is a very convenient simplification compared to specifying
#piecewise polynomials in the physical domain.
#, and perhaps the primary
#reason why almost all computer codes
#integrate over reference elements and assemble element
#contributions.

=== Formulas for local basis functions ===

The $\refphi_r(x)$ functions are simply the Lagrange
polynomials defined through the local nodes in the reference element.
For $d=1$ and two nodes per element, we have the linear Lagrange
polynomials

!bt
\begin{align}
\refphi_0(X) &= \half (1 - X)
label{fem:approx:fe:mapping:P1:phi0}\\
\refphi_1(X) &= \half (1 + X)
label{fem:approx:fe:mapping:P1:phi1}
\end{align}
!et
Quadratic polynomials, $d=2$, have the formulas

!bt
\begin{align}
\refphi_0(X) &= \half (X-1)X
label{fem:approx:fe:mapping:P2:phi0}\\
\refphi_1(X) &= 1 - X^2
label{fem:approx:fe:mapping:P2:phi1}\\
\refphi_2(X) &= \half (X+1)X
label{fem:approx:fe:mapping:P2:phi2}
\end{align}
!et
In general,

!bt
\begin{equation}
\refphi_r(X) = \prod_{s=0,s\neq r}^d \frac{X-\Xno{s}}{\Xno{r}-\Xno{s}},
\end{equation}
!et
where $\Xno{0},\ldots,\Xno{d}$ are the coordinates of the local nodes in
the reference element.
These are normally uniformly spaced: $\Xno{r} = -1 + 2r/d$,
$r\in\Ifd$.



===== Example on integration over a reference element =====
label{fem:approx:fe:intg:ref}

To illustrate the concepts from the previous section in a specific
example, we now
consider calculation of the element matrix and vector for a specific choice of
$d$ and $f(x)$. A simple choice is $d=1$ (P1 elements) and $f(x)=x(1-x)$
on $\Omega =[0,1]$. We have the general expressions
(ref{fem:approx:fe:mapping:Ae}) and (ref{fem:approx:fe:mapping:be})
for $\tilde A^{(e)}_{r,s}$ and $\tilde b^{(e)}_{r}$.
Writing these out for the choices (ref{fem:approx:fe:mapping:P1:phi0})
and (ref{fem:approx:fe:mapping:P1:phi1}), and using that $\det J = h/2$,
we can do the following calculations of the element matrix entries:

!bt
\begin{align}
\tilde A^{(e)}_{0,0}
&= \int_{-1}^1 \refphi_0(X)\refphi_0(X)\frac{h}{2} \dX\nonumber\\
&=\int_{-1}^1 \half(1-X)\half(1-X) \frac{h}{2} \dX =
\frac{h}{8}\int_{-1}^1 (1-X)^2 \dX = \frac{h}{3},
label{fem:approx:fe:intg:ref:Ae00}\\
\tilde A^{(e)}_{1,0}
&= \int_{-1}^1 \refphi_1(X)\refphi_0(X)\frac{h}{2} \dX\nonumber\\
&=\int_{-1}^1 \half(1+X)\half(1-X) \frac{h}{2} \dX =
\frac{h}{8}\int_{-1}^1 (1-X^2) \dX = \frac{h}{6},
label{fem:approx:fe:intg:ref:Ae10}\\
\tilde A^{(e)}_{0,1} &= \tilde A^{(e)}_{1,0},\\
\tilde A^{(e)}_{1,1}
&= \int_{-1}^1 \refphi_1(X)\refphi_1(X)\frac{h}{2} \dX\nonumber\\
&=\int_{-1}^1 \half(1+X)\half(1+X) \frac{h}{2} \dX =
\frac{h}{8}\int_{-1}^1 (1+X)^2 \dX = \frac{h}{3}
label{fem:approx:fe:intg:ref:Ae11}
\tp
\end{align}
!et

The corresponding entries in the element vector becomes
using (ref{fem:approx:fe:affine:mapping2}))

!bt
\begin{align}
\tilde b^{(e)}_{0}
&= \int_{-1}^1 f(x(X))\refphi_0(X)\frac{h}{2} \dX\nonumber\\
&= \int_{-1}^1 (x_m + \half hX)(1-(x_m + \half hX))
\half(1-X)\frac{h}{2} \dX \nonumber\\
&= - \frac{1}{24} h^{3} + \frac{1}{6} h^{2} x_{m} - \frac{1}{12} h^{2} - \half h x_{m}^{2} + \half h x_{m}
label{fem:approx:fe:intg:ref:be0}\\
\tilde b^{(e)}_{1}
&= \int_{-1}^1 f(x(X))\refphi_1(X)\frac{h}{2} \dX\nonumber\\
&= \int_{-1}^1 (x_m + \half hX)(1-(x_m + \half hX))
\half(1+X)\frac{h}{2} \dX \nonumber\\
&= - \frac{1}{24} h^{3} - \frac{1}{6} h^{2} x_{m} + \frac{1}{12} h^{2} -
\half h x_{m}^{2} + \half h x_{m}
\tp
\end{align}
!et
In the last two expressions we have used the element midpoint $x_m$.

Integration of lower-degree polynomials above is tedious,
and higher-degree polynomials involve much more algebra, but `sympy`
may help. For example, we can easily calculate
(ref{fem:approx:fe:intg:ref:Ae00}),
(ref{fem:approx:fe:intg:ref:Ae10}),
and (ref{fem:approx:fe:intg:ref:be0}) by

!bc ipy
>>> import sympy as sym
>>> x, x_m, h, X = sym.symbols('x x_m h X')
>>> sym.integrate(h/8*(1-X)**2, (X, -1, 1))
h/3
>>> sym.integrate(h/8*(1+X)*(1-X), (X, -1, 1))
h/6
>>> x = x_m + h/2*X
>>> b_0 = sym.integrate(h/4*x*(1-x)*(1-X), (X, -1, 1))
>>> print(b_0)
-h**3/24 + h**2*x_m/6 - h**2/12 - h*x_m**2/2 + h*x_m/2
!ec

# #ifdef EXTRA
# Svein thought this was a digression here...
For inclusion of formulas in documents (like the present one), `sympy` can print
expressions in LaTeX format:

!bc ipy
>>> print(sym.latex(b_0, mode='plain'))
- \frac{1}{24} h^{3} + \frac{1}{6} h^{2} x_{m}
- \frac{1}{12} h^{2} - \half h x_{m}^{2}
+ \half h x_{m}
!ec
# #endif

!split
======= Implementation =======
label{fem:approx:fe:impl}

Based on the experience from the previous example, it makes sense to
write some code to automate the analytical integration process for any
choice of finite element basis functions. In addition, we can automate
the assembly process and the solution of the linear system.  Another
advantage is that the code for these purposes document all details of
all steps in the finite element computational machinery.  The complete
code can be found in the module file "`fe_approx1D.py`":
"${fem_src}/fe_approx1D.py".


===== Integration =====
label{fem:approx:fe:impl:intg}

First we need a Python function for
defining $\refphi_r(X)$ in terms of a Lagrange polynomial
of degree `d`:

!bc pycod
import sympy as sym
import numpy as np

def basis(d, point_distribution='uniform', symbolic=False):
    """
    Return all local basis function phi as functions of the
    local point X in a 1D element with d+1 nodes.
    If symbolic=True, return symbolic expressions, else
    return Python functions of X.
    point_distribution can be 'uniform' or 'Chebyshev'.
    """
    X = sym.symbols('X')
    if d == 0:
        phi_sym = [1]
    else:
        if point_distribution == 'uniform':
            if symbolic:
	        # Compute symbolic nodes
                h = sym.Rational(1, d)  # node spacing
                nodes = [2*i*h - 1 for i in range(d+1)]
            else:
                nodes = np.linspace(-1, 1, d+1)
        elif point_distribution == 'Chebyshev':
            # Just numeric nodes
            nodes = Chebyshev_nodes(-1, 1, d)

        phi_sym = [Lagrange_polynomial(X, r, nodes)
                   for r in range(d+1)]
    # Transform to Python functions
    phi_num = [sym.lambdify([X], phi_sym[r], modules='numpy')
               for r in range(d+1)]
    return phi_sym if symbolic else phi_num

def Lagrange_polynomial(x, i, points):
    p = 1
    for k in range(len(points)):
        if k != i:
            p *= (x - points[k])/(points[i] - points[k])
    return p
!ec
Observe how we construct the `phi_sym` list to be
symbolic expressions for $\refphi_r(X)$ with `X` as a
`Symbol` object from `sympy`. Also note that the
`Lagrange_polynomial` function (here simply copied
from Section ref{fem:approx:global:Fourier})
works with both symbolic and numeric variables.

Now we can write the function that computes the element matrix
with a list of symbolic expressions for $\basphi_r$
(`phi = basis(d, symbolic=True)`):

!bc pycod
def element_matrix(phi, Omega_e, symbolic=True):
    n = len(phi)
    A_e = sym.zeros(n, n)
    X = sym.Symbol('X')
    if symbolic:
        h = sym.Symbol('h')
    else:
        h = Omega_e[1] - Omega_e[0]
    detJ = h/2  # dx/dX
    for r in range(n):
        for s in range(r, n):
            A_e[r,s] = sym.integrate(phi[r]*phi[s]*detJ, (X, -1, 1))
            A_e[s,r] = A_e[r,s]
    return A_e
!ec
In the symbolic case (`symbolic` is `True`),
we introduce the element length as a symbol
`h` in the computations. Otherwise, the real numerical value
of the element interval `Omega_e`
is used and the final matrix elements are numbers,
not symbols.
This functionality can be demonstrated:

!bc ipy
>>> from fe_approx1D import *
>>> phi = basis(d=1, symbolic=True)
>>> phi
[-X/2 + 1/2, X/2 + 1/2]
>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=True)
[h/3, h/6]
[h/6, h/3]
>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=False)
[0.0333333333333333, 0.0166666666666667]
[0.0166666666666667, 0.0333333333333333]
!ec

The computation of the element vector is done by a similar
procedure:

!bc pycod
def element_vector(f, phi, Omega_e, symbolic=True):
    n = len(phi)
    b_e = sym.zeros(n, 1)
    # Make f a function of X
    X = sym.Symbol('X')
    if symbolic:
        h = sym.Symbol('h')
    else:
        h = Omega_e[1] - Omega_e[0]
    x = (Omega_e[0] + Omega_e[1])/2 + h/2*X  # mapping
    f = f.subs('x', x)  # substitute mapping formula for x
    detJ = h/2  # dx/dX
    for r in range(n):
        b_e[r] = sym.integrate(f*phi[r]*detJ, (X, -1, 1))
    return b_e
!ec
Here we need to replace the symbol `x` in the expression for `f`
by the mapping formula such that `f` can be integrated
in terms of $X$, cf. the formula
$\tilde b^{(e)}_{r} = \int_{-1}^1 f(x(X))\refphi_r(X)\frac{h}{2}\dX$.

The integration in the element matrix function involves only products
of polynomials, which `sympy` can easily deal with, but for the
right-hand side `sympy` may face difficulties with certain types of
expressions `f`. The result of the integral is then an `Integral`
object and not a number or expression
as when symbolic integration is successful.
It may therefore be wise to introduce a fall back on numerical
integration. The symbolic integration can also spend considerable time
before reaching an unsuccessful conclusion, so we may also introduce a parameter
`symbolic` to turn symbolic integration on and off:

!bc pycod
def element_vector(f, phi, Omega_e, symbolic=True):
        ...
        if symbolic:
            I = sym.integrate(f*phi[r]*detJ, (X, -1, 1))
        if not symbolic or isinstance(I, sym.Integral):
            h = Omega_e[1] - Omega_e[0]  # Ensure h is numerical
            detJ = h/2
            integrand = sym.lambdify([X], f*phi[r]*detJ, 'mpmath')
            I = mpmath.quad(integrand, [-1, 1])
        b_e[r] = I
        ...
!ec
Numerical integration requires that the symbolic
integrand is converted
to a plain Python function (`integrand`) and that
the element length `h` is a real number.


===== Linear system assembly and solution =====
label{fem:approx:fe:impl:linsys}

The complete algorithm
for computing and assembling the elementwise contributions
takes the following form

!bc pycod
def assemble(nodes, elements, phi, f, symbolic=True):
    N_n, N_e = len(nodes), len(elements)
    if symbolic:
        A = sym.zeros(N_n, N_n)
        b = sym.zeros(N_n, 1)    # note: (N_n, 1) matrix
    else:
        A = np.zeros((N_n, N_n))
        b = np.zeros(N_n)
    for e in range(N_e):
        Omega_e = [nodes[elements[e][0]], nodes[elements[e][-1]]]

        A_e = element_matrix(phi, Omega_e, symbolic)
        b_e = element_vector(f, phi, Omega_e, symbolic)

        for r in range(len(elements[e])):
            for s in range(len(elements[e])):
                A[elements[e][r],elements[e][s]] += A_e[r,s]
            b[elements[e][r]] += b_e[r]
    return A, b
!ec
The `nodes` and `elements` variables represent the finite
element mesh as explained earlier.

Given the coefficient matrix `A` and the right-hand side `b`,
we can compute the coefficients $\sequencej{c}$ in the expansion
$u(x)=\sum_jc_j\basphi_j$ as the solution vector `c` of the linear
system:

!bc pycod
if symbolic:
    c = A.LUsolve(b)
else:
    c = np.linalg.solve(A, b)
!ec
When `A` and `b` are `sympy` arrays,
the solution procedure implied by `A.LUsolve` is symbolic.
Otherwise, `A` and `b` are `numpy` arrays and a standard
numerical solver is called.
The symbolic version is suited for small problems only
(small $N$ values) since the calculation time becomes prohibitively large
otherwise. Normally, the symbolic *integration* will be more time
consuming in small problems than the symbolic *solution* of the linear system.

===== Example on computing symbolic approximations =====
label{fem:approx:fe:impl:ex1:symbolic}

We can exemplify the use of `assemble` on the computational
case from Section ref{fem:approx:global:linearsystem} with
two P1 elements (linear basis functions) on the domain $\Omega=[0,1]$.
Let us first work with a symbolic element length:

!bc ipy
>>> h, x = sym.symbols('h x')
>>> nodes = [0, h, 2*h]
>>> elements = [[0, 1], [1, 2]]
>>> phi = basis(d=1, symbolic=True)
>>> f = x*(1-x)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
>>> A
[h/3,   h/6,   0]
[h/6, 2*h/3, h/6]
[  0,   h/6, h/3]
>>> b
[     h**2/6 - h**3/12]
[      h**2 - 7*h**3/6]
[5*h**2/6 - 17*h**3/12]
>>> c = A.LUsolve(b)
>>> c
[                           h**2/6]
[12*(7*h**2/12 - 35*h**3/72)/(7*h)]
[  7*(4*h**2/7 - 23*h**3/21)/(2*h)]
!ec

===== Using interpolation instead of least squares =====
label{fem:approx:fe:impl:ex1:collocation}

As an alternative to the least squares formulation,
we may compute the `c` vector based on
the interpolation method from Section ref{fem:approx:global:interp},
using finite element basis functions.
Choosing the nodes as interpolation points, the method can be written as

!bt
\[ u(\xno{i}) = \sum_{j\in\If} c_j\basphi_j(\xno{i}) = f(\xno{i}),\quad
i\in\If\tp\]
!et
The coefficient matrix $A_{i,j}=\basphi_j(\xno{i})$ becomes
the identity matrix because basis function number $j$ vanishes
at all nodes, except node $i$: $\basphi_j(\xno{i})=\delta_{ij}$.
Therefore, $c_i = f(\xno{i})$.

The associated `sympy` calculations are

!bc ipy
>>> fn = sym.lambdify([x], f)
>>> c = [fn(xc) for xc in nodes]
>>> c
[0, h*(1 - h), 2*h*(1 - 2*h)]
!ec
These expressions are much simpler than those based on least squares
or projection in combination with finite element basis functions.
However, which of the two methods that is most appropriate for a given
task is problem-dependent, so we need both methods in our toolbox.

===== Example on computing numerical approximations =====
label{fem:approx:fe:impl:ex1:numeric}

The numerical computations corresponding to the
symbolic ones in Section ref{fem:approx:fe:impl:ex1:symbolic}
(still done by `sympy` and the `assemble` function) go as follows:

!bc ipy
>>> nodes = [0, 0.5, 1]
>>> elements = [[0, 1], [1, 2]]
>>> phi = basis(d=1, symbolic=True)
>>> x = sym.Symbol('x')
>>> f = x*(1-x)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=False)
>>> A
[ 0.166666666666667, 0.0833333333333333,                  0]
[0.0833333333333333,  0.333333333333333, 0.0833333333333333]
[                 0, 0.0833333333333333,  0.166666666666667]
>>> b
[          0.03125]
[0.104166666666667]
[          0.03125]
>>> c = A.LUsolve(b)
>>> c
[0.0416666666666666]
[ 0.291666666666667]
[0.0416666666666666]
!ec

The `fe_approx1D` module contains functions for generating the
`nodes` and `elements` lists for equal-sized elements with
any number of nodes per element. The coordinates in `nodes`
can be expressed either through the element length symbol `h`
(`symbolic=True`) or by real numbers (`symbolic=False`):

!bc pycod
nodes, elements = mesh_uniform(N_e=10, d=3, Omega=[0,1],
                               symbolic=True)
!ec
There is also a function

!bc pycod
def approximate(f, symbolic=False, d=1, N_e=4, filename='tmp.pdf'):
!ec
which computes a mesh with `N_e` elements, basis functions of
degree `d`, and approximates a given symbolic expression
`f` by a finite element expansion $u(x) = \sum_jc_j\basphi_j(x)$.
When `symbolic` is `False`, $u(x) = \sum_jc_j\basphi_j(x)$
can be computed at a (large)
number of points and plotted together with $f(x)$. The construction
of the pointwise function $u$ from the solution vector `c` is done
elementwise by evaluating $\sum_rc_r\refphi_r(X)$ at a (large)
number of points in each element in the local coordinate system,
and the discrete $(x,u)$ values on
each element are stored in separate arrays that are finally
concatenated to form a global array for $x$ and for $u$.
The details are found in the `u_glob` function in
`fe_approx1D.py`.



===== The structure of the coefficient matrix =====
label{fem:approx:fe:A:structure}

Let us first see how the global matrix looks like if we assemble
symbolic element matrices, expressed in terms of `h`, from
several elements:

!bc ipy
>>> d=1; N_e=8; Omega=[0,1]  # 8 linear elements on [0,1]
>>> phi = basis(d)
>>> f = x*(1-x)
>>> nodes, elements = mesh_symbolic(N_e, d, Omega)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
>>> A
[h/3,   h/6,     0,     0,     0,     0,     0,     0,   0]
[h/6, 2*h/3,   h/6,     0,     0,     0,     0,     0,   0]
[  0,   h/6, 2*h/3,   h/6,     0,     0,     0,     0,   0]
[  0,     0,   h/6, 2*h/3,   h/6,     0,     0,     0,   0]
[  0,     0,     0,   h/6, 2*h/3,   h/6,     0,     0,   0]
[  0,     0,     0,     0,   h/6, 2*h/3,   h/6,     0,   0]
[  0,     0,     0,     0,     0,   h/6, 2*h/3,   h/6,   0]
[  0,     0,     0,     0,     0,     0,   h/6, 2*h/3, h/6]
[  0,     0,     0,     0,     0,     0,     0,   h/6, h/3]
!ec
The reader is encouraged to assemble the element matrices by hand and verify
this result, as this exercise will give a hands-on understanding of
what the assembly is about. In general we have a coefficient matrix that is
tridiagonal:


!bt
\begin{equation}
A = \frac{h}{6}
\left(
\begin{array}{cccccccccc}
2 & 1 & 0
&\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
1 & 4 & 1 & \ddots &   & &  & &  \vdots \\
0 & 1 & 4 & 1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & 1 & 4 & 1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & 1  & 4  & 1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & 1 & 2
\end{array}
\right)
label{fem:approx:fe:A:fullmat}
\end{equation}
!et

# #ifdef EXTRA
!bt
\begin{equation}
A =\left(
\begin{array}{cccccccccc}
A_{0,0} & A_{0,1} & 0
&\cdots &
\cdots & \cdots & \cdots &
\cdots & 0 \\
A_{1,0} & A_{1,1} & 0 & \ddots &   & &  & &  \vdots \\
0 & A_{2,1} & A_{2,2} & A_{2,3} &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & A_{i,i-1} & A_{i,i} & A_{i,i+1} & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & A_{N-1,N} \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & A_{N,N-1} & A_{N,N}
\end{array}\right)
\end{equation}
!et
# #endif

The structure of the right-hand side is more difficult to reveal since
it involves an assembly of elementwise integrals of
$f(x(X))\refphi_r(X)h/2$, which obviously depend on the
particular choice of $f(x)$.
Numerical integration can give some insight into the nature of
the right-hand side. For this purpose it
is easier to look at the integration in $x$ coordinates, which
gives the general formula (ref{fem:approx:fe:bi:formula1}).
For equal-sized elements of length $h$, we can apply the
Trapezoidal rule at the global node points to arrive at

!bt
\[
b_i = h\left( \half \basphi_i(\xno{0})f(\xno{0}) +
\half \basphi_i(\xno{N})f(\xno{N}) + \sum_{j=1}^{N-1}
\basphi_i(\xno{j})f(\xno{j})\right),
\]
!et
which leads to

!bt
\begin{equation}
b_i =
\left\lbrace\begin{array}{ll}
\half hf(x_i),& i=0\hbox{ or }i=N,\\
h f(x_i), & 1 \leq i \leq N-1
\end{array}\right.
\end{equation}
!et
The reason for this simple formula is just that $\basphi_i$ is either
0 or 1 at the nodes and 0 at all but one of them.

Going to P2 elements (`d=2`) leads
to the element matrix

!bt
\begin{equation}
A^{(e)} = \frac{h}{30}
\left(\begin{array}{ccc}
4 & 2 & -1\\
2 & 16 & 2\\
-1 & 2 & 4
\end{array}\right)
\end{equation}
!et
and the following global matrix, assembled here from four elements:

!bt
\begin{equation}
A = \frac{h}{30}
\left(
\begin{array}{ccccccccc}
4 & 2 & - 1 & 0
  & 0 & 0 & 0 & 0 & 0\\
  2 & 16 & 2
  & 0 & 0 & 0 & 0 & 0 & 0\\- 1 & 2 &
  8 & 2 & - 1 & 0 & 0 & 0 &
  0\\0 & 0 & 2 & 16 & 2 & 0 & 0
  & 0 & 0\\0 & 0 & - 1 & 2 & 8
  & 2 & - 1 & 0 & 0\\0 & 0 & 0 & 0 &
  2 & 16 & 2 & 0 & 0\\0 & 0 & 0
  & 0 & - 1 & 2 & 8 &
  2 & - 1\\0 & 0 & 0 & 0 & 0 & 0 &
  2 & 16 & 2\\0 & 0 & 0 & 0 & 0
  & 0 & - 1 & 2 & 4
\end{array}
\right)
\end{equation}
!et
In general, for $i$ odd we have the nonzeroes

!bt
\begin{equation*} A_{i,i-2} = -1,\quad A_{i-1,i}=2,\quad A_{i,i} = 8,\quad A_{i+1,i}=2,
\quad A_{i+2,i}=-1,\end{equation*}
!et
multiplied by $h/30$, and for $i$ even we have the nonzeros

!bt
\begin{equation*} A_{i-1,i}=2,\quad A_{i,i} = 16,\quad A_{i+1,i}=2,\end{equation*}
!et
multiplied by $h/30$. The rows with odd numbers correspond to
nodes at the element boundaries and get contributions from two
neighboring elements in the assembly process,
while the even numbered rows correspond to
internal nodes in the elements where only one element contributes
to the values in the global matrix.

===== Applications =====
label{fem:approx:fe:impl:ex2}

With the aid of the `approximate` function in the `fe_approx1D`
module we can easily investigate the quality of various finite element
approximations to some given functions. Figure ref{fem:approx:fe:x9:sin}
shows how linear and quadratic elements approximate the polynomial
$f(x)=x(1-x)^8$ on $\Omega =[0,1]$, using equal-sized elements.
The results arise from the program

!bc pycod
import sympy as sym
from fe_approx1D import approximate
x = sym.Symbol('x')

approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=4)
approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=2)
approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=8)
approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=4)
!ec
The quadratic functions are seen to be better than the linear ones for the same
value of $N$, as we increase $N$. This observation has some generality:
higher degree is not necessarily better on a coarse mesh, but it is as
we refine the mesh and the function is properly resolved.


FIGURE: [fig/fe_p1_p2_x9_248e, width=800 frac=1.0]  Comparison of the finite element approximations: 4 P1 elements with 5 nodes (upper left), 2 P2 elements with 5 nodes (upper right), 8 P1 elements with 9 nodes (lower left), and 4 P2 elements with 9 nodes (lower right).  label{fem:approx:fe:x9:sin}


===== Sparse matrix storage and solution =====
label{fem:approx:fe:impl:sparse}

idx{sparse matrices}

Some of the examples in the preceding section took several minutes to
compute, even on small meshes consisting of up to eight elements.
The main explanation for slow computations is unsuccessful
symbolic integration: `sympy` may use a lot of energy on
integrals like $\int f(x(X))\refphi_r(X)h/2 \dx$ before
giving up, and the program then resorts to numerical integration.
Codes that can deal with a large number of basis functions and
accept flexible choices of $f(x)$ should compute all integrals
numerically and replace the matrix objects from `sympy` by
the far more efficient array objects from `numpy`.

There is also another (potential)
reason for slow code: the solution algorithm for
the linear system performs much more work than necessary. Most of the
matrix entries $A_{i,j}$ are zero, because $(\basphi_i,\basphi_j)=0$
unless $i$ and $j$ are nodes in the same element. In 1D problems,
we do not need to store or compute with these zeros when solving the
linear system, but that requires solution methods adapted to the kind
of matrices produced by the finite element approximations.

A matrix whose majority of entries are zeros, is known as a "sparse": "https://en.wikipedia.org/wiki/Sparse_matrix" matrix.
Utilizing sparsity in software dramatically decreases
the storage demands and the CPU-time needed to compute the solution of
the linear system. This optimization is not very critical in 1D problems
where modern computers can afford computing with all the zeros in the
complete square matrix, but in 2D and especially in 3D, sparse
matrices are fundamental for feasible finite element computations.
One of the advantageous features of the finite element method is that it
produces very sparse matrices. The reason is that the basis functions
have local support such that the product of two basis functions, as
typically met in integrals, is mostly zero.

Using a numbering of nodes and elements from left to right over a 1D
domain, the assembled coefficient matrix has only a few diagonals
different from zero. More precisely, $2d+1$ diagonals around the main
diagonal are different from zero, where $d$ is the order of the polynomial. With a different numbering of global
nodes, say a random ordering, the diagonal structure is lost, but the
number of nonzero elements is unaltered. Figures
ref{fem:approx:fe:sparsity:P1} and ref{fem:approx:fe:sparsity:P3}
exemplify sparsity patterns.


FIGURE: [fig/sparsity_pattern_1D_30, width=800] Matrix sparsity pattern for left-to-right numbering (left) and random numbering (right) of nodes in P1 elements. label{fem:approx:fe:sparsity:P1}

FIGURE: [fig/sparsity_pattern_1DP3_30, width=800] Matrix sparsity pattern for left-to-right numbering (left) and random numbering (right) of nodes in P3 elements. label{fem:approx:fe:sparsity:P3}

The `scipy.sparse` library supports creation of sparse matrices
and linear system solution.

 * `scipy.sparse.diags` for matrix defined via diagonals
 * `scipy.sparse.dok_matrix` for matrix incrementally defined via index pairs $(i,j)$

The `dok_matrix` object is most convenient for finite element computations.
This sparse matrix format is called DOK, which stands for Dictionary Of Keys:
the implementation is basically a dictionary (hash) with the
entry indices `(i,j)` as keys.

Rather than declaring `A = np.zeros((N_n, N_n))`, a DOK sparse
matrix is created by

!bc pycod
import scipy.sparse
A = scipy.sparse.dok_matrix((N_n, N_n))
!ec
When there is any need to set or add some matrix entry `i,j`, just do

!bc pycod
A[i,j]  = entry
# or
A[i,j] += entry
!ec
The indexing creates the matrix entry on the fly, and
only the nonzero entries in the matrix will be stored.

To solve a system with right-hand side `b` (one-dimensional `numpy`
array) with a sparse coefficient matrix `A`, we must use some kind of
a sparse linear system solver. The safest choice is a method based on
sparse Gaussian elimination. One high-qualify package for
this purpose if "UMFPACK": "https://en.wikipedia.org/wiki/UMFPACK".
It is interfaced from SciPy by

!bc pycod
import scipy.sparse.linalg
c = scipy.sparse.linalg.spsolve(A.tocsr(), b, use_umfpack=True)
!ec
The call `A.tocsr()` is not strictly needed (a warning is issued
otherwise), but ensures that the solution algorithm can efficiently
work with a copy of the sparse matrix in *Compressed Sparse Row*  (CSR) format.

An advantage of the `scipy.sparse.diags` matrix over the DOK format is
that the former allows vectorized assignment to the matrix.
Vectorization is possible for approximation problems when all elements
are of the same type. However, when solving differential equations,
vectorization may be more difficult in particular because of boundary conditions.
It also appears that the DOK sparse matrix format available in the `scipy.sparse` package is fast
enough even for big 1D problems on today's laptops, so the need for
improving efficiency occurs only in 2D and 3D problems, but then the
complexity of the mesh favors the DOK format.

# 2DO

#Examples to come....

#Exercise: introduce a random numbering of global nodes; need arbitrary, sparse matrix.


!split
======= Comparison of finite elements and finite differences =======
label{fem:approx:fe:fd}

The previous sections on approximating $f$ by a finite element
function $u$ utilize the projection/Galerkin or least squares
approaches to minimize the approximation error. We may, alternatively,
use the collocation/interpolation method as described in Section
ref{fem:approx:fe:impl:ex1:collocation}.  Here we shall compare these
three approaches with what one does in the finite difference method
when representing a given function on a mesh.


===== Finite difference approximation of given functions =====
label{fem:approx:fe:fd:fdproj}

Approximating a given function $f(x)$ on a mesh in a finite difference
context will typically just sample $f$ at the mesh points. If $u_i$ is
the value of the approximate $u$ at the mesh point $\xno{i}$, we have
$u_i = f(\xno{i})$.  The collocation/interpolation method using finite
element basis functions gives exactly the same representation, as
shown Section ref{fem:approx:fe:impl:ex1:collocation},

!bt
\[ u(\xno{i}) = c_i = f(\xno{i})\tp\]
!et

How does a finite element Galerkin or least squares approximation
differ from this straightforward interpolation of $f$? This is the
question to be addressed next.  We now limit the scope to P1 elements
since this is the element type that gives formulas closest to those
arising in the finite difference method.

===== Interpretation of a finite element approximation in terms of finite difference operators =====
label{fem:approx:fe:fd:feproj}


The linear system arising from a Galerkin or least squares approximation
reads in general

!bt
\[
\sum_{j\in\If} c_j (\baspsi_i,\baspsi_j) = (f,\baspsi_i),\quad i\in\If\tp
\]
!et
In the finite element approximation we choose $\baspsi_i =\basphi_i$.
With $\basphi_i$ corresponding to P1 elements and a uniform mesh of
element length $h$ we have in Section
ref{fem:approx:global:linearsystem} calculated the matrix with entries
$(\basphi_i,\basphi_j)$.  Equation number $i$ reads

!bt
\begin{equation}
\frac{h}{6}(u_{i-1} + 4u_i + u_{i+1}) = (f,\basphi_i)
\tp
label{fem:deq:1D:approx:deq:massmat:diffeq2}
\end{equation}
!et
The first and last equation, corresponding to $i=0$ and $i=N$ are slightly
different, see Section ref{fem:approx:fe:A:structure}.

The finite difference counterpart to
(ref{fem:deq:1D:approx:deq:massmat:diffeq2}) is just $u_i=f_i$
as explained in Section ref{fem:approx:fe:fd:fdproj}.
To easier compare this result to
the finite element approach to approximating functions, we can rewrite
the left-hand side of (ref{fem:deq:1D:approx:deq:massmat:diffeq2})
as

!bt
\begin{equation}
h(u_i + \frac{1}{6}(u_{i-1} - 2u_i + u_{i+1}))
\tp
\end{equation}
!et
Thinking in terms of finite differences, we can write this expression
using finite difference operator notation:

!bt
\[ [h(u + \frac{h^2}{6}D_x D_x u)]_i,\]
!et
which is nothing but the standard discretization of
(see also Appendix ref{sec:form:fdop})

!bt
\[ h(u + \frac{h^2}{6}u'')\tp\]
!et


Before interpreting the approximation procedure as solving a
differential equation, we need to work out what the right-hand side is
in the context of P1 elements.
Since $\basphi_i$ is the linear function that is 1 at
$\xno{i}$ and zero at all other nodes, only the interval $[\xno{i-1},\xno{i+1}]$
contribute to the integral on the right-hand side. This integral is
naturally split into two parts according to
(ref{fem:approx:fe:phi:1:formula2}):

!bt
\[ (f,\basphi_i) = \int_{\xno{i-1}}^{\xno{i}} f(x)\frac{1}{h} (x - \xno{i-1}) \dx
+ \int_{\xno{i}}^{\xno{i+1}} f(x)(1 - \frac{1}{h}(x - x_{i})) \dx
\tp
\]
!et
However, if $f$ is not known we cannot do much else with this expression.
It is clear that many values of
$f$ around $\xno{i}$ contribute to the right-hand side, not just
the single point value $f(\xno{i})$
as in the finite difference method.

To proceed with the right-hand side, we can
turn to numerical integration schemes.
The Trapezoidal method for $(f,\basphi_i)$, based on
sampling the integrand $f\basphi_i$ at the node points $\xno{i}=i h$
gives

!bt
\[ (f,\basphi_i) = \int_\Omega f\basphi_i \dx\approx h\half(
f(\xno{0})\basphi_i(\xno{0}) + f(\xno{N})\basphi_i(\xno{N}))
+ h\sum_{j=1}^{N-1} f(\xno{j})\basphi_i(\xno{j})
\tp
\]
!et
Since $\basphi_i$ is zero at all these points, except at $\xno{i}$, the
Trapezoidal rule collapses to one term:

!bt
\begin{equation}
(f,\basphi_i) \approx hf(\xno{i}),
\end{equation}
!et
for $i=1,\ldots,N-1$,
which is the same result as with collocation/interpolation, and of course
the same result as in the finite difference method.
For the end points
$i=0$ and $i=N$ we get contribution from only one element so

!bt
\begin{equation}
(f,\basphi_i) \approx {\half}hf(\xno{i}),\quad i=0,\ i=N
\tp
\end{equation}
!et

Simpson's rule with sample points also in the middle of
the elements, at $\xno{i+\half}=(\xno{i} + \xno{i+1})/2$,
can be written as

!bt
\[ \int_\Omega g(x)\dx \approx \frac{\tilde h}{3}\left( g(\xno{0}) +
2\sum_{j=1}^{N-1} g(\xno{j})
+ 4\sum_{j=0}^{N-1} g(\xno{j+\half}) + f(\xno{2N})\right),
\]
!et
where $\tilde h= h/2$ is the spacing between the sample points.
Our integrand is $g=f\basphi_i$. For all the node points,
$\basphi_i(\xno{j})=\delta_{ij}$, and therefore
$\sum_{j=1}^{N-1} f(\xno{j})\basphi_i(\xno{j})=f(\xno{i})$.
At the midpoints, $\basphi_i(\xno{i\pm\half})=1/2$ and
$\basphi_i(\xno{j+\half})=0$ for $j>1$ and $j<i-1$.
Consequently,

!bt
\[ \sum_{j=0}^{N-1} f(\xno{j+\half})\basphi_i(\xno{j+\half})
= \half(f(\xno{j-\half}) + f(\xno{j+\half}))\tp\]
!et
When $1\leq i\leq N-1$ we then get

!bt
\begin{equation}
(f,\basphi_i) \approx
\frac{h}{3}(f_{i-\half} + f_i + f_{i+\half})
\tp
\end{equation}
!et
This result shows that, with Simpson's rule, the finite element method
operates with the average of $f$ over three points, while the finite difference
method just applies $f$ at one point. We may interpret this as
a "smearing" or smoothing of $f$ by the finite element method.

We can now summarize our findings. With the approximation of
$(f,\basphi_i)$ by the Trapezoidal rule, P1 elements give rise
to equations that can be expressed as a finite difference
discretization of

!bt
\begin{equation}
u + \frac{h^2}{6} u'' = f,\quad u'(0)=u'(L)=0,
\end{equation}
!et
expressed with operator notation as

!bt
\begin{equation}
[u + \frac{h^2}{6} D_x D_x u = f]_i\tp  \end{equation}
!et
As $h\rightarrow 0$, the extra term proportional to $u''$ goes to zero,
and the two methods converge to the same solution.

With the Simpson's rule, we may say that we solve

!bt
\begin{equation}
[u + \frac{h^2}{6} D_x D_x u = \bar f]_i,
\end{equation}
!et
where $\bar f_i$ means the average $\frac{1}{3}(f_{i-1/2} + f_i + f_{i+1/2})$.

The extra term $\frac{h^2}{6} u''$ represents a smoothing effect: with
just this term, we would find $u$ by integrating $f$ twice and thereby
smooth $f$ considerably. In addition, the finite element
representation of $f$ involves an average, or a smoothing, of $f$ on
the right-hand side of the equation system. If $f$ is a noisy
function, direct interpolation $u_i=f_i$ may result in a noisy $u$
too, but with a Galerkin or least squares formulation and P1 elements,
we should expect that $u$ is smoother than $f$ unless $h$ is very
small.

The interpretation that finite elements tend to smooth the solution
is valid in applications far beyond approximation of 1D functions.


===== Making finite elements behave as finite differences =====
label{fem:deq:1D:approx:fem_vs_fdm}

With a simple trick, using numerical integration, we can easily produce
the result $u_i=f_i$ with the Galerkin or least square formulation
with P1 elements. This is useful in many occasions when we deal
with more difficult differential equations and want the finite element
method to have properties like the finite difference method (solving
standard linear wave equations is one primary example).

=== Computations in physical space ===

We have already seen that applying the Trapezoidal rule to the
right-hand side $(f,\basphi_i)$ simply gives $f$ sampled at $\xno{i}$.
Using the Trapezoidal rule on the  matrix entries
$A_{i,j}=(\basphi_i,\basphi_j)$ involves a sum
!bt
\[ \sum_k \basphi_i(\xno{k})\basphi_j(\xno{k}),\]
!et
but $\basphi_i(\xno{k})=\delta_{ik}$ and
$\basphi_j(\xno{k})=\delta_{jk}$.
The product $\basphi_i\basphi_j$ is then different from zero only
when sampled at $\xno{i}$ and $i=j$. The Trapezoidal
approximation to the integral
is then

!bt
\[ (\basphi_i,\basphi_j) \approx h,\quad i=j,\]
!et
and zero if $i\neq j$. This means that we have obtained a diagonal matrix!
The first and last diagonal elements, $(\basphi_0,\basphi_0)$ and
$(\basphi_N,\basphi_N)$ get contribution only from the first and last
element, respectively, resulting in the approximate integral value $h/2$.
The corresponding right-hand side also has a factor $1/2$ for $i=0$ and $i=N$.
Therefore, the least squares or Galerkin approach with P1 elements and
Trapezoidal integration results in

!bt
\[ c_i = f_i,\quad i\in\If\tp  \]
!et

Simpsons's rule can be used to achieve a similar result for P2 elements, i.e,
a diagonal coefficient matrix, but with the previously derived
average of $f$ on the right-hand side.

=== Elementwise computations ===

Identical results to those above will arise if we perform elementwise
computations. The idea is to use the Trapezoidal rule on the reference
element for computing the element matrix and vector. When assembled,
the same equations $c_i=f(\xno{i})$ arise. Exercise
ref{fem:approx:fe:exer:1D:trapez} encourages you to carry out the
details.

# see ex_fe_approx1D_session.py for an example of symbolic comp.

idx{mass matrix} idx{mass lumping} idx{lumped mass matrix}

=== Terminology ===

The matrix with entries $(\basphi_i,\basphi_j)$ typically arises from
terms proportional to $u$ in a differential equation where $u$ is the
unknown function. This matrix is often called the *mass matrix*,
because in the early days of the finite element method, the matrix
arose from the mass times acceleration term in Newton's second law of
motion. Making the mass matrix diagonal by, e.g., numerical
integration, as demonstrated above, is a widely used technique and is
called *mass lumping*. In time-dependent problems it can sometimes
enhance the numerical accuracy and computational efficiency of the
finite element method.  However, there are also examples where mass
lumping destroys accuracy.



!split
======= A generalized element concept =======
label{fem:approx:fe:element}


So far, finite element computing has employed the `nodes` and
`element` lists together with the definition of the basis functions
in the reference element. Suppose we want to introduce a piecewise
constant approximation with one basis function $\refphi_0(x)=1$ in
the reference element, corresponding to a $\basphi_i(x)$ function that
is 1 on element number $i$ and zero on all other elements.
Although we could associate the function value
with a node in the middle of the elements, there are no nodes at the
ends, and the previous code snippets will not work because we
cannot find the element boundaries from the `nodes` list.

In order to get a richer space of finite element approximations, we need
to revise the simple node and element concept presented so far and
introduce a more powerful terminology. Much literature employs the
definition of node and element introduced in the previous sections
so it is important have this knowledge, besides being a good pedagogical
background from understanding the extended element concept in the following.

===== Cells, vertices, and degrees of freedom =====
label{fem:approx:fe:element:terminology}

idx{cell} idx{vertex} idx{degree of freedom} idx{reference cell}

We now introduce *cells* as the subdomains $\Omega^{(e)}$ previously
referred to as elements. The cell boundaries are uniquely defined in terms of *vertices*.
This applies to cells in both 1D and higher dimensions.
We also define a set of *degrees of freedom* (dof), which are
the quantities we aim to compute. The most common type of degree
of freedom is the value of the unknown function $u$ at some point.
(For example, we can introduce nodes as before and say the degrees of
freedom are the values of $u$ at the nodes.) The basis functions are
constructed so that they equal unity for one particular degree of
freedom and zero for the rest. This property ensures that when
we evaluate $u=\sum_j c_j\basphi_j$ for degree of freedom number $i$,
we get $u=c_i$. Integrals are performed over cells, usually by
mapping the cell of interest to a *reference cell*.


With the concepts of cells, vertices, and degrees of freedom we
increase the decoupling of the geometry (cell, vertices) from the
space of basis functions. We will associate different
sets of basis functions with a cell. In 1D, all cells are intervals,
while in 2D we can have cells that are triangles with straight sides,
or any polygon, or in fact any two-dimensional geometry. Triangles and
quadrilaterals are most common, though. The popular cell types in 3D
are tetrahedra and hexahedra.

===== Extended finite element concept =====
label{fem:approx:fe:element:def}

idx{finite element, definition} idx{dof map}

The concept of a *finite element* is now

  * a *reference cell* in a local reference coordinate system;
  * a set of *basis functions* $\refphi_i$ defined on the cell;
  * a set of *degrees of freedom* that uniquely how basis functions
    from different elements are glued together across element interfaces.
    A common technique is to choose
    the basis functions such that $\refphi_i=1$ for degree of freedom
    number $i$ that is associated with nodal point
    $x_i$ and $\refphi_i=0$ for all other degrees of freedom. This
    technique ensures the desired continuity;
  * a mapping between local and global degree of freedom numbers,
    here called the *dof map*;
  * a geometric *mapping* of the reference cell onto the cell in the physical
    domain.


There must be a geometric description of a cell. This is trivial in 1D
since the cell is an interval and is described by the interval limits,
here called vertices. If the cell is $\Omega^{(e)}=[x_L,x_R]$,
vertex 0 is $x_L$ and vertex 1 is $x_R$. The reference cell in 1D
is $[-1,1]$ in the reference coordinate system $X$.

idx{finite element expansion!reference element}

The expansion of $u$ over one cell is often used:

!bt
\begin{equation}
u(x) = \tilde u(X) = \sum_{r} c_r\refphi_r(X),\quad x\in\Omega^{(e)},\
X\in [-1,1],
\end{equation}
!et
where the sum is taken over the numbers of the degrees of freedom and
$c_r$ is the value of $u$ for degree of freedom number $r$.

Our previous P1, P2, etc., elements are defined by introducing $d+1$
equally spaced nodes in the reference cell, a polynomial space (P$d$) containing
a complete set of polynomials of order
$d$,   and saying that the degrees
of freedom are the $d+1$ function values at these nodes.  The basis
functions must be 1 at one node and 0 at the others, and the Lagrange
polynomials have exactly this property.  The nodes can be numbered
from left to right with associated degrees of freedom that are
numbered in the same way.  The degree of freedom mapping becomes what
was previously represented by the `elements` lists.  The cell mapping
is the same affine mapping (ref{fem:approx:fe:affine:mapping}) as
before.

!bnotice
The extended finite element concept introduced above is quite general and
has served as a successful recipe for implementing many finite element frameworks
and for developing the theory behind. Here, we have seen several different examples
but the exposition is most focused on 1D examples and the diversity is limited
as many of the different methods in 2D and 3D collapse to the same method in 1D.    
The curious reader is advised to for instance look into the numerous 
examples of finite elements implemented in FEniCS cite{fenics_book}
to gain insight into the variety of methods that exists. 
!enotice



===== Implementation =====
label{fem:approx:fe:element:impl}

idx{`cells` list} idx{`vertices` list} idx{`dof_map` list}

Implementationwise,

  * we replace `nodes` by `vertices`;
  * we introduce `cells` such that `cell[e][r]` gives the mapping
    from local vertex `r` in cell `e` to the global vertex number
    in `vertices`;
  * we replace `elements` by `dof_map` (the contents are the same
    for P$d$ elements).

Consider the example from Section
ref{fem:approx:fe:def:elements:nodes} where $\Omega =[0,1]$ is divided
into two cells, $\Omega^{(0)}=[0,0.4]$ and $\Omega^{(1)}=[0.4,1]$, as
depicted in Figure ref{fem:approx:fe:def:elements:nodes:fig:P2}.  The
vertices are $[0,0.4,1]$. Local vertex 0 and 1 are $0$ and $0.4$ in
cell 0 and $0.4$ and $1$ in cell 1.  A P2 element means that the
degrees of freedom are the value of $u$ at three equally spaced points
(nodes) in each cell. The data structures become

!bc pycod
vertices = [0, 0.4, 1]
cells = [[0, 1], [1, 2]]
dof_map = [[0, 1, 2], [2, 3, 4]]
!ec

If we would approximate $f$ by piecewise constants, known as
P0 elements, we simply
introduce one point or node in an element, preferably $X=0$,
and define one degree of freedom, which is the function value
at this node. Moreover, we set $\refphi_0(X)=1$.
The `cells` and `vertices` arrays remain the same, but
`dof_map` is altered:

!bc pycod
dof_map = [[0], [1]]
!ec

We use the `cells` and `vertices` lists to retrieve information
on the geometry of a cell, while `dof_map` is the
$q(e,r)$ mapping introduced earlier in the
assembly of element matrices and vectors.
For example, the `Omega_e` variable (representing the cell interval)
in previous code snippets must now be computed as

!bc pycod
Omega_e = [vertices[cells[e][0], vertices[cells[e][1]]
!ec
The assembly is done by

!bc pycod
A[dof_map[e][r], dof_map[e][s]] += A_e[r,s]
b[dof_map[e][r]] += b_e[r]
!ec

We will hereafter drop the `nodes` and `elements` arrays
and work exclusively with `cells`, `vertices`, and `dof_map`.
The module `fe_approx1D_numint.py` now replaces the module
`fe_approx1D` and offers similar functions that work with
the new concepts:

!bc pycod
from fe_approx1D_numint import *
x = sym.Symbol('x')
f = x*(1 - x)
N_e = 10
vertices, cells, dof_map = mesh_uniform(N_e, d=3, Omega=[0,1])
phi = [basis(len(dof_map[e])-1) for e in range(N_e)]
A, b = assemble(vertices, cells, dof_map, phi, f)
c = np.linalg.solve(A, b)
# Make very fine mesh and sample u(x) on this mesh for plotting
x_u, u = u_glob(c, vertices, cells, dof_map,
                resolution_per_element=51)
plot(x_u, u)
!ec


These steps are offered in the `approximate` function, which we here
apply to see how well four P0 elements (piecewise constants)
can approximate a parabola:

!bc pycod
from fe_approx1D_numint import *
x=sym.Symbol("x")
for N_e in 4, 8:
    approximate(x*(1-x), d=0, N_e=N_e, Omega=[0,1])
!ec
Figure ref{fem:approx:fe:element:impl:fig:P0:x2} shows the result.


FIGURE: [fig/fe_p0_x2_4e_8e, width=600] Approximation of a parabola by 4 (left) and 8 (right) P0 elements. label{fem:approx:fe:element:impl:fig:P0:x2}


===== Computing the error of the approximation =====
label{fem:approx:fe:error}

So far we have focused on computing the coefficients $c_j$ in the
approximation $u(x)=\sum_jc_j\basphi_j$ as well as on plotting $u$ and
$f$ for visual comparison. A more quantitative comparison needs to
investigate the error $e(x)=f(x)-u(x)$. We mostly want a single number to
reflect the error and use a norm for this purpose, usually the $L^2$ norm

!bt
\[ ||e||_{L^2} = \left(\int_{\Omega} e^2 \dx\right)^{1/2}\tp\]
!et
Since the finite element approximation is defined for all $x\in\Omega$,
and we are interested in how $u(x)$ deviates from $f(x)$ through all
the elements,
we can either integrate analytically or use an accurate numerical
approximation. The latter is more convenient as it is a generally
feasible and simple approach. The idea is to sample $e(x)$
at a large number of points in each element. The function `u_glob`
in the `fe_approx1D_numint` module does this for $u(x)$ and returns
an array `x` with coordinates and an array `u` with the $u$ values:

!bc pycod
x, u = u_glob(c, vertices, cells, dof_map,
              resolution_per_element=101)
e = f(x) - u
!ec
Let us use the Trapezoidal method to approximate the integral. Because
different elements may have different lengths, the `x` array may have
a non-uniformly distributed set of coordinates. Also, the `u_glob`
function works in an element by element fashion such that coordinates
at the boundaries between elements appear twice. We therefore need
to use a "raw" version of the Trapezoidal rule where we just add up
all the trapezoids:

!bt
\[ \int_\Omega g(x) \dx \approx \sum_{j=0}^{n-1} \half(g(x_j) +
g(x_{j+1}))(x_{j+1}-x_j),\]
!et
if $x_0,\ldots,x_n$ are all the coordinates in `x`. In
vectorized Python code,

!bc pycod
g_x = g(x)
integral = 0.5*np.sum((g_x[:-1] + g_x[1:])*(x[1:] - x[:-1]))
!ec
Computing the $L^2$ norm of the error, here named `E`, is now achieved by

!bc pycod
e2 = e**2
E = np.sqrt(0.5*np.sum((e2[:-1] + e2[1:])*(x[1:] - x[:-1]))
!ec

!bnotice How does the error depend on $h$ and $d$?
Theory and experiments show that the least squares or projection/Galerkin
method in combination with P$d$ elements of equal length $h$ has an error

!bt
\begin{equation}
||e||_{L^2} = C|f^{(d+1)}|h^{d+1},
label{fem:approx:fe:error:theorem}
\end{equation}
!et
where $C$ is a constant depending on $d$ and $\Omega=[0,L]$,
but not on $h$, and the norm $|f^{(d+1)}|$ is defined through

!bt
\[ |f^{(d+1)}|^2 = \int_0^L \left(\frac{d^{d+1}f}{dx^{d+1}}\right)^2dx\]
!et

!enotice


===== Example on cubic Hermite polynomials =====
label{fem:approx:fe:element:impl:Hermite}

idx{Hermite polynomials}

The finite elements considered so far represent $u$ as piecewise
polynomials with discontinuous derivatives at the cell boundaries.
Sometimes it is desirable to have continuous derivatives. A primary
example is the solution of differential equations with fourth-order
derivatives where standard finite element formulations lead to
a need for basis functions with continuous first-order derivatives.
The most common type of such basis functions in 1D is the
so-called cubic Hermite polynomials.
The construction of such polynomials, as explained next, will further
exemplify the concepts of a cell, vertex, degree of freedom, and dof map.

Given a reference cell $[-1,1]$, we seek cubic polynomials
with the values of the *function* and its *first-order derivative* at
$X=-1$ and $X=1$ as the four degrees of freedom. Let us number
the degrees of freedom as

  * 0: value of function at $X=-1$
  * 1: value of first derivative at $X=-1$
  * 2: value of function at $X=1$
  * 3: value of first derivative at $X=1$

By having the derivatives as unknowns, we ensure that
the derivative of a basis function in two neighboring elements
is the same at the node points.

The four basis functions can be written in a general form
!bt
\[ \refphi_i (X) = \sum_{j=0}^3 C_{i,j}X^j, \]
!et
with four coefficients $C_{i,j}$, $j=0,1,2,3$, to be determined for
each $i$. The constraints
that basis function number $i$ must be 1 for degree of
freedom number $i$ and zero for the other three degrees of freedom,
gives four equations to determine $C_{i,j}$ for each $i$. In mathematical
detail,
!bt
\begin{align*}
\refphi_0 (-1) &= 1,\quad \refphi_0 (1)=\refphi_0'(-1)=\refphi_i' (1)=0,\\
\refphi_1' (-1) &= 1,\quad \refphi_1 (-1)=\refphi_1(1)=\refphi_1' (1)=0,\\
\refphi_2 (1) &= 1,\quad \refphi_2 (-1)=\refphi_2'(-1)=\refphi_2' (1)=0,\\
\refphi_3' (1) &= 1,\quad \refphi_3 (-1)=\refphi_3'(-1)=\refphi_3 (1)=0
\tp
\end{align*}
!et
These four $4\times 4$ linear equations can be solved, yielding the
following formulas
for the cubic basis functions:

!bt
\begin{align}
\refphi_0(X) &= 1 - \frac{3}{4}(X+1)^2 + \frac{1}{4}(X+1)^3\\
\refphi_1(X) &= -(X+1)(1 - \half(X+1))^2\\
\refphi_2(X) &= \frac{3}{4}(X+1)^2 - \half(X+1)^3\\
\refphi_3(X) &= -\half(X+1)(\half(X+1)^2 - (X+1))\\
\end{align}
!et

The construction of the dof map needs a scheme for numbering the
global degrees of freedom. A natural left-to-right numbering
has the function value at vertex $\xno{i}$
as degree of freedom number $2i$ and the value of the derivative
at $\xno{i}$ as degree of freedom number $2i+1$, $i=0,\ldots,N_e+1$.


!split
======= Numerical integration =======

Finite element codes usually apply numerical approximations to
integrals. Since the integrands in the coefficient matrix often
are (lower-order) polynomials, integration rules that can
integrate polynomials exactly are popular.

Numerical integration rules can be expressed in a common form,

!bt
\begin{equation}
\int_{-1}^{1} g(X)\dX \approx \sum_{j=0}^M w_j g(\bar X_j),
\end{equation}
!et
where $\bar X_j$ are *integration points* and $w_j$ are
*integration weights*, $j=0,\ldots,M$.
Different rules correspond to different choices of points and weights.

The very simplest method is the *Midpoint rule*,
!bt
\begin{equation}
\int_{-1}^{1} g(X)\dX \approx 2g(0),\quad \bar X_0=0,\ w_0=2,
\end{equation}
!et
which integrates linear functions exactly.

===== Newton-Cotes rules =====
label{fem:approx:fe:numint1}

idx{numerical integration!Midpoint rule}
idx{numerical integration!Trapezoidal rule}
idx{numerical integration!Simpson's rule}
idx{numerical integration!Newton-Cotes formulas}
idx{Midpoint rule} idx{Trapezoidal rule} idx{Simpson's rule}
idx{Newton-Cotes rules}

The "Newton-Cotes": "http://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas"
rules are based on a fixed uniform distribution of the integration points.
The first two formulas in this family are the well-known
*Trapezoidal rule*,

!bt
\begin{equation}
\int_{-1}^{1} g(X)\dX \approx g(-1) + g(1),\quad \bar X_0=-1,\ \bar X_1=1,\ w_0=w_1=1,
label{fem:approx:fe:numint1:trapez}
\end{equation}
!et
and *Simpson's rule*,

!bt
\begin{equation}
\int_{-1}^{1} g(X)\dX \approx \frac{1}{3}\left(g(-1) + 4g(0)
+ g(1)\right),
label{fem:approx:fe:numint1:Simpson}
\end{equation}
!et
where

!bt
\begin{equation}
\bar X_0=-1,\ \bar X_1=0,\ \bar X_2=1,\ w_0=w_2=\frac{1}{3},\ w_1=\frac{4}{3}\tp  \end{equation}
!et
Newton-Cotes rules up to five points is supported in the
module file "`numint.py`": "${fem_src}/numint.py".

For higher accuracy one can divide the reference cell into a set of
subintervals and use the rules above on each subinterval. This approach
results in *composite* rules, well-known from basic introductions
to numerical integration of $\int_{a}^{b}f(x)\dx$.

===== Gauss-Legendre rules with optimized points =====

idx{Gauss-Legendre quadrature}

More accurate rules, for a given $M$, arise if the location of the
integration points are optimized for polynomial integrands.  The
"Gauss-Legendre rules":
"http://en.wikipedia.org/wiki/Gaussian_quadrature" (also known as
Gauss-Legendre quadrature or Gaussian quadrature) constitute one such
class of integration methods. Two widely applied Gauss-Legendre rules
in this family have the choice

!bt
\begin{align}
M=1&:\quad \bar X_0=-\frac{1}{\sqrt{3}},\
\bar X_1=\frac{1}{\sqrt{3}},\ w_0=w_1=1\\
M=2&:\quad \bar X_0=-\sqrt{\frac{3}{{5}}},\ \bar X_0=0,\
\bar X_2= \sqrt{\frac{3}{{5}}},\ w_0=w_2=\frac{5}{9},\ w_1=\frac{8}{9}\tp  \end{align}
!et
These rules integrate 3rd and 5th degree polynomials exactly.
In general, an $M$-point Gauss-Legendre rule integrates a polynomial
of degree $2M+1$ exactly.
The code `numint.py` contains a large collection of Gauss-Legendre rules.

# 2DO
# Newton-Cotes: bedre med det som overskrift over
# Later:
# lumped mass via num int; example or exercise

# non-uniform meshes
# hand-calculation, extend software?
# example: half a Gaussian hat with one fine-grid area and a coarse-grid area
# adaptivity

# #ifdef OLD
===== Summary of a finite element =====

The concept of a finite element contains four key components, which we
now formulate in a more abstract sense suitable for later use.
A finite element is defined by


Property 3 ensures that a finite element function $u$ can be written as

!bt
\begin{equation*} u(x) = \sum_{j\in\If}c_j\refphi_j(x),\end{equation*}
!et
where $c_j$ is the value of degree of freedom number $j$ of $u$.
The most common example of a degree of freedom is the function value
at a point. With a mapping between local degrees of freedom and
global degrees of freedom, one can relate the expansion of $u$ on an
element to its expansion in the global physical domain.

The elements seen so far are all one-dimensional with Lagrange polynomials
as basis functions, based on uniformly distributed nodes in the
reference element $[-1,1]$.
The degrees
of freedom are then the function values at $d+1$ nodes.
A linear mapping is used to map $[-1,1]$ onto the particular element
$\Omega^{(e)}$ in the physical coordinate system.
#All these ingredients can be changed: we may have non-uniformly distributed
#local nodes, other function families can be used as basis functions,
#the degrees of freedom do not need to be function values at the nodes,
#and the mapping between the reference element and the physical space may
#be nonlinear.

[hpl: project with sin and cos functions over an element]

We shall see that the above characteristics of an element generalize to
higher dimensions and to much more complicated elements.
The concept of degrees of freedom is important: we may choose other
parameters than the function values at points as the interpretation of the
coefficients
$c_j$. Here is one example. Suppose we want an approximation to
$f$ based on piecewise *constant* functions. Then we can construct
an element with one local basis function, $\refphi_0(X)=1$.
The associated degree of freedom can be taken as the function value at
a node in the middle of the element. In this case the element will have
only one node and no nodes on the boundary. Alternatively, we can omit
the concept of nodes and say the degree of freedom is the *mean value*
of a function rather than a point value. That is, $c_0$ is the mean
value of $u$ over the element. To get a mean value (degree of freedom
value) of 1 for $\refphi_0(x)$ over $[-1,1]$, we must have
$\refphi_0(X)=1/2$. A global basis functions is associated with one
element, typically $\basphi_i$ equals $1/h_i$, where $h_i$ is the
length of element $i$. Then $\int_{\Omega^{(i)}}\basphi_i \dx=1$.
The mapping from local degrees of freedom to global degrees of freedom
is simple: local degree of freedom 0 in element $e$ maps to
global degree of freedom $e$.


#===== Accuracy of piecewise polynomial approximations =====

#Experimental. State theory. Make exercise.

===== Approximation of vector-valued functions =====

# #endif


!split
======= Finite elements in 2D and 3D =======

Finite element approximation is particularly powerful in 2D and 3D because
the method can handle a geometrically complex domain $\Omega$ with ease.
The principal idea is, as in 1D, to divide the domain into cells
and use polynomials for approximating a function over a cell.
Two popular cell shapes are triangles and quadrilaterals.
It is common to denote finite elements on triangles and tetrahedrons as P while
elements defined in terms of quadrilaterals and boxes are denoted by Q.
Figures ref{fem:approx:fe:2D:fig:rectP1}, ref{fem:approx:fe:2D:fig:circP1},
and ref{fem:approx:fe:2D:fig:rectQ1} provide examples. P1 elements
means linear functions ($a_0 + a_1x + a_2y$) over triangles, while Q1 elements
have bilinear functions ($a_0 + a_1x + a_2y + a_3xy$) over rectangular cells.
Higher-order elements can easily be defined.


FIGURE: [fig/mesh2D_rect_P1, width=800] Example on 2D P1 elements. label{fem:approx:fe:2D:fig:rectP1}

FIGURE: [fig/mesh2D_quarter_circle, width=400] Example on 2D P1 elements in a deformed geometry. label{fem:approx:fe:2D:fig:circP1}

FIGURE: [fig/mesh2D_rect_Q1, width=400] Example on 2D Q1 elements. label{fem:approx:fe:2D:fig:rectQ1}


===== Basis functions over triangles in the physical domain =====

Cells with triangular shape will be in main focus here.  With the P1
triangular element, $u$ is a linear function over each cell, as
depicted in Figure ref{fem:approx:fe:2D:fig:femfunc}, with
discontinuous derivatives at the cell boundaries.

FIGURE: [fig/demo2D_4x3r, width=400] Example on scalar function defined in terms of piecewise linear 2D functions defined on triangles. label{fem:approx:fe:2D:fig:femfunc}

We give the vertices of the cells global and local numbers as in 1D.
The degrees of freedom in the P1 element are the function values at
a set of nodes, which are the three vertices.
The basis function $\basphi_i(x,y)$ is then 1 at the vertex with global vertex
number $i$ and zero at all other vertices.
On an element, the three degrees of freedom uniquely determine
the linear basis functions in that element, as usual.
The global
$\basphi_i(x,y)$ function is then a combination of the linear functions
(planar surfaces)
over all the neighboring cells
that have vertex number $i$ in common. Figure ref{fem:approx:fe:2D:fig:basphi}
tries to illustrate the shape of such a ``pyramid''-like function.

FIGURE: [fig/demo2D_basisfunc, width=400] Example on a piecewise linear 2D basis function over a patch of triangles. label{fem:approx:fe:2D:fig:basphi}

=== Element matrices and vectors ===

As in 1D, we split the integral over $\Omega$ into a sum of integrals
over cells. Also as in 1D, $\basphi_i$ overlaps $\basphi_j$
(i.e., $\basphi_i\basphi_j\neq 0$) if and only if
$i$ and $j$ are vertices in the same cell. Therefore, the integral
of $\basphi_i\basphi_j$ over an element is nonzero only when $i$ and $j$
run over the vertex numbers in the element. These nonzero contributions
to the coefficient matrix are, as in 1D, collected in an element matrix.
The size of the element matrix becomes $3\times 3$ since there are
three degrees of freedom
that $i$ and $j$ run over. Again, as in 1D, we number the
local vertices in a cell, starting at 0, and add the entries in
the element matrix into the global system matrix, exactly as in 1D.
All details and code appear below.



===== Basis functions over triangles in the reference cell =====

As in 1D, we can define the basis functions and the degrees of freedom
in a reference cell and then use a mapping from the reference coordinate
system to the physical coordinate system.
We also need a mapping of local degrees of freedom numbers to global degrees
of freedom numbers.
# (`dof_map`).

The reference cell in an $(X,Y)$ coordinate system has vertices
$(0,0)$, $(1,0)$, and $(0,1)$, corresponding to local vertex numbers
0, 1, and 2, respectively. The P1 element has linear functions
$\refphi_r(X,Y)$ as basis functions, $r=0,1,2$.
Since a linear function $\refphi_r(X,Y)$ in 2D is on
the form $C_{r,0} + C_{r,1}X + C_{r,2}Y$, and hence has three
parameters $C_{r,0}$, $C_{r,1}$, and $C_{r,2}$, we need three
degrees of freedom. These are in general taken as the function values at a
set of nodes. For the P1 element the set of nodes is the three vertices.
Figure ref{fem:approx:fe:2D:fig:P12D} displays the geometry of the
element and the location of the nodes.

FIGURE: [fig/fenics-book/elements/P1_2d, width=100 frac=0.3] 2D P1 element. label{fem:approx:fe:2D:fig:P12D}

Requiring $\refphi_r=1$ at node number $r$ and
$\refphi_r=0$ at the two other nodes, gives three linear equations to
determine $C_{r,0}$, $C_{r,1}$, and $C_{r,2}$. The result is

!bt
\begin{align}
\refphi_0(X,Y) &= 1 - X - Y,\\
\refphi_1(X,Y) &= X,\\
\refphi_2(X,Y) &= Y
\end{align}
!et

Higher-order approximations are obtained by increasing the polynomial order,
adding additional nodes, and letting the degrees of freedom be
function values at the nodes. Figure ref{fem:approx:fe:2D:fig:P22D}
shows the location of the six nodes in the P2 element.

FIGURE: [fig/fenics-book/elements/P2_2d, width=100 frac=0.3] 2D P2 element. label{fem:approx:fe:2D:fig:P22D}

# 2DO: write up local basis funcs for P2

A polynomial of degree $p$ in $X$ and $Y$ has $n_p=(p+1)(p+2)/2$ terms
and hence needs $n_p$ nodes. The values at the nodes constitute $n_p$
degrees of freedom. The location of the nodes for
$\refphi_r$ up to degree 6 is displayed in Figure
ref{fem:approx:fe:2D:fig:P162D}.

FIGURE: [fig/fenics-book/elements/P1-6_2d, width=400 frac=1.0] 2D P1, P2, P3, P4, P5, and P6 elements. label{fem:approx:fe:2D:fig:P162D}

The generalization to 3D is straightforward: the reference element is a
"tetrahedron": "http://en.wikipedia.org/wiki/Tetrahedron"
with vertices $(0,0,0)$, $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$
in a $X,Y,Z$ reference coordinate system. The P1 element has its degrees
of freedom as four nodes, which are the four vertices, see Figure
ref{fem:approx:fe:2D:fig:P1:123D}. The P2 element adds additional
nodes along the edges of the cell, yielding a total of 10 nodes and
degrees of freedom, see
Figure ref{fem:approx:fe:2D:fig:P2:123D}.

FIGURE: [fig/fenics-book/elements/P1-1d2d3d, width=400 frac=1.0] P1 elements in 1D, 2D, and 3D. label{fem:approx:fe:2D:fig:P1:123D}

FIGURE: [fig/fenics-book/elements/P2-1d2d3d, width=400 frac=1.0] P2 elements in 1D, 2D, and 3D. label{fem:approx:fe:2D:fig:P2:123D}

idx{simplex elements} idx{simplices} idx{faces} idx{edges}

The interval in 1D, the triangle in 2D, the tetrahedron in 3D, and
its generalizations to higher space dimensions are known
as *simplex* cells (the geometry) or *simplex* elements (the geometry,
basis functions, degrees of freedom, etc.). The plural forms
"simplices": "http://en.wikipedia.org/wiki/Simplex" and
simplexes are
also a much used shorter terms when referring to this type of cells or elements.
The side of a simplex is called a *face*, while the tetrahedron also
has *edges*.


__Acknowledgment.__
Figures ref{fem:approx:fe:2D:fig:P12D}-ref{fem:approx:fe:2D:fig:P2:123D}
are created by Anders Logg and taken from the "FEniCS book": "https://launchpad.net/fenics-book": *Automated Solution of Differential Equations by the Finite Element Method*, edited by A. Logg, K.-A. Mardal, and G. N. Wells, published
by "Springer": "http://goo.gl/lbyVMH", 2012.



===== Affine mapping of the reference cell =====

Let $\refphi_r^{(1)}$ denote the basis functions associated
with the P1 element in 1D, 2D, or 3D, and let $\xdno{q(e,r)}$ be
the physical coordinates of local vertex number $r$ in cell $e$.
Furthermore,
let $\X$ be a point in the reference coordinate system corresponding
to the point $\x$ in the physical coordinate system.
The affine mapping of any $\X$ onto $\x$ is
then defined by

idx{affine mapping}

!bt
\begin{equation}
\x = \sum_{r} \refphi_r^{(1)}(\X)\xdno{q(e,r)},
label{fem:approx:fe:affine:map}
\end{equation}
!et
where $r$ runs over the local vertex numbers in the cell.
The affine mapping essentially stretches, translates, and rotates
the triangle. Straight or planar faces of the reference cell are
therefore mapped onto
straight or planar faces in the physical coordinate system. The mapping can
be used for both P1 and higher-order elements, but note that the
mapping itself always applies the P1 basis functions.

FIGURE: [fig/ElmT3n2D_map, width=400] Affine mapping of a P1 element. label{fem:approx:fe:map:fig:2DP1}


===== Isoparametric mapping of the reference cell =====

idx{isoparametric mapping} idx{mapping of reference cells!isoparametric mapping}

Instead of using the P1 basis functions in the mapping
(ref{fem:approx:fe:affine:map}),
we may use the basis functions of the actual P$d$ element:

!bt
\begin{equation}
\x = \sum_{r} \refphi_r(\X)\xdno{q(e,r)},
label{fem:approx:fe:isop:map}
\end{equation}
!et
where $r$ runs over all nodes, i.e., all points associated with the
degrees of freedom. This is called an *isoparametric mapping*.
For P1 elements it is identical to the affine mapping
(ref{fem:approx:fe:affine:map}), but for higher-order elements
the mapping of the straight or planar faces of the reference cell will
result in a *curved* face in the physical coordinate system.
For example, when we use the basis functions of the triangular P2 element
in 2D in (ref{fem:approx:fe:isop:map}), the straight faces of the
reference triangle are mapped onto curved faces of parabolic shape in
the physical coordinate system, see Figure ref{fem:approx:fe:map:fig:2DP2}.

FIGURE: [fig/ElmT6n2D_map, width=400] Isoparametric mapping of a P2 element. label{fem:approx:fe:map:fig:2DP2}

From (ref{fem:approx:fe:affine:map}) or
(ref{fem:approx:fe:isop:map}) it is easy to realize that the
vertices are correctly mapped. Consider a vertex with local number $s$.
Then $\refphi_s=1$ at this vertex and zero at the others.
This means that only one term in the sum is nonzero and $\x=\xdno{q(e,s)}$,
which is the coordinate of this vertex in the global coordinate system.

# #ifdef 2DO
idx{super-parametric mapping} idx{sub-parametric mapping}
idx{mapping of reference cells!super-parametric mapping}
idx{mapping of reference cells!sub-parametric mapping}

===== Other mappings =====

One may map the reference element onto the element in the physical space
using

#Need to cover rectangle and hexahedron too. Tensor-product elements.
#Draw in pysketcher the local coor sys and the global coor sys and use
#isoparametric mappings of triangles and quadrilaterals.

Make exercises. Test interpolation order of finite elements in 2D and 3D.
# #endif

===== Computing integrals =====

Let $\tilde\Omega^r$ denote the reference cell and $\Omega^{(e)}$
the cell in the physical coordinate system. The transformation of
the integral from the physical to the reference coordinate system reads

!bt
\begin{align}
\int_{\Omega^{(e)}}\basphi_i (\x) \basphi_j (\x) \dx &=
\int_{\tilde\Omega^r} \refphi_i (\X) \refphi_j (\X)
\det J\, \dX,\\
\int_{\Omega^{(e)}}\basphi_i (\x) f(\x) \dx &=
\int_{\tilde\Omega^r} \refphi_i (\X) f(\x(\X)) \det J\, \dX,
\end{align}
!et
where $\dx$ means the infinitesimal area element $dx dy$ in 2D and
$dx dy dz$ in 3D, with a similar
definition of $\dX$. The quantity $\det J$ is the determinant of the
Jacobian of the mapping $\x(\X)$. In 2D,

!bt
\begin{equation}
J = \left[\begin{array}{cc}
\frac{\partial x}{\partial X} & \frac{\partial x}{\partial Y}\\
\frac{\partial y}{\partial X} & \frac{\partial y}{\partial Y}
\end{array}\right], \quad
\det J = \frac{\partial x}{\partial X}\frac{\partial y}{\partial Y}
- \frac{\partial x}{\partial Y}\frac{\partial y}{\partial X}
\tp
label{fem:approx:fe:2D:mapping:J:detJ}
\end{equation}
!et
With the affine mapping
(ref{fem:approx:fe:affine:map}), $\det J=2\Delta$, where $\Delta$ is
the area or volume of the cell in the physical coordinate system.

__Remark.__
Observe that finite elements in 2D and 3D builds on the same
*ideas* and *concepts* as in 1D, but there is simply much
more to compute because the
specific mathematical formulas in 2D and 3D are more complicated
and the book keeping with dof maps also gets more complicated.
The manual work is tedious, lengthy, and error-prone
so automation by the computer is a must.


# 2DO
# First: two triangles
# vertices = [(0,0), (1,0), (0,1), (1,1)]
# cells = [[0, 1, 3], [0, 3, 2]]
# dof_map = cells
# write up affine mapping
# D is the area that sympy.Triangle can compute :-) No, do that directly! 0.5...
# rhs: choose simple f=x*y, try hand-calculation or two-step
# sympy: first integrate in y with (0,1-x) as limits, then
# integrate the result in x
# a = integrate(x*y*(1-x-y), (y, 0, 1-x))
# b = integrate(a, (x,0,1))
# use the same for local element matrix
# show assembly
# should have pysketcher prog for drawing 2D mesh, mark and number nodes
# and elements

# Should have example with x**8*(1-x)*y**8*(1-y) worked out, but
# need software

# Need 2D exercises

======= Implementation =======
label{fe:approx:fenics}

idx{FEniCS}

Our previous programs for doing 1D approximation by finite element
basis function had a focus on all the small details needed to compute
the solution. When going to 2D and 3D, the basic algorithms are the
same, but the amount of computational details with basis functions,
reference functions, mappings, numerical integration and so on,
becomes overwhelming because of all the flexibility and choices of
elements. For this purpose, we *must*, except in the simplest cases
with P1 elements, use some well-developed, existing computer
library. 
===== Example on approximation in 2D using FEniCS =====
label{fem:approx:fenics:2D}

Here we shall use "FEniCS": "http://fenicsproject.org", which
is a free, open finite element package for advanced computations. The
package can be programmed in C++ or Python. How it works is best
illustrated by an example.

=== Mathematical problem ===

We want to approximate the function $f(x)=2xy - x^2$ by P1 and P2 elements
on $[0,2]\times[-1,1]$ using a division into $8\times 8$ squares, which are
then divided into rectangles and then into triangles.

=== The code ===

Observe that the code employs the basic concepts from 1D, but is
capable of using *any* element in FEniCS on *any* mesh in *any* number of
space dimensions (!).

@@@CODE src/approx_fenics_2Dcase.py fromto: from fenics@ 

Figure ref{fem:approx:fenics:2D:fig1} shows the computed `u1`. The plots of
`u2` and `f` are identical and therefore not shown.
The plot shows that visually the approximation is quite close to 
`f`, but to quantify it more precisely we simply compute the
error using the function `errornorm`. The output
of errors becomes

!bc
L2 errors: e1=0.01314,   e2=4.93418e-15
L2 norms:  n1=4.46217,   n2=4.46219
!ec

Hence, the second order approximation `u2` is able to reproduce
`f` up to floating point precision, whereas the first
order approximation `u1` has an error of slightly more than $\frac{1}{3}$\%. 

# Remember to rotate PDF file from internal FEniCS plotting:
# pdftk dolfin_plot_1.pdf cat 1-endnorth output rotated_file.pdf

FIGURE: [fig/fenics_2D_plot_approx, width=400 frac=0.7] Plot of the computed approximation using Lagrange elements of second order. label{fem:approx:fenics:2D:fig1}


idx{`TestFunction`}
idx{`TrialFunction`}
idx{`FunctionSpace`}
idx{`solve` (FEniCS function)}
idx{`project` (FEniCS function)}
idx{`Expression`}

=== Dissection of the code ===

The function `approx` is a general solver function for any $f$ and
$V$.  We define the unknown $u$ in the variational form $a=a(u,v) = \int uv\dx$
as a `TrialFunction` object and the test function $v$ as a
`TestFunction` object. Then we define the variational form through
the integrand `u*v*dx`. The linear form $L$ is similarly defined as
`f*v*dx`. Here, `f` is an `Expression` object in FEniCS, i.e., a
formula defined in terms of a C++ expression. This expression is in turn
jit-compiled into a Python object for fast evaluation. With `a` and `L` defined,
we re-define `u` to be a finite element function `Function`, which is
now the unknown scalar field to be computed by the simple expression
`solve(a == L, u)`. We remark that the above function `approx`
is implemented in FEniCS (in a slightly more general fashion)
in the function  `project`.

The `problem` function applies `approx` to solve a specific problem.

=== Integrating SymPy and FEniCS ===

The definition of $f$ must be expressed in C++.  This part requires
two definitions: one of $f$ and one of $\Omega$, or more precisely:
the mesh (discrete $\Omega$ divided into cells).  The definition of
$f$ is here expressed in C++ (it will be compiled for fast
evaluation), where the independent coordinates are given by a C/C++
vector `x`. This means that $x$ is `x[0]`, $y$ is `x[1]`, and $z$ is
`x[2]`. Moreover, `x[0]**2` must be written as `pow(x[0], 2)` in
C/C++.

Fortunately, we can easily integrate SymPy and `Expression` objects,
because SymPy can take a formula and translate it to C/C++ code, and
then we can require a Python code to numerically evaluate the formula.
Here is how we can specify `f` in SymPy and use it in FEniCS as an
`Expression` object:

!bc pyshell
>>> import sympy as sym
>>> x, y = sym.symbols('x[0] x[1]')
>>> f = 2*x*y - x**2
>>> print(f)
-x[0]**2 + 2*x[0]*x[1]
>>> f = sym.printing.ccode(f)   # Translate to C code
>>> print(f)
-pow(x[0], 2) + 2*x[0]*x[1]
>>> import fenics as fe
>>> f = fe.Expression(f, degree=2)
!ec
Here, the function `ccode` generates C code and we use
`x` and `y` as placeholders for
`x[0]` and `x[1]`, which represent the coordinate of
a general point `x` in any dimension. The output of  `ccode`
can then be used directly in  `Expression`.

# #ifdef KENTEX
It is often challenging to determine the quality of an
approximation by visual inspection of 2D and 3D plots.
Line plots or pointwise evaluation makes quantification
easier. The following code shows how to evaluate
the approximation using 13 points along the line $y=1-x$.
Figure ref{fem:approx:fenics:plotalongline} displays
$f$ and the approximation using first order Lagrange elements.
Clearly, the approximation is very good and the
maximal error among the 13 points is $\approx 0.01$.

!bc pycod
# plot along line y=1-x, sample at 13 points
import numpy as np
point = np.array([0.0, 0.0])
value = np.array([0.0])
f_values = []
u1_values = []
for i in range(0, 13):
    point[0] = i/13.0
    point[1] = 1-point[0]
    f.eval(value, point)
    f_values.append(value[0])
    u1.eval(value, point)
    u1_values.append(value[0])
!ec
Note that we must define `x` as `x[0]` and `y` as `x[1]` for the
C++ syntax to be correct.

FIGURE: [fig/fenics_plot_along_line, width=400] Plot of the computed approximation using Lagrange elements of first order along the line $y=1-x$. label{fem:approx:fenics:plotalongline}
# #endif

===== Refined code with curve plotting =====
label{fem:approx:fenics:2D:2}

=== Interpolation and projection ===

The operation with defining `a`, `L`, and solving for a `u` is so
common that it has been implemented in the FEniCS function `project`:

!bc pycod
u = project(f, V)
!ec
So, there is no need for our `approx` function!

If we want to do interpolation (or collocation) instead, we simply do

!bc pycod
u = interpolate(f, V)
!ec

=== Plotting the solution along a line ===

Having `u` and `f` available as finite element functions (`Function`
objects), we can easily plot the solution along a line since FEniCS
has functionality for evaluating a `Function` at arbitrary points
*inside the domain*. For example, here is the code for plotting $u$ and
$f$ along a line $x=\hbox{const}$ or $y=\hbox{const}$.

@@@CODE src/approx_fenics.py fromto: import numpy@import fenics

=== Integrating plotting and computations ===

It is now very easy to give some graphical impression of the approximations
for various kinds of 2D elements.
Basically, to solve the problem of approximating $f=2xy-x^2$ on $\Omega = [-1,1]\times [0,2]$ by P2 elements on a $2\times 2$ mesh,
we want to integrate the function above with following type of computations:

!bc pycod
import fenics as fe
f = fe.Expression('2*x[0]*x[1] - pow(x[0], 2)', degree=2)
mesh = fe.RectangleMesh(fe.Point(1,-1), fe.Point(2,1), 2, 2)
V = fe.FunctionSpace(mesh, 'P', 2)
u = fe.project(f, V)
err = fe.errornorm(f, u, 'L2')
print(err)
!ec
However, we can now easily compare different type of elements and
mesh resolutions:

@@@CODE src/approx_fenics.py fromto: import fenics@
(We note that this code issues a lot of warnings from the `u(point)`
evaluations.)

We show in Figure ref{fem:approx:fenics:2D:2:fig1}
how $f$ is approximated by P0, P1, and P2 elements
on a very coarse $2\times 2$ mesh consisting of 8 cells.

We have also added the result obtained by P2 elements.

FIGURE: [fig/approx_fenics_f1, width=800 frac=1] Comparison of P0, P1, and P2 approximations (left to right) along a line in a 2D mesh. label{fem:approx:fenics:2D:2:fig1}

!bquestion Questions
There are two striking features in the figure:

 o The P2 solution is exact. Why?
 o The P1 solution does not seem to be a least squares approximation. Why?
!equestion

# 1. f is of degree 2 so V covers f and therefore the P2 solution is an exact
# of f everywhere.
# 2. P1 solution is along a line and the curve is more over than under
# f, but in other places it must be the other way around such that over
# the entire domain $\Omega$ we have a least squares approximation!

With this code, found in the file "`approx_fenics.py`": "$approx_fenics.py",
we can easily run lots of experiments with the Lagrange element family.
Just write the
SymPy expression and choose the mesh resolution!

======= Exercises =======

===== Problem: Define nodes and elements =====
label{fem:approx:fe:exer:mesh1}
file=fe_numberings1

Consider a domain $\Omega =[0,2]$ divided into the three elements
$[0,1]$, $[1,1.2]$, and $[1.2,2]$.

For P1 and P2 elements, set up the list of coordinates and nodes
(`nodes`) and the numbers of the nodes that belong to each element
(`elements`) in two cases: 1) nodes and elements numbered from left to
right, and 2) nodes and elements numbered from right to left.

!bsol
We can write up figure sketches and the data structure in code:

@@@CODE exer/fe_numberings1.py
!esol


===== Problem: Define vertices, cells, and dof maps =====
label{fem:approx:fe:exer:mesh2}
file=fe_numberings2

Repeat Problem ref{fem:approx:fe:exer:mesh1}, but define the
data structures `vertices`, `cells`, and `dof_map` instead of
`nodes` and `elements`.

!bsol
Written in Python, the solution becomes

@@@CODE exer/fe_numberings2.py
!esol

===== Problem: Construct matrix sparsity patterns =====
label{fem:approx:fe:exer:defmesh:sparsity}
file=fe_sparsity_pattern

Problem ref{fem:approx:fe:exer:mesh1} describes a element mesh
with a total of five elements, but with two different element and
node orderings. For each of the two orderings,
make a $5\times 5$ matrix and fill in the entries that will be nonzero.

!bhint
A matrix entry $(i,j)$ is nonzero if $i$ and $j$ are nodes in the
same element.
!ehint

!bsol
If we create an empty matrix, we can run through all elements and
then over all local node pairs and mark that the corresponding
entry $(i,j)$ in the global matrix is a nonzero entry.
The `elements` data structure is sufficient. Below is a program
that fills matrix entries with an `X` and prints the matrix sparsity
pattern.

@@@CODE exer/fe_sparsity_pattern.py
The output becomes

!bc
P1 elements, left-to-right numbering
X X 0 0
X X X 0
0 X X X
0 0 X X

P1 elements, right-to-left numbering
X X 0 0
X X X 0
0 X X X
0 0 X X

P2 elements, left-to-right numbering
X X X 0 0 0 0
X X X 0 0 0 0
X X X X X 0 0
0 0 X X X 0 0
0 0 X X X X X
0 0 0 0 X X X
0 0 0 0 X X X

P1 elements, right-to-left numbering
X X X 0 0 0 0
X X X 0 0 0 0
X X X X X 0 0
0 0 X X X 0 0
0 0 X X X X X
0 0 0 0 X X X
0 0 0 0 X X X
!ec
!esol

===== Problem: Perform symbolic finite element computations =====
label{fem:approx:fe:exer:Asinwt:symbolic}
file=fe_sin_P1

Perform symbolic calculations to find formulas for the coefficient
matrix and right-hand side when approximating $f(x) = \sin (x)$ on
$\Omega=[0, \pi]$ by two P1 elements of size $\pi/2$.  Solve the
system and compare $u(\pi/2)$ with the exact value 1.

!bsol
Here are suitable `sympy` commands:

@@@CODE exer/fe_sin_P1.py
Running the program, we get the matrix system

!bt
\[
\left[\begin{matrix}\frac{\pi}{6} & \frac{\pi}{12} & 0\\\frac{\pi}{12} & \frac{\pi}{3} & \frac{\pi}{12}\\0 & \frac{\pi}{12} & \frac{\pi}{6}\end{matrix}\right]
\left[\begin{matrix}\frac{1}{\pi} \left(- \frac{24}{\pi} + 8\right)\\\frac{-28 + \frac{168}{\pi}}{7 \pi}\\\frac{1}{\pi} \left(- \frac{24}{\pi} + 8\right)\end{matrix}\right]
=
\left[\begin{matrix}- \frac{2}{\pi} + 1\\\frac{4}{\pi}\\- \frac{2}{\pi} + 1\end{matrix}\right]
\]
!et
The solution at the midpoint is $1.15847$, i.e., 16% error.
!esol

# Hint: wolframalpha or sympy can help with (1-x)*sin(a*x+b),
# which is the integral
# that arises on the right-hand side.


===== Problem: Approximate a steep function by P1 and P2 elements =====
label{fem:approx:exer:tanh:P1P2}
file=fe_tanh_P1P2

Given

!bt
\begin{equation*} f(x) = \tanh(s(x-\half))\end{equation*}
!et
use the Galerkin or least squares method with finite elements to find
an approximate function $u(x)$. Choose $s=20$ and try
$N_e=4,8,16$ P1 elements and
$N_e=2,4,8$ P2 elements.
Integrate $f\basphi_i$ numerically.

!bhint
You can automate the computations by calling the `approximate` method
in the `fe_approx1D_numint` module.
!ehint

!bsol
The set of calls to `approximate` becomes

@@@CODE exer/fe_tanh_P1P2.py fromto: from fe_approx1D_@

FIGURE: [fig/fe_p1_tanh, width=800 frac=1]

FIGURE: [fig/fe_p2_tanh, width=800 frac=1]

!esol

===== Problem: Approximate a steep function by P3 and P4 elements =====
label{fem:approx:exer:tanh:P3P4}
file=fe_tanh_P3P4

!bsubex
Solve Problem ref{fem:approx:exer:tanh:P1P2} using $N_e=1,2,4$ P3 and P4
elements.

!bsol
We can easily adopt the code from Exercise ref{fem:approx:exer:tanh:P1P2}:

@@@CODE exer/fe_tanh_P3P4.py fromto: from fe_approx1D_@# Interpolation

FIGURE: [fig/fe_p3_tanh, width=800 frac=1]

FIGURE: [fig/fe_p4_tanh, width=800 frac=1]
!esol
!esubex

!bsubex
How will an interpolation method work in
this case with the same number of nodes?

!bsol
The coefficients arising from the interpolation method are trivial to compute
since $c_i=f(x_i)$, where $x_i$ are the global nodes. The function
`u_glob` in the `fe_approx1D_numint` module can be used to compute
appropriate arrays for plotting the resulting finite element function.
We create plots where the finite element approximation is shown along
with $f(x)$ and the interpolation points.
Since `u_glob` requires the `vertices`, `cells`, and `dof_map` data
structures, we must compute these for the values of number of
elements ($N_e$) and the polynomial degree ($d$).

@@@CODE exer/fe_tanh_P3P4.py fromto: # Interpolation@

FIGURE: [fig/tanh_fe_interpol_P3, width=800 frac=1]

FIGURE: [fig/tanh_fe_interpol_P4, width=800 frac=1]
!esol
!esubex

===== Exercise: Investigate the approximation error in finite elements =====
label{fem:approx:fe:exer:Asinwt:interpol:error}
file=Pd_approx_error

The theory (ref{fem:approx:fe:error:theorem}) from Section
ref{fem:approx:fe:error} predicts that the error in the P$d$
approximation of a function should behave as $h^{d+1}$, where $h$ is
the length of the element. Use experiments to verify this asymptotic
behavior (i.e., for small enough $h$).  Choose three examples:
$f(x)=Ae^{-\omega x}$ on $[0,3/\omega]$, $f(x) = A\sin (\omega x)$ on
$\Omega=[0, 2\pi/\omega]$ for constant $A$ and $\omega$, and
$f(x)=\sqrt{x}$ on $[0,1]$.

!bhint
Run a series of experiments: $(h_i,E_i)$, $i=0,\ldots,m$, where $E_i$
is the $L^2$ norm of the error corresponding to element length $h_i$.
Assume an error model $E=Ch^r$ and compute $r$ from two successive
experiments:

!bt
\[ r_i = \ln (E_{i+1}/E_i)/\ln (h_{i+1}/h_i),\quad i=0,\ldots,m-1\tp\]
!et
Hopefully, the sequence $r_0,\ldots,r_{m-1}$ converges to the true
$r$, and $r_{m-1}$ can be taken as an approximation to $r$.
Run such experiments for different $d$ for the different $f(x)$ functions.
!ehint

!bhint
The `approximate` function in `fe_approx1D_numint.py` is handy for
calculating the numerical solution. This function returns the
finite element solution as the coefficients $\sequencei{c}$.
To compute $u$, use `u_glob` from the same module.
Use the Trapezoidal rule to integrate the $L^2$ error:

!bc pycod
xc, u = u_glob(c, vertices, cells, dof_map)
e = f_func(xc) - u
L2_error = 0
e2 = e**2
for i in range(len(xc)-1):
    L2_error += 0.5*(e2[i+1] + e2[i])*(xc[i+1] - xc[i])
L2_error = np.sqrt(L2_error)
!ec
The reason for this Trapezoidal integration is
that `u_glob` returns coordinates `xc` and corresponding `u` values
where some of the coordinates (the cell vertices) coincides, because
the solution is computed in one element at a time, using all local
nodes. Also note that there are many coordinates in $xc$ per cell
such that we can accurately compute the error inside each cell.
!ehint

!bsol
Here is an appropriate program:

@@@CODE exer/Pd_approx_error.py fromto: from fe_approx1D_numint@
The output becomes

!bc
case=sqrt d=1, r: [1.0, 1.0, 1.0, 1.0, 1.0]
case=sqrt d=2, r: [1.0, 1.0, 1.0, 1.0, 1.0]
case=sqrt d=3, r: [1.0, 1.0, 1.0, 1.0, 1.0]
case=sqrt d=4, r: [1.0, 1.0, 1.0, 1.0, 1.0]
case=exp d=1, r: [2.01, 2.01, 2.0, 2.0, 2.0]
case=exp d=2, r: [2.81, 2.89, 2.94, 2.97, 2.98]
case=exp d=3, r: [3.98, 4.0, 4.0, 4.0, 4.0]
case=exp d=4, r: [4.87, 4.93, 4.96, 4.98, 4.99]
case=sin d=1, r: [2.15, 2.06, 2.02, 2.0, 2.0]
case=sin d=2, r: [2.68, 2.83, 2.93, 2.97, 2.99]
case=sin d=3, r: [4.06, 4.04, 4.01, 4.0, 4.0]
case=sin d=4, r: [4.79, 4.9, 4.96, 4.98, 4.99]
!ec
showing that the convergence rate stabilizes quite quickly at $N_e=128$
cells. While the theory predicts the rate as $d+1$, this is only
fulfilled for the exponential and sine functions, while the square root
functions gives a rate 1 regardless of $d$. The reason is that the
estimate (ref{fem:approx:fe:error:theorem}) contains the integral of
the derivatives of $f$ over $[0,1]$. For $f=\sqrt{x}$, we
have $f'=\half x^{-1/2}$, $f''=-\frac{1}{4}x^{-3/2}$, and all integrals
of $f''$ and higher derivatives are infinite on $[0,L]$. Our experiments
show that the method still converges, but $f$ is not smooth enough that
higher-order elements give superior convergence rates.
!esol

===== Problem: Approximate a step function by finite elements =====
label{fem:approx:fe:exer:Heaviside}
file=fe_Heaviside_P1P2

Approximate the step function

!bt
\begin{equation*} f(x) = \left\lbrace\begin{array}{ll}
0 & \mbox{ if } 0\leq x < \halfi,\\
1 & \mbox{ if } \halfi \leq x \geq \halfi
\end{array}\right.
\end{equation*}
!et
by 2, 4, 8, and 16 elements and  P1, P2, P3, and P4. Compare approximations visually.


!bhint
This $f$ can also be expressed in terms of the Heaviside function $H(x)$:
$f(x) = H(x-\halfi)$.
Therefore, $f$ can be defined by

!bc pycod
f = sym.Heaviside(x - sym.Rational(1,2))
!ec
making the `approximate` function in the
`fe_approx1D.py` module an obvious candidate to solve the
problem. However, `sympy` does not handle symbolic integration
with this particular integrand, and the `approximate` function faces a problem
when converting `f` to a Python function (for plotting) since
`Heaviside` is not an available function in `numpy`.

An alternative is to perform hand calculations. This is an instructive
task, but in practice only feasible for few elements and P1 and P2 elements.
It is better to copy the functions `element_matrix`, `element_vector`,
`assemble`, and `approximate` from the `fe_approx1D_numint.py` file
and edit these functions such that they can compute approximations
with `f` given as a Python function and not a symbolic expression.
Also assume that `phi` computed by the `basis` function is a Python
callable function. Remove all instances of the `symbolic` variable
and associated code.
!ehint

!bsol
The modifications of `element_matrix`, `element_vector`,
`assemble`, and `approximate` from the `fe_approx1D_numint.py` file
are listed below.

@@@CODE exer/fe_Heaviside_P1P2.py fromto: from fe_approx1D_numint@def exercise
With a purely numerical version of the `approximate` function, we can
easily investigate the suggested approximations in this exercise:

@@@CODE exer/fe_Heaviside_P1P2.py fromto: def exercise@if __name
Running this function reveals that even finite elements
(and not only sines, as demonstrated in Exercise ref{fem:approx:exer:Fourier})
give oscillations around a discontinuity.

FIGURE: [fig/fe_Heaviside_P1, width=800 frac=1]

FIGURE: [fig/fe_Heaviside_P2, width=800 frac=1]

FIGURE: [fig/fe_Heaviside_P3, width=800 frac=1]

FIGURE: [fig/fe_Heaviside_P4, width=800 frac=1]

__Remarks.__
It is of extreme importance to use a Gauss-Legendre numerical integration
rule that matches the degree of polynomials in the basis.
Using a rule with fewer points may lead to very strange results.

!esol

===== Exercise: 2D approximation with orthogonal functions =====
label{fem:approx:fe:exer:2Dsines:symbolic}
file=approx2D_ls_orth

!bsubex
Assume we have basis functions $\basphi_i(x,y)$ in 2D that are
orthogonal such that $(\basphi_i,\basphi_j)=0$ when $i\neq j$.  The
function `least_squares` in the file "`approx2D.py`":
"${fem_src}/fe_approx2D.py" will then spend much time on computing
off-diagonal terms in the coefficient matrix that we know are zero.
To speed up the computations, make a version `least_squares_orth` that
utilizes the orthogonality among the basis functions.

!bsol
We 1) remove the `j` loop in the `least_squares` function and set
`j = i`,
2) make `A` a vector (i.e., $(N+1, 1)$ matrix as `b` and `c`),
3) solve for `c[i,0]` as soon as `A[i,0]` and `b[i,0]` are computed.

@@@CODE exer/approx2D_ls_orth.py fromto: import sympy as sym@def sine_basis
!esol
!esubex

!bsubex
Apply the function to approximate

!bt
\[ f(x,y) = x(1-x)y(1-y)e^{-x-y}\]
!et
on $\Omega = [0,1]\times [0,1]$ via basis functions

!bt
\[ \basphi_i(x,y) = \sin ((p+1)\pi x)\sin((q+1)\pi y),\quad i=q(N_x+1) + p,
\]
!et
where $p=0,\ldots,N_x$ and $q=0,\ldots,N_y$.

!bhint
Get ideas from the function `least_squares_orth` in
Section ref{fem:approx:global:orth} and
file "`approx1D.py`": "${fem_src}/fe_approx1D.py".
!ehint

!bsol
A function for computing the basis functions may look like this:

@@@CODE exer/approx2D_ls_orth.py fromto: def sine_basis@def test_least

Application of this basis to approximate the given function is coded in
the following function:

@@@CODE exer/approx2D_ls_orth.py fromto: def demo@if__name
A lesson learned is that `symbolic=False` is important, otherwise `sympy`
consumes a lot of CPU time on trying to integrate symbolically.

The figure below shows the error in the approximation for $N=0$ (left)
and $N=2$ (right). The coefficients for $N=2$ decay rapidly:

!bc
[0.025, 0.0047, 0.0014, 0.0047, 0.0009, 0.0003, 0.0014, 0.0003,
 8.2e-5]
!ec

FIGURE: [fig/approx2D_ls_orth_sine_c, width=800 frac=1]

!esol
!esubex

!bsubex
Make a unit test for the `least_squares_orth` function.

!bsol
Let us use the basis in b), fix the coefficients of some function
$f$, and check that the computed approximation, with the
same basis, has the same coefficients (this test employs the principle
that if $f\in V$, then $u=f$).

@@@CODE exer/approx2D_ls_orth.py fromto: def test_least@def demo
!esol
!esubex

===== Exercise: Use the Trapezoidal rule and P1 elements =====
label{fem:approx:fe:exer:1D:trapez}
file=fe_P1_trapez

Consider approximation of some $f(x)$ on an interval $\Omega$ using
the least squares or Galerkin methods with P1 elements. Derive
the element matrix and vector using the
Trapezoidal rule (ref{fem:approx:fe:numint1:trapez}) for calculating
integrals on the reference element. Assemble the contributions, assuming
a uniform cell partitioning, and show that the resulting linear system
has the form $c_i=f(\xno{i})$ for $i\in\If$.

!bsol
The Trapezoidal rule for integrals on $[-1,1]$
is given by (ref{fem:approx:fe:numint1:trapez}).
The expressions for the entries in the element matrix
are given by (ref{fem:approx:fe:mapping:Ae}) in
Section ref{fem:approx:fe:mapping}:

!bt
\begin{align*} \tilde A^{(e)}_{r,s} &=
\int_{-1}^1 \refphi_r(X)\refphi_s(X)\det J\,\dX\\
&\approx \frac{h}{2}(\refphi_r(-1)\refphi_s(-1)
+ \refphi_r(1)\refphi_s(1))\tp
\end{align*}
!et
We know that if $\refphi_r(\pm 1)$ is 0 or 1, so evaluating
the formula above for $r,s=0,1$ gives

!bt
\[ \tilde A^{(e)} = \frac{h}{2}\left(\begin{array}{cc}
1 & 0\\
0 & 1
\end{array}\right)\tp\]
!et
As usual, $h$ is the length of the element in physical coordinates.

The element vector in the reference element is given by
(ref{fem:approx:fe:mapping:be}):

!bt
\begin{align*}
\tilde b^{(e)}_{r} &=  \int_{-1}^1 f(x(X))\refphi_r(X)\det J\,\dX\\
&\approx \frac{h}{2}(f(x(-1))\refphi_r(-1)
+ f(x(1))\refphi_r(1))\tp
\end{align*}
!et
Evaluating the formula for $r=0,1$ leads to

!bt
\[ \tilde b^{(e)} = \frac{h}{2}\left(\begin{array}{c}
f(x_L)\\
f(x_R)
\end{array}\right),\]
!et
where $x_L$ and $x_R$ are the $x$ coordinates of the local points
$X=-1$ and $X=1$, respectively.

With a uniform mesh with nodes $\xno{i}=ih$, the element matrix and
vectors assemble to a coefficient matrix

!bt
\[ \frac{h}{2}\hbox{diag}(1, 2, \ldots, 2, 1),\]
!et
and right-hand side vector

!bt
\[ \frac{h}{2}(f(\xno{0}), 2f(\xno{1}), \ldots, 2f(\xno{N_n-1}),
f(\xno{N_n}))\tp\]
!et
The factors $h/2$ and $2$ cancel, so we are left with the solution of
the system as

!bt
\[ c_i = f(\xno{i})\tp\]
!et
!esol

===== Exercise: Compare P1 elements and interpolation =====
label{fem:approx:fe:exer:1D:P1:vs:interp}
file=fe_P1_vs_interp

We shall approximate the function

!bt
\[ f(x) = 1 + \epsilon\sin (2\pi nx),\quad x\in \Omega = [0,1],\]
!et
where $n\in\Integer$ and $\epsilon \geq 0$.

!bsubex
Plot $f(x)$ for $n=1,2,3$ and find the wave length of the function.
!esubex

## lambda = 1/n

!bsubex
We want to use $N_P$ elements per wave length. Show that the number
of elements is then $nN_P$.
!esubex

## N_ph = lambda, 1/h=N_e

!bsubex
The critical quantity for accuracy is the number of elements per
wave length, not the element size in itself. It therefore suffices
to study an $f$ with just one wave length in $\Omega = [0,1]$.
Set $\epsilon = 0.5$.

Run the least squares or projection/Galerkin method for
$N_P=2,4,8,16,32$. Compute the error $E=||u-f||_{L^2}$.

!bhint
Use the `fe_approx1D_numint` module to compute $u$ and use
the technique from Section ref{fem:approx:fe:error} to
compute the norm of the error.
!ehint
!bhint
Read up on the NyquistShannon sampling theorem.
!ehint
!esubex

!bsubex
Repeat the set of experiments in the above point, but
use interpolation/collocation based on the node points to
compute $u(x)$ (recall that $c_i$ is now simply $f(\xno{i})$).
Compute the error $E=||u-f||_{L^2}$.
Which method seems to be most accurate?
!esubex

===== Exercise: Implement 3D computations with global basis functions =====
label{fem:approx:fe:exer:3D:approx3D}
file=approx3D

Extend the "`approx2D.py`": "${fem_src}/approx2D.py" code to 3D
applying ideas from Section ref{fem:approx:3D:global}.
Construct some 3D problem to make a test function for the
implementation.

!bhint
Drop symbolic integration since it is in general too slow for 3D problems.
Also use `scipy.integrate.nquad` instead of `mpmath.quad`
for numerical integration, since it is much faster.
!ehint

!bsol
We take a copy of `approx2D.py` and drop the `comparison_plot` function since
plotting in 3D is much more complicated (could make a special version with
curves through lines in the 3D domain, for instance).
Furthermore, we remove the lines with symbolic integration and replace
the calls to `mpmath.quad` by calls to
`scipy.integrate.nquad`. The resulting function becomes

@@@CODE exer/approx3D.py fromto: import sympy@def sine_basis

As test example, we can use the basis

!bt
\[ \baspsi_{p,q,r} = \sin((p+1)\pi x)\sin((q+1)\pi y)\sin((r+1)\pi z),\]
!et
for $p=1,\ldots,N_x$, $q=1,\ldots,N_y$, $r=1,\ldots,N_z$.
We choose $f$ as some prescribed combination of these functions and
check that the computed $u$ is exactly equal to $f$.

@@@CODE exer/approx3D.py fromto: def sine_basis@if __name
!esol

===== Exercise: Use Simpson's rule and P2 elements =====
label{fem:approx:fe:exer:1D:simpson}
file=fe_P2_simpson

Redo Exercise ref{fem:approx:fe:exer:1D:trapez}, but use P2
elements and Simpson's rule based on sampling the integrands at
the nodes in the reference cell.

!bsol
Simpson's rule for integrals on $[-1,1]$
is given by (ref{fem:approx:fe:numint1:Simpson}).
The expressions for the entries in the element matrix
are given by (ref{fem:approx:fe:mapping:Ae}):

!bt
\begin{align*} \tilde A^{(e)}_{r,s} &=
\int_{-1}^1 \refphi_r(X)\refphi_s(X)\det J\,\dX\\
&\approx \frac{1}{3}\frac{h}{2}(\refphi_r(-1)\refphi_s(-1)
+ 4\refphi_r(0)\refphi_s(0)
+ \refphi_r(1)\refphi_s(1))\tp
\end{align*}
!et
The expressions for $\refphi_r(X)$ are given by
(ref{fem:approx:fe:mapping:P1:phi0})-(ref{fem:approx:fe:mapping:P1:phi1}).
Evaluating the formula for $r,s=0,1,2$ gives the element matrix

!bt
\[ \tilde A^{(e)} = \frac{h}{6}\left(\begin{array}{ccc}
1 & 0 & 0\\
0 & 4 & 0\\
0 & 0 & 1
\end{array}\right)\tp\]
!et
As usual, $h$ is the length of the element in physical coordinates.

The element vector in the reference element is given by
(ref{fem:approx:fe:mapping:be}):

!bt
\begin{align*}
\tilde b^{(e)}_{r} &=  \int_{-1}^1 f(x(X))\refphi_r(X)\det J\,\dX\\
&\approx \frac{1}{3}\frac{h}{2}(f(x(-1))\refphi_r(-1)
+ 4f(x(0))\refphi_r(0)
+ f(x(1))\refphi_r(1))\tp
\end{align*}
!et
Evaluating the formula for $r=0,1,2$ leads to

!bt
\[ \tilde b^{(e)} = \frac{h}{2}\left(\begin{array}{c}
f(x_L)\\
4f(x_c)
f(x_R)
\end{array}\right),\]
!et
where $x_L$, $x_c$, and $x_R$ are the $x$ coordinates of the local points
$X=-1$, $X=0$, and $X=1$, respectively. These correspond to the nodes
in the element.

With a uniform mesh with nodes $\xno{i}=ih$, the element matrix and
vectors assemble to a coefficient matrix

!bt
\[ \frac{h}{6}\hbox{diag}(1, 4, 2, 4, 2, 4, \ldots, 2, 4, 1),\]
!et
and right-hand side vector

!bt
\[ \frac{h}{6}(f(\xno{0}), 4f(\xno{1}), 2f(\xno{2}),
4f(\xno{3}), 2f(\xno{4}), \ldots, 2f(\xno{N_n-2}),
4f(\xno{N_n-1}), f(\xno{N_n}))\tp\]
!et
The factors $h/6$, $2$ and $4$ all cancel, so we are left with the solution of
the system as

!bt
\[ c_i = f(\xno{i})\tp\]
!et
!esol

===== Exercise: Make a 3D code for Lagrange elements of arbitrary order =====

Extend the code from Section ref{fem:approx:fenics:2D:2} to 3D.
