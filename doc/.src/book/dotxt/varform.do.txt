# 2DO
# Must illustrate how to make weak form of continuous problem and
# discretize. Do that in time-dependent problems too.

# Maybe <,> \langle, \rangle as inner product

# Must say something about error estimates!


# no (au')', have (\alpha u')' - it solves all the problems with a and a(.,.)

========= Variational formulations with global basis functions =========
label{ch:varform:global}

======= Basic principles for approximating differential equations =======
label{fem:deq:1D:principles}

The finite element method is a very flexible approach for solving partial
differential equations. Its two most attractive features are the ease
of handling domains of complex shape in two and three dimensions and
the ease of using higher-degree polynomials in the approximations.
The latter feature typically leads to errors proportional to
$h^{d+1}$, where $h$ is the element length and $d$ is the polynomial
degree. When the solution is sufficiently smooth, the ability to use
larger $d$ creates methods that are much more computationally efficient
than standard finite difference methods (and equally efficient finite
difference methods are technically much harder to construct).

The finite element method is usually applied for discretization in
space, and therefore spatial problems will be our focus in the coming
sections.  Extensions to time-dependent problems usually employs
finite difference approximations in time.

Before studying how finite element methods are used to tackle
differential equations, we first look at how global basis functions
and the least squares, Galerkin, and collocation principles can be
used to solve differential equations.

===== Differential equation models =====
label{fem:deq:1D:models}

Let us consider an abstract differential equation for a function $u(x)$ of
one variable, written as

!bt
\begin{equation}
\mathcal{L}(u) = 0,\quad x\in\Omega\tp  \end{equation}
!et
Here are a few examples on possible choices of $\mathcal{L}(u)$, of
increasing complexity:

!bt
\begin{align}
\mathcal{L}(u) &= \frac{d^2u}{dx^2} - f(x),
label{fem:deq:1D:L1}\\
\mathcal{L}(u) &= \frac{d}{dx}\left(\dfc(x)\frac{du}{dx}\right) + f(x),
label{fem:deq:1D:L2}\\
\mathcal{L}(u) &= \frac{d}{dx}\left(\dfc(u)\frac{du}{dx}\right) - au + f(x),
label{fem:deq:1D:L3}\\
\mathcal{L}(u) &= \frac{d}{dx}\left(\dfc(u)\frac{du}{dx}\right) + f(u,x)
label{fem:deq:1D:L4}
\tp
\end{align}
!et
Both $\dfc(x)$ and $f(x)$ are considered as specified functions,
while $a$ is a prescribed parameter.  Differential equations
corresponding to (ref{fem:deq:1D:L1})-(ref{fem:deq:1D:L2}) arise in
diffusion phenomena, such as stationary (time-independent)
transport of heat in solids and
flow of viscous fluids between flat plates. The form
(ref{fem:deq:1D:L3}) arises when transient diffusion or wave
phenomena are discretized in time by finite differences. The equation
(ref{fem:deq:1D:L4}) appears in chemical models when diffusion of a
substance is combined with chemical reactions. Also in biology,
(ref{fem:deq:1D:L4}) plays an important role, both for spreading of
species and in models involving generation and
propagation of electrical signals.

Let $\Omega =[0,L]$ be the domain in one space dimension.
In addition to the differential equation, $u$ must fulfill
boundary conditions at the boundaries of the domain, $x=0$ and $x=L$.
When $\mathcal{L}$ contains up to second-order derivatives, as in the
examples above, we need one boundary condition at each of
the (two) boundary points, here abstractly specified as

!bt
\begin{equation}
\mathcal{B}_0(u)=0,\ x=0,\quad \mathcal{B}_1(u)=0,\ x=L
\end{equation}
!et

There are three common choices of boundary conditions:

!bt
\begin{align}
\mathcal{B}_i(u) &= u - g,\quad &\hbox{Dirichlet condition}\\
\mathcal{B}_i(u) &= -\dfc \frac{du}{dx} - g,\quad &\hbox{Neumann condition}\\
\mathcal{B}_i(u) &= -\dfc \frac{du}{dx} - H(u-g),\quad &\hbox{Robin condition}
\end{align}
!et
Here, $g$ and $H$ are specified quantities.

From now on we shall use $\uex(x)$ as symbol for the *exact* solution,
fulfilling

!bt
\begin{equation}
\mathcal{L}(\uex)=0,\quad x\in\Omega,
\end{equation}
!et
while $u(x)$ is our notation for an *approximate* solution of the differential
equation.

!bnotice Remark on notation
In the literature about the finite element method,
it is common to use $u$ as the exact solution and $u_h$ as the
approximate solution, where $h$ is a discretization parameter. However,
the vast part of the present text is about the approximate solutions,
and having a subscript $h$ attached all the time
is cumbersome. Of equal importance is the close correspondence between
implementation and mathematics that we strive to achieve in this text:
when it is natural to use `u` and not `u_h` in
code, we let the mathematical notation be dictated by the code's
preferred notation. In the relatively few cases where we need to work
with the exact solution of the PDE problem we call it $\uex$ in
mathematics and `u_e` in the code (the function for computing
`u_e` is named `u_exact`).
#After all, it is the powerful computer implementations
#of the finite element method that justifies studying the mathematical
#formulation and aspects of the method.
!enotice

===== Simple model problems and their solutions =====
label{fem:deq:1D:models:simple}

A common model problem used much in the forthcoming examples is


!bt
\begin{equation}
-u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u(0)=0,\ u(L)=D
\tp
label{fem:deq:1D:model1}
\end{equation}
!et
A closely related problem with a different boundary condition at
$x=0$ reads

!bt
\begin{equation}
-u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u'(0)=C,\ u(L)=D\tp
label{fem:deq:1D:model2}
\end{equation}
!et
A third variant has a variable coefficient,

!bt
\begin{equation}
-(\dfc(x)u'(x))' = f(x),\quad x\in\Omega=[0,L],\quad u'(0)=C,\ u(L)=D\tp
label{fem:deq:1D:model3}
\end{equation}
!et

The solution $u$ to the model problem (ref{fem:deq:1D:model1})
can be determined as
!bt
\begin{align*}
u'(x) &= -\int_0^x f(x) + c_0, \\
u(x) &= \int_0^x u'(x) + c_1,
\end{align*}
!et
where $c_0$ and $c_1$ are determined by the boundary conditions
such that $u'(0) = C$ and $u(L) = D$.

Computing the solution is easily done
using `sympy`. Some common code is defined first:

@@@CODE src/u_xx_f_sympy.py fromto: import sympy@def model1
The following function computes the solution
symbolically for the model problem (ref{fem:deq:1D:model1}):

@@@CODE src/u_xx_f_sympy.py fromto: def model1@def model2
Calling `model1(2, L, D)` results in the solution

!bt
\begin{equation}
u(x) = \frac{1}{L}x \left(D + L^{2} - L x\right)
label{fem:deq:1D:model1:sol}
\end{equation}
!et
The model problem (ref{fem:deq:1D:model2}) can be solved by

@@@CODE src/u_xx_f_sympy.py fromto: def model2@def model3
to yield

!bt
\begin{equation}
u(x) = - x^{2} + C x - C L + D + L^{2},
label{fem:deq:1D:model2:sol}
\end{equation}
!et
if $f(x)=2$. Model (ref{fem:deq:1D:model3}) requires a bit more involved
code,

@@@CODE src/u_xx_f_sympy.py fromto: def model3@def test1
With $f(x)=0$ and $\dfc(x)=1+x^2$ we get

!bt
\[ u(x) =
\frac{C \tan^{-1}\left (L \right ) - C \tan^{-1}\left (x \right ) + D \tan^{-1}\left (x \right )}{\tan^{-1}\left (L \right )}
\]
!et

===== Forming the residual =====
label{fem:deq:1D:residual:min}

The fundamental idea is to seek an approximate solution
$u$ in some space $V$,

!bt
\begin{equation*}
V = \hbox{span}\{ \baspsi_0(x),\ldots,\baspsi_N(x)\},
\end{equation*}
!et
which means that $u$ can always be expressed as a linear combination
of the basis functions $\sequencej{\baspsi}$, with $\If$ as
the index set $\{0,\ldots,N\}$:

!bt
\begin{equation*} u(x) = \sum_{j\in\If} c_j\baspsi_j(x)\tp\end{equation*}
!et
The coefficients $\sequencej{c}$ are unknowns to be computed.

(Later, in Section ref{fem:deq:1D:essBC}, we will see that if we specify boundary values of $u$ different
from zero, we must look for an approximate solution
$u(x) = B(x) + \sum_{j} c_j\baspsi_j(x)$,
where $\sum_{j}c_j\baspsi_j\in V$ and $B(x)$ is some function for
incorporating the right boundary values. Because of $B(x)$, $u$ will not
necessarily lie in $V$. This modification does not imply any difficulties.)

We need principles for deriving $N+1$ equations to determine the
$N+1$ unknowns $\sequencei{c}$.
When approximating a given function $f$ by $u=\sum_jc_j\basphi_j$,
a key idea is to minimize the square norm of the
approximation error $e=u-f$ or (equivalently) demand that $e$ is
orthogonal to $V$. Working with $e$ is not so useful here since
the approximation error in our case is $e=\uex - u$ and $\uex$ is
unknown. The only general indicator we have on the quality of the approximate
solution is to what degree $u$ fulfills the differential equation.
Inserting $u=\sum_j c_j \baspsi_j$ into $\mathcal{L}(u)$ reveals that the
result is not zero, because $u$ in general is an approximation and not identical to $\uex$.
The nonzero result,

idx{residual}

!bt
\begin{equation}
R = \mathcal{L}(u) = \mathcal{L}(\sum_j c_j \baspsi_j),
\end{equation}
!et
is called the *residual* and measures the
error in fulfilling the governing equation.

Various principles for determining $\sequencej{c}$ try to minimize
$R$ in some sense. Note that $R$ varies with $x$ and
the $\sequencej{c}$ parameters. We may write this dependence
explicitly as

!bt
\begin{equation}
R = R(x; c_0, \ldots, c_N)\tp  \end{equation}
!et
Below, we present three principles for making $R$ small:
a least squares method, a projection or Galerkin method, and
a collocation or interpolation method.

===== The least squares method =====

The least-squares method aims to find $\sequencei{c}$ such that
the square norm of the residual

!bt
\begin{equation}
||R|| = (R, R) = \int_{\Omega} R^2 \dx
\end{equation}
!et
is minimized. By introducing
an inner product of two functions $f$ and $g$
on $\Omega$ as

!bt
\begin{equation}
(f,g) = \int_{\Omega} f(x)g(x) \dx,
\end{equation}
!et
the least-squares method can be defined as

!bt
\begin{equation}
\min_{c_0,\ldots,c_N} E = (R,R)\tp  \end{equation}
!et
Differentiating with respect to the free parameters $\sequencei{c}$
gives the $N+1$ equations

!bt
\begin{equation}
\int_{\Omega} 2R\frac{\partial R}{\partial c_i} \dx = 0\quad
\Leftrightarrow\quad (R,\frac{\partial R}{\partial c_i})=0,\quad
i\in\If\tp
label{fem:deq:1D:LS:eq1}
\end{equation}
!et

===== The Galerkin method =====

The least-squares
principle is equivalent to demanding the error to be orthogonal to
the space $V$ when approximating a function $f$ by $u\in V$.
With a differential equation
we do not know the true error so we must instead require the residual $R$
to be orthogonal to $V$. This idea implies
seeking $\sequencei{c}$ such that

!bt
\begin{equation}
(R,v)=0,\quad \forall v\in V\tp
label{fem:deq:1D:Galerkin0}
\end{equation}
!et
This is the Galerkin method for differential equations.


#As shown in (ref{fem:approx:vec:Np1dim:Galerkin}) and (ref{fem:approx:vec:Np1dim:Galerkin0}),
The above abstract statement can be made concrete by choosing a concrete basis.
For example, the statement is equivalent to $R$ being orthogonal to the $N+1$
basis functions $\{\baspsi_i\}$ spanning $V$ (and this is
the most convenient way to express (ref{fem:deq:1D:Galerkin0}):

!bt
\begin{equation}
(R,\baspsi_i)=0,\quad i\in\If,
label{fem:deq:1D:Galerkin}
\end{equation}
!et
resulting in $N+1$ equations for determining $\sequencei{c}$.

===== The Method of Weighted Residuals =====

idx{weighted residuals}
idx{method of weighted residuals}

A generalization of the Galerkin method is to demand that $R$
is orthogonal to some space $W$, but not necessarily the same
space as $V$ where we seek the unknown function.
This generalization is called the *method of weighted residuals*:

!bt
\begin{equation}
(R,v)=0,\quad \forall v\in W\tp
label{fem:deq:1D:WRM0}
\end{equation}
!et
If $\{w_0,\ldots,w_N\}$ is a basis for $W$, we can equivalently
express the method of weighted residuals as

!bt
\begin{equation}
(R,w_i)=0,\quad i\in\If\tp
label{fem:deq:1D:WRM}
\end{equation}
!et
The result is $N+1$ equations for $\sequencei{c}$.

The least-squares method can also be viewed as a weighted residual
method with $w_i = \partial R/\partial c_i$.

idx{variational formulation}
idx{weak formulation}

!bnotice Variational formulation of the continuous problem
Statements like (ref{fem:deq:1D:Galerkin0}), (ref{fem:deq:1D:Galerkin}),
(ref{fem:deq:1D:WRM0}), or
(ref{fem:deq:1D:WRM}))
are known as
"weak formulations": "https://en.wikipedia.org/wiki/Weak_formulation"
or *variational formulations*.
These equations are in this text primarily used for a numerical approximation
$u\in V$, where $V$ is a *finite-dimensional* space with dimension
$N+1$. However, we may also let the exact solution $\uex$ fulfill a
variational formulation $(\mathcal{L}(\uex),v)=0$ $\forall v\in V$,
but the exact solution lies in general in a space with infinite
dimensions (because an infinite number of parameters are needed to
specify the solution). The variational formulation for $\uex$
in an infinite-dimensional space $V$ is
a mathematical way of stating the problem and acts as an
alternative to the usual (strong) formulation of a differential equation with
initial and/or boundary conditions.

Much of the literature on finite
element methods takes a differential equation problem and first
transforms it to a variational formulation in an infinite-dimensional space
$V$, before searching for an approximate solution in a finite-dimensional
subspace of $V$. However, we prefer the more intuitive approach with an
approximate solution $u$ in a finite-dimensional space $V$ inserted in
the differential equation, and then the resulting residual is demanded to be
orthogonal to $V$.
!enotice


!bnotice Remark on terminology
The terms weak or variational formulations often refer to a statement like
(ref{fem:deq:1D:Galerkin0}) or (ref{fem:deq:1D:WRM0})
after *integration by parts* has been performed (the integration by
parts technique is
explained in Section ref{fem:deq:1D:varform}).
The result after
integration by parts is what is obtained after taking the *first
variation* of a minimization problem (see
Section ref{fem:deq:1D:optimization}).
However, in this text we use variational formulation as a common term for
formulations which, in contrast to the differential equation $R=0$,
instead demand that an average of $R$ is zero: $(R,v)=0$ for all $v$ in some space.
!enotice

===== Test and Trial Functions =====

idx{trial function} idx{test function} idx{trial space} idx{test space}

In the context of the Galerkin method and the method of weighted residuals it is
common to use the name *trial function* for the approximate $u =
\sum_j c_j \baspsi_j$.
# Sometimes the functions that spans the space where $u$ lies are also called
# trial functions.
The space containing the trial function is known as the *trial space*.
The function $v$ entering the orthogonality requirement in
the Galerkin method and the method of weighted residuals is called
*test function*, and so are the $\baspsi_i$ or $w_i$ functions that are
used as weights in the inner products with the residual.  The space
where the test functions comes from is naturally called the
*test space*.

We see that in the method of weighted residuals the test and trial spaces
are different and so are the test and trial functions.
In the Galerkin method the test and trial spaces are the same (so far).
#Later in Section ref{fem:deq:1D:essBC} we shall see that boundary
#conditions may lead to a difference between the test and trial spaces
#in the Galerkin method.


===== The collocation method =====

The idea of the collocation method is to demand that $R$ vanishes
at $N+1$ selected points $\xno{0},\ldots,\xno{N}$ in $\Omega$:

!bt
\begin{equation}
R(\xno{i}; c_0,\ldots,c_N)=0,\quad i\in\If\tp
label{fem:deq:1D:collocation}
\end{equation}
!et
The collocation method can also be viewed as a method of weighted residuals
with Dirac delta functions as weighting functions.
Let $\delta (x-\xno{i})$ be the Dirac delta function centered around
$x=\xno{i}$ with the properties that $\delta (x-\xno{i})=0$ for $x\neq \xno{i}$
and

!bt
\begin{equation}
\int_{\Omega} f(x)\delta (x-\xno{i}) \dx =
f(\xno{i}),\quad \xno{i}\in\Omega\tp
label{fem:deq:1D:Dirac}
\end{equation}
!et
Intuitively, we may think of $\delta (x-\xno{i})$ as a very peak-shaped
function around $x=\xno{i}$ with an integral $\int_{-\infty}^\infty \delta(x-\xno{i})dx$ that evaluates to unity. Mathematically, it can be shown that
$\delta (x-\xno{i})$ is the limit of a Gaussian function centered at
$x=\xno{i}$ with a standard deviation that approaches zero.
Using this latter model, we can roughly visualize delta functions as
done in Figure ref{fem:deq:1D:fig:Dirac}.
Because of (ref{fem:deq:1D:Dirac}), we can let $w_i=\delta(x-\xno{i})$
be weighting functions in the method of weighted residuals,
and (ref{fem:deq:1D:WRM}) becomes equivalent to
(ref{fem:deq:1D:collocation}).

FIGURE: [fig/delta_func_weight, width=400] Approximation of delta functions by narrow Gaussian functions. label{fem:deq:1D:fig:Dirac}


=== The subdomain collocation method ===

The idea of this approach is to demand the integral of $R$ to vanish
over $N+1$ subdomains $\Omega_i$ of $\Omega$:

!bt
\begin{equation}
\int_{\Omega_i} R\, \dx=0,\quad i\in\If\tp  \end{equation}
!et
This statement can also be expressed as a weighted residual method

!bt
\begin{equation}
\int_{\Omega} Rw_i\, \dx=0,\quad i\in\If, \end{equation}
!et
where $w_i=1$ for $x\in\Omega_i$ and $w_i=0$ otherwise.


===== Examples on using the principles =====
label{fem:deq:1D:ex:sines}

Let us now apply global basis functions to illustrate the different
principles for making the residual $R$ small.

=== The model problem ===

We consider the differential equation problem

!bt
\begin{equation}
-u''(x) = f(x),\quad x\in\Omega=[0,L],\quad u(0)=0,\ u(L)=0
\tp
label{fem:deq:1D:model1b}
\end{equation}
!et

=== Basis functions ===

Our choice of basis functions $\baspsi_i$
for $V$ is

!bt
\begin{equation}
\baspsi_i(x) = \sinL{i},\quad i\in\If\tp
label{fem:deq:1D:ex:sines:psi}
\end{equation}
!et

An important property of these functions is that $\baspsi_i(0)=\baspsi_i(L)=0$,
which means that the boundary conditions on $u$ are fulfilled:

!bt
\[ u(0) = \sum_jc_j\baspsi_j(0) = 0,\quad u(L) = \sum_jc_j\baspsi_j(L) =0
\tp \]
!et
Another nice property is that the chosen sine functions
are orthogonal on $\Omega$:

!bt
\begin{equation}
\int\limits_0^L \sinL{i}\sinL{j}\, \dx = \left\lbrace
\begin{array}{ll} \half L & i=j  \\ 0, & i\neq j
\end{array}\right.
\end{equation}
!et
provided $i$ and $j$ are integers.

# Sympy can do this!
# k, m, n = symbols('k m n', integer=True)
#>>> integrate(sin(k*x)*sin(m*x), (x, 0, 2*pi))
#0
#>>>integrate(sin(k*x)*sin(k*x), (x, 0, 2*pi))
#pi

=== The residual ===

We can readily calculate the following explicit expression for the
residual:

!bt
\begin{align}
R(x;c_0, \ldots, c_N) &= u''(x) + f(x),\nonumber\\
&= \frac{d^2}{dx^2}\left(\sum_{j\in\If} c_j\baspsi_j(x)\right)
+ f(x),\nonumber\\
&= \sum_{j\in\If} c_j\baspsi_j''(x) + f(x)\tp
label{fem:deq:1D:ex:sines:res}
\end{align}
!et

=== The least squares method ===

The equations (ref{fem:deq:1D:LS:eq1})
in the least squares method require an expression for
$\partial R/\partial c_i$. We have

!bt
\begin{equation}
\frac{\partial R}{\partial c_i} =
\frac{\partial}{\partial c_i}
\left(\sum_{j\in\If} c_j\baspsi_j''(x) + f(x)\right)
= \sum_{j\in\If} \frac{\partial c_j}{\partial c_i}\baspsi_j''(x)
= \baspsi_i''(x)\tp  \end{equation}
!et
The governing equations for the unknown parameters $\sequencej{c}$ are then

!bt
\begin{equation}
(\sum_j c_j \baspsi_j'' + f,\baspsi_i'')=0,\quad i\in\If,
\end{equation}
!et
which can be rearranged as

!bt
\begin{equation}
\sum_{j\in\If}(\baspsi_i'',\baspsi_j'')c_j = -(f,\baspsi_i''),\quad i\in\If\tp
\end{equation}
!et
This is nothing but a linear system

!bt
\begin{equation*}
\sum_{j\in\If}A_{i,j}c_j = b_i,\quad i\in\If\tp
\end{equation*}
!et
The entries in the coefficient matrix are given by

!bt
\begin{align*}
A_{i,j} &= (\baspsi_i'',\baspsi_j'')\nonumber\\
& = \pi^4(i+1)^2(j+1)^2L^{-4}\int_0^L \sinL{i}\sinL{j}\, \dx
\end{align*}
!et
The orthogonality of the sine functions simplify the coefficient matrix:

!bt
\begin{equation}
A_{i,j} = \left\lbrace \begin{array}{ll}
{1\over2}L^{-3}\pi^4(i+1)^4 & i=j  \\
0,                          & i\neq j
\end{array}\right.
\end{equation}
!et
The right-hand side reads

!bt
\begin{equation}
b_i = -(f,\baspsi_i'') = (i+1)^2\pi^2L^{-2}\int_0^Lf(x)\sinL{i}\, \dx
\end{equation}
!et
Since the coefficient matrix is diagonal we can easily solve for

!bt
\begin{equation}
c_i = \frac{2L}{\pi^2(i+1)^2}\int_0^Lf(x)\sinL{i}\, \dx\tp
label{fem:deq:1D:ex:sines:solution}
\end{equation}
!et
With the special choice of $f(x)=2$, the coefficients
can be calculated in `sympy` by

!bc pycod
import sympy as sym

i, j = sym.symbols('i j', integer=True)
x, L = sym.symbols('x L')
f = 2
a = 2*L/(sym.pi**2*(i+1)**2)
c_i = a*sym.integrate(f*sym.sin((i+1)*sym.pi*x/L), (x, 0, L))
c_i = simplify(c_i)
print c_i
!ec
The answer becomes

!bt
\begin{equation*}
c_i = 4 \frac{L^{2} \left(\left(-1\right)^{i} + 1\right)}{\pi^{3}
\left(i^{3} + 3 i^{2} + 3 i + 1\right)}
\end{equation*}
!et
Now, $1+(-1)^i=0$ for $i$ odd, so only the coefficients with even index
are nonzero. Introducing $i=2k$ for $k=0,\ldots,N/2$ to count the
relevant indices (for $N$ odd, $k$ goes to $(N-1)/2$), we get the solution

!bt
\begin{equation}
u(x) = \sum_{k=0}^{N/2} \frac{8L^2}{\pi^3(2k+1)^3}\sinL{2k}\tp  \end{equation}
!et
The coefficients decay very fast: $c_2 = c_0/27$, $c_4=c_0/125$.
The solution will therefore be dominated by the first term,

!bt
\begin{equation*} u(x) \approx \frac{8L^2}{\pi^3}\sin\left(\pi\frac{x}{L}\right)\tp  \end{equation*}
!et


=== The Galerkin method ===

The Galerkin principle (ref{fem:deq:1D:Galerkin0})
applied to (ref{fem:deq:1D:model1b}) consists of inserting
our special residual (ref{fem:deq:1D:ex:sines:res}) in
(ref{fem:deq:1D:Galerkin0})

!bt
\begin{equation*}
(u''+f,v)=0,\quad \forall v\in V,
\end{equation*}
!et
or

!bt
\begin{equation}
(u'',v) = -(f,v),\quad\forall v\in V\tp  \end{equation}
!et
This is the variational formulation, based on the Galerkin principle,
of our differential equation.
The $\forall v\in V$ requirement is equivalent to
demanding the equation $(u'',v) = -(f,v)$ to be fulfilled for all
basis functions $v=\baspsi_i$, $i\in\If$, see
(ref{fem:deq:1D:Galerkin0}) and (ref{fem:deq:1D:Galerkin}).
We therefore have

!bt
\begin{equation}
(\sum_{j\in\If} c_j\baspsi_j'', \baspsi_i)=-(f,\baspsi_i),\quad i\in\If\tp  \end{equation}
!et
This equation can be rearranged to a form that explicitly shows
that we get a linear system for the unknowns $\sequencej{c}$:

!bt
\begin{equation}
\sum_{j\in\If} (\baspsi_i,\baspsi_j'')c_j = (f, \baspsi_i),\quad i\in\If\tp  \end{equation}
!et
For the particular choice of the basis functions (ref{fem:deq:1D:ex:sines:psi})
we get in fact the same linear system
as in the least squares method
because $\baspsi''= -(i+1)^2\pi^2L^{-2}\baspsi$.
Consequently, the solution $u(x)$ becomes identical to the one produced
by the least squares method.

=== The collocation method ===

For the collocation method (ref{fem:deq:1D:collocation}) we need to
decide upon a set of $N+1$ collocation points in $\Omega$. A simple
choice is to use uniformly spaced points: $\xno{i}=i\Delta x$, where
$\Delta x = L/N$ in our case ($N\geq 1$). However, these points
lead to at least two rows in the matrix consisting of zeros
(since $\baspsi_i(\xno{0})=0$ and $\baspsi_i(\xno{N})=0$), thereby making the matrix
singular and non-invertible. This forces us to choose some other
collocation points, e.g., random points or points uniformly distributed
in the interior of $\Omega$.
Demanding the residual to vanish
at these points leads, in our model problem (ref{fem:deq:1D:model1b}), to
the equations

!bt
\begin{equation}
-\sum_{j\in\If} c_j\baspsi_j''(\xno{i}) = f(\xno{i}),\quad i\in\If,
\end{equation}
!et
which is seen to be a linear system with entries

!bt
\begin{equation*} A_{i,j}=-\baspsi_j''(\xno{i})=
(j+1)^2\pi^2L^{-2}\sin\left((j+1)\pi \frac{x_i}{L}\right),\end{equation*}
!et
in the coefficient matrix and entries
$b_i=2$ for the right-hand side (when $f(x)=2$).

The special case of $N=0$
can sometimes be of interest. A natural choice is then the midpoint
$\xno{0}=L/2$ of the domain, resulting in
$A_{0,0} = -\baspsi_0''(\xno{0}) = \pi^2L^{-2}$, $f(x_0)=2$,
and hence $c_0=2L^2/\pi^2$.


=== Comparison ===

In the present model problem, with $f(x)=2$, the exact solution is
$u(x)=x(L-x)$, while for $N=0$ the Galerkin and least squares method
result in $u(x)=8L^2\pi^{-3}\sin (\pi x/L)$ and the
collocation method leads to $u(x)=2L^2\pi^{-2}\sin (\pi x/L)$.
We can quickly use `sympy` to verify that the maximum error
occurs at the midpoint $x=L/2$ and find what the errors are.
First we set up the error expressions:


!bc pyshell
>>> import sympy as sym
>>> # Computing with Dirichlet conditions: -u''=2 and sines
>>> x, L = sym.symbols('x L')
>>> e_Galerkin = x*(L-x) - 8*L**2*sym.pi**(-3)*sym.sin(sym.pi*x/L)
>>> e_colloc = x*(L-x) - 2*L**2*sym.pi**(-2)*sym.sin(sym.pi*x/L)
!ec
If the derivative of the errors vanish at $x=L/2$, the errors reach
their maximum values here (the errors vanish at the boundary points).

!bc pyshell
>>> dedx_Galerkin = sym.diff(e_Galerkin, x)
>>> dedx_Galerkin.subs(x, L/2)
0
>>> dedx_colloc = sym.diff(e_colloc, x)
>>> dedx_colloc.subs(x, L/2)
0
!ec
Finally, we can compute the maximum error at $x=L/2$ and evaluate
the expressions numerically with three decimals:

!bc pyshell
>>> sym.simplify(e_Galerkin.subs(x, L/2).evalf(n=3))
-0.00812*L**2
>>> sym.simplify(e_colloc.subs(x, L/2).evalf(n=3))
0.0473*L**2
!ec
The error in the collocation method is about 6 times larger than
the error in the Galerkin or least squares method.


===== Integration by parts =====
label{fem:deq:1D:varform}

idx{integration by parts}

A problem arises if we want to apply popular finite element functions
to solve our model problem (ref{fem:deq:1D:model1b})
by the standard least squares, Galerkin, or collocation methods: the piecewise
polynomials $\baspsi_i(x)$ have discontinuous derivatives at the
cell boundaries which makes it problematic to compute
the second-order derivative.  This fact actually makes the least squares and
collocation methods less suitable for finite element approximation of
the unknown function. (By rewriting the equation $-u''=f$ as a
system of two first-order equations, $u'=v$ and $-v'=f$, the
least squares method can be applied. Also, differentiating discontinuous
functions can actually be handled by distribution theory in
mathematics.)  The Galerkin method and the method of
weighted residuals can, however, be applied together with finite
element basis functions if we use *integration by parts*
as a means for transforming a second-order derivative to a first-order
one.

Consider the model problem (ref{fem:deq:1D:model1b}) and its
Galerkin formulation

!bt
\begin{equation*} -(u'',v) = (f,v)\quad\forall v\in V\tp  \end{equation*}
!et
Using integration by parts in the Galerkin method,
we can ``move'' a derivative of $u$ onto $v$:

!bt
\begin{align}
\int_0^L u''(x)v(x) \dx &= - \int_0^Lu'(x)v'(x)\dx
+ [vu']_0^L\nonumber\\
&= - \int_0^Lu'(x)v'(x) \dx
+ u'(L)v(L) - u'(0)v(0)\tp
label{fem:deq:1D:intbyparts}
\end{align}
!et
Usually, one integrates the problem at the stage where the $u$ and $v$
functions enter the formulation.
Alternatively, but less common, we can integrate by parts in the expressions for
the matrix entries:

!bt
\begin{align}
\int_0^L\baspsi_i(x)\baspsi_j''(x) \dx &=
- \int_0^L\baspsi_i'(x)\baspsi_j'(x) dx
+ [\baspsi_i\baspsi_j']_0^L\nonumber\\
&= - \int_0^L\baspsi_i'(x)\baspsi_j'(x) \dx
+ \baspsi_i(L)\baspsi_j'(L) - \baspsi_i(0)\baspsi_j'(0)\tp
label{fem:deq:1D:intbyparts0}
\end{align}
!et
Integration by parts serves to reduce the order of the derivatives and
to make the coefficient matrix symmetric since
$(\baspsi_i',\baspsi_j') = (\baspsi_j',\baspsi_i')$.
The symmetry property depends
on the type of terms that enter the differential equation.
As will be seen later in Section ref{fem:deq:1D:BC:nat},
integration by parts also provides a method for implementing
boundary conditions involving $u'$.

With the choice (ref{fem:deq:1D:ex:sines:psi}) of basis functions we see
that the ``boundary terms''
$\baspsi_i(L)\baspsi_j'(L)$ and $\baspsi_i(0)\baspsi_j'(0)$
vanish since $\baspsi_i(0)=\baspsi_i(L)=0$.
#A boundary term associated with
#a location at the boundary where we have Dirichlet conditions will always
#vanish because $\baspsi_i=0$ at such locations.
We therefore end up with the following alternative Galerkin formulation:

!bt
\[ -(u'',v) = (u', v') = (f,v)\quad \forall v\in V\tp\]
!et

idx{weak form} idx{strong form}

=== Weak form ===

Since the variational formulation after integration by parts make
weaker demands on the differentiability of $u$ and the basis
functions $\baspsi_i$,
the resulting integral formulation is referred to as a *weak form* of
the differential equation problem. The original variational formulation
with second-order derivatives, or the differential equation problem
with second-order derivative, is then the *strong form*, with
stronger requirements on the differentiability of the functions.

For differential equations with second-order derivatives, expressed as
variational formulations and solved by finite element methods, we will
always perform integration by parts to arrive at expressions involving
only first-order derivatives.


===== Boundary function =====
label{fem:deq:1D:essBC:Bfunc}

So far we have assumed zero Dirichlet boundary conditions, typically
$u(0)=u(L)=0$, and we have demanded that $\baspsi_i(0)=\baspsi_i(L)=0$
for $i\in\If$. What about a boundary condition like $u(L)=D\neq0$?
This condition immediately faces a problem:
$u = \sum_j c_j\basphi_j(L) = 0$ since all $\basphi_i(L)=0$.

We remark that we  faced exactly the same problem  in Section ref{fem:approx:global:Fourier} where
we considered Fourier series approximations of functions that where non-zero at the boundaries.
We will use the same trick as we did earlier to get around this problem.

A boundary condition of the form $u(L)=D$ can be implemented by
demanding that all $\baspsi_i(L)=0$, but adding a
*boundary function* $B(x)$ with the right boundary value, $B(L)=D$, to
the expansion for $u$:

!bt
\begin{equation*} u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_j(x)
\tp
\end{equation*}
!et
This $u$ gets the right value at $x=L$:

!bt
\begin{equation*} u(L) = B(L) + \sum_{j\in\If} c_j\baspsi_j(L) = B(L) = D\tp  \end{equation*}
!et
The idea is that for any boundary where $u$ is known we demand $\baspsi_i$ to
vanish and construct a function $B(x)$ to attain the boundary value of $u$.
There are no restrictions on how $B(x)$ varies with $x$ in the interior of the
domain, so this variation needs to be constructed in some way. Exactly how
we decide the variation to be, is not important.

For example, with $u(0)=0$ and
$u(L)=D$, we can choose $B(x)=x D/L$, since this form ensures that
$B(x)$ fulfills the boundary conditions: $B(0)=0$ and $B(L)=D$.
The unknown function is then sought on the form

!bt
\begin{equation}
u(x) = \frac{x}{L}D + \sum_{j\in\If} c_j\baspsi_j(x),
label{fem:deq:1D:essBC:Bfunc:u1}
\end{equation}
!et
with $\baspsi_i(0)=\baspsi_i(L)=0$.

The particular shape of the $B(x)$ function is not important
as long as its boundary
values are correct. For example, $B(x)=D(x/L)^p$ for any power $p$
will work fine in the above example. Another choice could be
$B(x)=D\sin (\pi x/(2L))$.

As a more general example, consider a domain $\Omega = [a,b]$
where the boundary conditions are $u(a)=U_a$ and $u(b)=U_b$.  A class
of possible $B(x)$ functions is

!bt
\begin{equation} B(x)=U_a + \frac{U_b-U_a}{(b-a)^p}(x-a)^p,\quad p>0
\tp
label{fem:deq:1D:essBC:Bfunc:gen}
\end{equation}
!et
Real applications will most likely use the simplest version, $p=1$,
but here such a $p$ parameter was included to demonstrate that there
are many choices of $B(x)$ in a problem. Fortunately, there is a general, unique
technique for constructing $B(x)$ when we use finite element basis functions for
$V$.

[kam: in the below, I cannot really find where it is stated that
we need to adjust the right-hand side as well]

!bsummary How to deal with nonzero Dirichlet conditions
The general procedure of incorporating Dirichlet boundary
conditions goes as follows.
Let $\partial\Omega_E$ be the part(s) of the boundary
$\partial\Omega$ of the domain $\Omega$ where $u$ is specified.
Set $\baspsi_i=0$ at the points in $\partial\Omega_E$ and seek $u$
as

!bt
\begin{equation}
u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_j(x),
label{fem:deq:1D:essBC:Bfunc:u2}
\end{equation}
!et
where $B(x)$ equals the boundary conditions on $u$ at $\partial\Omega_E$.
!esummary

__Remark.__
With the $B(x)$ term, $u$ does not in general lie in $V=\hbox{span}\,
\{\baspsi_0,\ldots,\baspsi_N\}$ anymore. Moreover, when a prescribed value
of $u$ at the boundary, say $u(a)=U_a$ is different from zero, it does
not make sense to say that $u$ lies in a vector space, because
this space does not obey the requirements of addition and scalar multiplication.
For example,
$2u$ does not lie in the space since its boundary value is $2U_a$,
which is incorrect. It only makes sense to split $u$ in two parts,
as done above, and have the unknown part $\sum_j c_j \baspsi_j$ in a
proper function space.
# Sometimes it is said that $u$ is in the *affine space* $B+V$.

===== Abstract notation for variational formulations =====
label{fem:deq:1D:varform:abstract}

We have seen that variational formulations end up with a formula involving
$u$ and $v$, such as $(u',v')$ and a formula involving $v$ and known
functions, such as $(f,v)$. A widely used notation is to introduce an abstract
variational statement written as

!bt
\[ a(u,v)=L(v)\quad\forall v\in V,\]
!et
where $a(u,v)$ is a so-called *bilinear form* involving all the terms
that contain both the test and trial
function, while $L(v)$ is a *linear form* containing all the terms without
the trial function. For example, the statement

!bt
\[ \int_{\Omega} u' v' \dx =
\int_{\Omega} fv\dx\quad\hbox{or}\quad (u',v') = (f,v)
\quad\forall v\in V\]
!et
can be written in abstract form: *find $u$ such that*

!bt
\[ a(u,v) = L(v)\quad \forall v\in V,\]
!et
where we have the definitions

!bt
\[ a(u,v) = (u',v'),\quad L(v) = (f,v)\tp  \]
!et

The term *linear* means that

!bt
\[ L(\alpha_1 v_1 + \alpha_2 v_2) =\alpha_1 L(v_1) + \alpha_2 L(v_2)\]
!et
for two test functions $v_1$ and $v_2$, and
scalar parameters $\alpha_1$ and $\alpha_2$. Similarly, the term *bilinear*
means that $a(u,v)$ is linear in both its arguments:

!bt
\begin{align*}
a(\alpha_1 u_1 + \alpha_2 u_2, v) &= \alpha_1 a(u_1,v) + \alpha_2 a(u_2, v),
\\
a(u, \alpha_1 v_1 + \alpha_2 v_2) &= \alpha_1 a(u,v_1) + \alpha_2 a(u, v_2)
\tp
\end{align*}
!et
In nonlinear problems these linearity properties do not hold in general
and the abstract notation is then

!bt
\[ F(u;v)=0\quad\forall v\in V\tp\]
!et

The matrix system associated with $a(u,v)=L(v)$ can also be written in
an abstract form by inserting $v=\baspsi_i$ and $u=\sum_j c_j\baspsi_j$
in $a(u,v)=L(v)$. Using the linear properties, we get

!bt
\[ \sum_{j\in\If} a(\baspsi_j,\baspsi_i) c_j = L(\baspsi_i),\quad i\in\If,
\]
!et
which is a linear system

!bt
\[ \sum_{j\in\If}A_{i,j}c_j = b_i,\quad i\in\If,\]
!et
where

!bt
\[ A_{i,j} =a(\baspsi_j,\baspsi_i), \quad b_i = L(\baspsi_i)\tp\]
!et
In many problems, $a(u,v)$ is symmetric such that
$a(\baspsi_j,\baspsi_i) = a(\baspsi_i,\baspsi_j)$. In those cases the
coefficient matrix becomes symmetric, $A_{i,j}=A_{j,i}$, a property
that can simplify solution algorithms for linear systems
and make them more stable. The property also reduces memory
requirements and the computational work.


The abstract notation $a(u,v)=L(v)$ for linear differential equation problems
is much used in the literature and
in description of finite element software (in particular the
"FEniCS": "http://fenicsproject.org" documentation). We shall
frequently summarize variational forms using this notation.

===== Variational problems and minimization of functionals =====
label{fem:deq:1D:optimization}

=== Example ===

Many physical problems can be modeled as  partial differential equations
and as  minimization problems. For example, the deflection $u(x)$ of an
elastic string subject to a transversal force $f(x)$
is governed by the differential equation problem

!bt
\[ -u''(x) = f(x),\quad x\in (0,L),\quad x(0)=x(L)=0\tp\]
!et
Equivalently, the deflection $u(x)$ is the function $v$ that minimizes
the potential energy $F(v)$ in a string,

!bt
\[ F(v) = \half\int_0^L \left((v')^2 - fv\right)\dx\tp\]
!et
That is, $F(u) = \min_{v\in V}F(v)$. The quantity $F(v)$ is called a
functional: it takes one or more functions as input and produces a
number. Loosely speaking, we may say that a functional is
``a function of functions''. Functionals very often involve
integral expressions as above.

A range of physical problems can be formulated either as a differential
equation or as a minimization of some functional. Quite often, the differential
equation arises from Newton's 2nd law of motion while the functional
expresses a certain kind of energy.

Many traditional applications of the finite element method, especially
in solid mechanics and constructions with beams and plates, start with
formulating $F(v)$ from physical principles, such as minimization of
elastic energy, and then proceeds with deriving $a(u,v)=L(v)$, which
is the formulation usually desired in software implementations.


=== The general minimization problem ===

The relation between a differential equation and minimization of a functional
can be expressed in a general mathematical way using our abstract
notation for a variational form: $a(u,v)=L(v)$.
It can be shown that the variational statement

!bt
\[ a(u,v)=L(v)\quad\forall v\in V,\]
!et
is equivalent to minimizing the functional

!bt
\[ F(v) = {\half}a(v,v) - L(v) \]
!et
over all functions $v\in V$. That is,

!bt
\[ F(u)\leq F(v)\quad \forall v\in V\tp \]
!et

=== Derivation ===

[kam:
need that F is convex and a is positive, symmetric, right? Check
]

To see this, we write $F(u)\leq F(\eta)$, $\forall\eta\in V$ instead
and set $\eta=u + \epsilon v$, where $v\in V$ is an
arbitrary function in $V$.
For any given arbitrary $v$, we can view $F(v)$ as a function $g(\epsilon)$
and find the extrema of $g$, which is a function of one variable.
We have

!bt
\[ F(\eta) = F(u+\epsilon v) = \half a(u+\epsilon v,u + \epsilon v) - L(u + \epsilon v)\tp\]
!et
From the linearity of $a$ and $L$ we get

!bt
\begin{align*}
g(\epsilon) &= F(u+\epsilon u)\\
&= \half a(u + \epsilon v, u+\epsilon v) - L(u + \epsilon v)\\
&= \half a(u, u+\epsilon v) + \half \epsilon a(v, u+\epsilon u) - L(u) - \epsilon L(v)\\
&= \half a(u, u) + \half \epsilon a(u,v) + \half\epsilon a(v, u) +
\half\epsilon^2 a(v,v) - L(u) - \epsilon L(v)\tp
\end{align*}
!et
If we now assume that $a$ is symmetric, $a(u,v)=a(v,u)$, we can write

!bt
\[ g(\epsilon) = \half a(u,u) + \epsilon a(u,v) + \half\epsilon^2 a(v,v) -
L(u) -\epsilon L(v)\tp\]
!et
The extrema of $g$ is found by searching for $\epsilon$ such that
$g'(\epsilon)=0$:

!bt
\[ g'(\epsilon) = a(u,v) - L(v) + \epsilon a(v,v) = 0\tp\]
!et
This linear equation in $\epsilon$ has a solution $\epsilon = (a(u,v) - L(u))/a(v,v)$ if $a(v,v) > 0$. But recall that $a(u,v)=L(v)$ for any $v$, so
we must have $\epsilon =0$. Since the reasoning above holds for any $v\in V$,
the function $\eta = u+\epsilon v$ that makes $F(\eta)$ extreme must have
$\epsilon=0$, i.e., $\eta = u$, the solution of $a(u,v)=L(v)$ for any $v$ in
$V$.

Looking at $g''(\epsilon) = a(v,v)$, we realize that $\epsilon=0$ corresponds
to a unique minimum if $a(v,v)>0$.

The equivalence of a variational form $a(u,v)=L(v)\ \forall v\in V$ and
the minimization problem $F(u)\leq F(v)\ \forall v\in V$ requires that
1) $a$ is bilinear and $L$ is linear, 2) $a(u,v)$ is symmetric:
$a(u,v)=a(v,u)$, and 3) that $a(v,v)>0$.

=== Minimization of the discretized functional ===

Inserting $v=\sum_j c_j\baspsi_j$ turns minimization of $F(v)$ into
minimization of a quadratic function of the parameters $c_0,\ldots,c_N$:

!bt
\[ \bar F(c_0,\ldots,c_N) = \sum_{j\in\If}\sum_{i\in\If} a(\baspsi_i,\baspsi_j)c_ic_j - \sum_{j\in\If} L(\baspsi_j)c_j
\]
!et
of $N+1$ parameters.

Minimization of $\bar F$ implies

!bt
\[ \frac{\partial\bar F}{\partial c_i}=0,\quad i\in\If\tp\]
!et
After some algebra one finds

!bt
\[ \sum_{j\in\If} a(\baspsi_i,\baspsi_j)c_j = L(\baspsi_i),\quad i\in\If,\]
!et
which is the same system as the one arising from $a(u,v)=L(v)$.

=== Calculus of variations ===

A branch of applied mathematics, called "calculus of variations": "https://en.wikipedia.org/wiki/Calculus_of_variations", deals
with the technique of minimizing functionals to derive differential
equations. The technique involves taking the *variation* (a kind of
derivative) of functionals, which have given name to
terms like variational form, variational
problem, and variational formulation.

!split
======= Examples on variational formulations =======
label{fem:deq:1D:varform:ex}

The following sections derive variational formulations for some
prototype differential equations in 1D, and demonstrate how we with
ease can handle variable coefficients, mixed Dirichlet and Neumann
boundary conditions, first-order derivatives, and nonlinearities.

===== Variable coefficient =====

Consider the problem

!bt
\begin{equation}
-\frac{d}{dx}\left( \dfc(x)\frac{du}{dx}\right) = f(x),\quad x\in\Omega =[0,L],\
u(0)=C,\ u(L)=D\tp
\end{equation}
!et
There are two new features of this problem compared with
previous examples: a variable
coefficient $\dfc (x)$ and nonzero Dirichlet conditions at both boundary points.

Let us first deal with the boundary conditions. We seek

!bt
\[ u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_i(x)\tp\]
!et
Since the Dirichlet conditions demand

!bt
\[ \baspsi_i(0)=\baspsi_i(L)=0,\quad i\in\If,\]
!et
the function $B(x)$
must fulfill $B(0)=C$ and $B(L)=D$. The we are guaranteed that $u(0)=C$
and $u(L)=D$. How $B$ varies in between
$x=0$ and $x=L$ is not of importance. One possible choice is

!bt
\[ B(x) = C + \frac{1}{L}(D-C)x,\]
!et
which follows from (ref{fem:deq:1D:essBC:Bfunc:gen}) with $p=1$.

We seek $(u-B)\in V$. As usual,

!bt
\[ V = \hbox{span}\{\baspsi_0,\ldots,\baspsi_N\}\tp\]
!et
Note that any $v\in V$ has the property $v(0)=v(L)=0$.

The residual arises by inserting our $u$ in the differential equation:

!bt
\[ R = -\frac{d}{dx}\left( \dfc\frac{du}{dx}\right) -f\tp \]
!et
Galerkin's method is

!bt
\[
(R, v) = 0,\quad \forall v\in V,
\]
!et
or written with explicit integrals,

!bt
\[
\int_{\Omega} \left(-\frac{d}{dx}\left( \dfc\frac{du}{dx}\right) -f\right)v \dx = 0,\quad \forall v\in V \tp
\]
!et
We proceed with integration by parts to lower the derivative from
second to first order:

!bt
\[ -\int_{\Omega} \frac{d}{dx}\left( \dfc(x)\frac{du}{dx}\right) v \dx
= \int_{\Omega} \dfc(x)\frac{du}{dx}\frac{dv}{dx}\dx -
\left[\dfc\frac{du}{dx}v\right]_0^L
\tp
\]
!et

The boundary term vanishes since $v(0)=v(L)=0$.
The variational formulation is then

!bt
\[
\int_{\Omega} \dfc(x)\frac{du}{dx}\frac{dv}{dx}\dx = \int_{\Omega} f(x)v\dx,\quad
\forall v\in V\tp
\]
!et
The variational formulation can alternatively be written in a more
compact form:

!bt
\[
(\dfc u',v') = (f,v),\quad \forall v\in V
\tp
\]
!et
The corresponding abstract notation reads

!bt
\[ a(u,v)=L(v)\quad\forall v\in V,\]
!et
with
!bt
\[ a(u,v)= (\dfc u',v'),\quad L(v)=(f,v) \tp  \]
!et

We may insert $u=B + \sum_jc_j\baspsi_j$ and $v=\baspsi_i$ to
derive the linear system:

!bt
\[
(\dfc B' + \dfc \sum_{j\in\If} c_j \baspsi_j', \baspsi_i') =
(f,\baspsi_i), \quad i\in\If \tp
\]
!et
Isolating everything with the $c_j$ coefficients on the left-hand side
and all known terms on the right-hand side
gives

!bt
\[ \sum_{j\in\If} (\dfc\baspsi_j', \baspsi_i')c_j  =
(f,\baspsi_i) + (\alpha (D-C)L^{-1}, \baspsi_i'), \quad i\in\If
\tp
\]
!et
This is nothing but a linear system $\sum_j A_{i,j}c_j=b_i$
with


!bt
\begin{align*}
A_{i,j} &= (\alpha \baspsi_j', \baspsi_i') = \int_{\Omega} \dfc(x)\baspsi_j'(x),
\baspsi_i'(x)\dx,\\
b_i &= (f,\baspsi_i) + (\alpha (D-C)L^{-1},\baspsi_i')=
\int_{\Omega} \left(f(x)\baspsi_i(x) + \dfc(x)\frac{D-C}{L}\baspsi_i'(x)\right) \dx
\tp
\end{align*}
!et

===== First-order derivative in the equation and boundary condition =====

The next problem to formulate in terms of a variational form reads

!bt
\begin{equation}
-u''(x) + bu'(x) = f(x),\quad x\in\Omega =[0,L],\
u(0)=C,\ u'(L)=E\tp
\end{equation}
!et
The new features are a first-order derivative $u'$ in the equation
and the boundary
condition involving the derivative: $u'(L)=E$.
Since we have a Dirichlet condition at $x=0$,
we must force $\baspsi_i(0)=0$ and use a boundary function
to take care of the condition $u(0)=C$.
Because there is no Dirichlet
condition on $x=L$ we do not make any requirements to $\baspsi_i(L)$.
The simplest possible choice of $B(x)$ is $B(x)=C$.

The expansion for $u$ becomes

!bt
\[ u = C + \sum_{j\in\If} c_j \baspsi_i(x)
\tp
\]
!et

The variational formulation arises from multiplying the equation by
a test function $v\in V$ and integrating over $\Omega$:

!bt
\[  (-u'' + bu' - f, v) = 0,\quad\forall v\in V\]
!et
We apply integration by parts to the $u''v$ term only. Although we could
also integrate $u' v$ by parts, this is not common.
The result becomes

!bt
\[ (u',v') + (bu',v) = (f,v) + [u' v]_0^L, \quad\forall v\in V \tp \]
!et
Now, $v(0)=0$ so

!bt
\[ [u' v]_0^L = u'(L)v(L) = E v(L),\]
!et
because $u'(L)=E$.
Thus, integration by parts allows us to take care of the Neumann condition
in the boundary term.


idx{natural boundary condition} idx{essential boundary condition}

!bnotice Natural and essential boundary conditions
A common mistake is to forget a boundary term like $[u'v]_0^L$ in
the integration by parts. Such a mistake implies that we actually
impose the condition $u'=0$ unless there is a Dirichlet condition
(i.e., $v=0$) at that point! This fact has great practical
consequences, because it is easy to forget the boundary term, and that
implicitly set a boundary condition!

Since homogeneous Neumann conditions can be incorporated without
``doing anything'' (i.e., omitting the boundary term), and
non-homogeneous Neumann conditions can just be inserted in the
boundary term, such conditions are known as *natural boundary
conditions*.  Dirichlet conditions require more essential steps in the
mathematical formulation, such as forcing all $\basphi_i=0$ on the
boundary and constructing a $B(x)$, and are therefore known as
*essential boundary conditions*.
!enotice

The final variational form reads

!bt
\[ (u',v') + (bu',v) = (f,v) + E v(L), \quad\forall v\in V \tp \]
!et
In the abstract notation we have

!bt
\[ a(u,v)=L(v)\quad\forall v\in V,\]
!et
with the particular formulas
!bt
\[ a(u,v)=(u',v') + (bu',v),\quad L(v)= (f,v) + E v(L)\tp \]
!et

The associated linear system is derived by inserting $u=B+\sum_jc_j\baspsi_j$
and replacing $v$ by $\baspsi_i$ for $i\in\If$. Some algebra results in

!bt
\[ \sum_{j\in\If} \underbrace{((\baspsi_j',\baspsi_i') + (b\baspsi_j',\baspsi_i))}_{A_{i,j}} c_j = \underbrace{(f,\baspsi_i) + E \baspsi_i(L)}_{b_i}
\tp
\]
!et
Observe that in this problem, the coefficient matrix is not symmetric,
because of the term

!bt
\[
(b\baspsi_j',\baspsi_i)=\int_{\Omega} b\baspsi_j'\baspsi_i \dx
 \neq \int_{\Omega} b \baspsi_i' \baspsi_j \dx = (\baspsi_i',b\baspsi_j)
\tp
\]
!et

#Too early:
#For finite element basis functions, it is worth noticing that the boundary term
#$E\baspsi_i(L)$ is nonzero only in the entry $b_N$ since all
#$\baspsi_i$, $i\neq N$, are zero at $x=L$, provided the degrees of freedom
#are numbered from left to right in 1D so that $\xno{N}=L$.

===== Nonlinear coefficient =====

Finally, we show that the techniques used above to derive variational
forms apply to nonlinear differential equation
problems as well. Here is a model problem with
a nonlinear coefficient $\alpha(u)$ and a nonlinear right-hand side $f(u)$:

!bt
\begin{equation}
-(\dfc(u)u')' = f(u),\quad x\in [0,L],\ u(0)=0,\ u'(L)=E
\tp
\end{equation}
!et
Our space $V$ has basis $\sequencei{\baspsi}$, and because of the
condition $u(0)=0$, we must require $\baspsi_i(0)=0$, $i\in\If$.

Galerkin's method is about inserting the approximate
$u$, multiplying the differential equation by $v\in V$, and integrate,

!bt
\[ -\int_0^L \frac{d}{dx}\left(\dfc(u)\frac{du}{dx}\right)v \dx =
\int_0^L f(u)v \dx\quad\forall v\in V
\tp
\]
!et
The integration by parts does not differ from the case where we have
$\dfc(x)$ instead of $\dfc(u)$:

!bt
\[ \int_0^L \dfc(u)\frac{du}{dx}\frac{dv}{dx}\dx =
\int_0^L f(u)v\dx + [\dfc(u)vu']_0^L\quad\forall v\in V
\tp
\]
!et
The term $\dfc(u(0))v(0)u'(0)=0$ since $v(0)$.
The other term, $\dfc(u(L))v(L)u'(L)$,
is used to impose the other boundary condition $u'(L)=E$, resulting in

!bt
\[ \int_0^L \dfc(u)\frac{du}{dx}\frac{dv}{dx}\dx =
\int_0^L f(u)v\dx + \dfc(u(L))v(L)E\quad\forall v\in V,
\]
!et
or alternatively written more compactly as

!bt
\[ (\dfc(u)u', v') = (f(u),v) + \dfc(u(L))v(L)E\quad\forall v\in V
\tp
\]
!et
Since the problem is nonlinear, we cannot identify a bilinear
form $a(u,v)$ and a linear form $L(v)$.
An abstract formulation is typically *find $u$ such that*

!bt
\[ F(u;v) = 0\quad\forall v\in V,\]
!et
with
!bt
\[ F(u;v) = (a(u)u', v') - (f(u),v) - a(L)v(L)E
\tp
\]
!et

By inserting $u=\sum_j c_j\baspsi_j$ and $v=\baspsi_i$ in $F(u;v)$,
we get a *nonlinear system of
algebraic equations* for the unknowns $c_i$, $i\in\If$. Such systems must
be solved by constructing a sequence of linear systems whose solutions
hopefully converge to the solution of the nonlinear system. Frequently applied
methods are Picard iteration and Newton's method.

======= Implementation of the algorithms =======
label{fem:global:deq:1D:code}

Our hand calculations can benefit greatly by symbolic computing, as shown
earlier, so it is natural to extend our approximation programs based on
`sympy` to the problem domain of variational formulations.

===== Extensions of the code for approximation =====
label{fem:deq:1D:code:global}

The user must prepare a function `integrand_lhs(psi, i, j)` for
returning the integrand of the integral that contributes to matrix
entry $(i,j)$ on the left-hand side.  The `psi` variable is a Python dictionary holding the
basis functions and their derivatives in symbolic form. More
precisely, `psi[q]` is a list of


!bt
\begin{equation*}
\{\frac{d^q\baspsi_0}{dx^q},\ldots,\frac{d^q\baspsi_{N_n-1}}{dx^q}\}
\tp
\end{equation*}
!et
Similarly, `integrand_rhs(psi, i)` returns the integrand
for entry number $i$ in the right-hand side vector.

Since we also have contributions to the right-hand side vector (and
potentially also the matrix) from boundary terms without any integral,
we introduce two additional functions, `boundary_lhs(psi, i, j)` and
`boundary_rhs(psi, i)` for returning terms in the variational
formulation that are not to be integrated over the domain $\Omega$.
Examples, to be shown later, will explain in more detail how these
user-supplied function may look like.

The linear system can be computed and solved symbolically by
the following function:

!bc pycod
import sympy as sym

def solver(integrand_lhs, integrand_rhs, psi, Omega,
           boundary_lhs=None, boundary_rhs=None):
    N = len(psi[0]) - 1
    A = sym.zeros((N+1, N+1))
    b = sym.zeros((N+1, 1))
    x = sym.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            integrand = integrand_lhs(psi, i, j)
            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if boundary_lhs is not None:
                I += boundary_lhs(psi, i, j)
            A[i,j] = A[j,i] = I   # assume symmetry
        integrand = integrand_rhs(psi, i)
        I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
        if boundary_rhs is not None:
            I += boundary_rhs(psi, i)
        b[i,0] = I
    c = A.LUsolve(b)
    u = sum(c[i,0]*psi[0][i] for i in range(len(psi[0])))
    return u, c
!ec

===== Fallback on numerical methods =====

Not surprisingly, symbolic solution of differential
equations, discretized by a Galerkin or least squares method
with global basis functions,
is of limited interest beyond the simplest problems, because
symbolic integration might be very time consuming or impossible, not
only in `sympy` but also in
"WolframAlpha": "http://wolframalpha.com"
(which applies the perhaps most powerful symbolic integration
software available today: Mathematica). Numerical integration
as an option is therefore desirable.

The extended `solver` function below tries to combine symbolic and
numerical integration.  The latter can be enforced by the user, or it
can be invoked after a non-successful symbolic integration (being
detected by an `Integral` object as the result of the integration
in `sympy`).
# see Section ref{fem:approx:global:Lagrange}).
Note that for a
numerical integration, symbolic expressions must be converted to
Python functions (using `lambdify`), and the expressions cannot contain
other symbols than `x`. The real `solver` routine in the
"`varform1D.py`": "${fem_src}/varform1D.py"
file has error checking and meaningful error messages in such cases.
The `solver` code below is a condensed version of the real one, with
the purpose of showing how to automate the Galerkin or least squares
method for solving differential equations in 1D with global basis functions:

[kam: this point has been made many times already]

!bc pycod
def solver(integrand_lhs, integrand_rhs, psi, Omega,
           boundary_lhs=None, boundary_rhs=None, symbolic=True):
    N = len(psi[0]) - 1
    A = sym.zeros((N+1, N+1))
    b = sym.zeros((N+1, 1))
    x = sym.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            integrand = integrand_lhs(psi, i, j)
            if symbolic:
                I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
                if isinstance(I, sym.Integral):
                    symbolic = False  # force num.int. hereafter
            if not symbolic:
                integrand = sym.lambdify([x], integrand)
                I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
            if boundary_lhs is not None:
                I += boundary_lhs(psi, i, j)
            A[i,j] = A[j,i] = I
        integrand = integrand_rhs(psi, i)
        if symbolic:
            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))
            if isinstance(I, sym.Integral):
                symbolic = False
        if not symbolic:
            integrand = sym.lambdify([x], integrand)
            I = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
        if boundary_rhs is not None:
            I += boundary_rhs(psi, i)
        b[i,0] = I
    c = A.LUsolve(b)
    u = sum(c[i,0]*psi[0][i] for i in range(len(psi[0])))
    return u, c
!ec

===== Example: constant right-hand side =====

To demonstrate the code above, we address

!bt
\begin{equation*} -u''(x)=b,\quad x\in\Omega=[0,1],\quad u(0)=1,\ u(1)=0,\end{equation*}
!et
with $b$ as a (symbolic) constant. A possible basis for the space $V$
is $\baspsi_i(x) = x^{i+1}(1-x)$, $i\in\If$. Note that
$\baspsi_i(0)=\baspsi_i(1)=0$ as required by the Dirichlet conditions.
We need a $B(x)$ function to take care of the known boundary
values of $u$. Any function $B(x)=1-x^p$, $p\in\Real$, is a candidate,
and one arbitrary choice from this family
is $B(x)=1-x^3$. The unknown function is then written as

!bt
\begin{equation*}
u(x) = B(x) + \sum_{j\in\If} c_j\baspsi_j(x)\tp
\end{equation*}
!et

Let us use the Galerkin method to derive the variational formulation.
Multiplying the differential
equation by $v$ and integrating by parts yield

!bt
\begin{equation*}
\int_0^1 u'v' \dx = \int_0^1 fv \dx\quad\forall v\in V,
\end{equation*}
!et
and with $u=B + \sum_jc_j\baspsi_j$ we get the linear system


!bt
\begin{equation}
\sum_{j\in\If}\left(\int_0^1\baspsi_i'\baspsi_j' \dx\right)c_j =
\int_0^1(f\baspsi_i-B'\baspsi_i') \dx,
\quad i\in\If\tp
\end{equation}
!et


The application can be coded as follows with `sympy`:

!bc pycod
import sympy as sym
x, b = sym.symbols('x b')
f = b
B = 1 - x**3
dBdx = sym.diff(B, x)

# Compute basis functions and their derivatives
N = 3
psi = {0: [x**(i+1)*(1-x) for i in range(N+1)]}
psi[1] = [sym.diff(psi_i, x) for psi_i in psi[0]]

def integrand_lhs(psi, i, j):
    return psi[1][i]*psi[1][j]

def integrand_rhs(psi, i):
    return f*psi[0][i] - dBdx*psi[1][i]

Omega = [0, 1]

from varform1D import solver
u_bar, c = solver(integrand_lhs, integrand_rhs, psi, Omega,
                  verbose=True, symbolic=True)
u = B + u_bar
print 'solution u:', sym.simplify(sym.expand(u))
!ec

[kam: does not work]
[hpl: We must get it to work. It's a small extension of the
code for approximation and very natural to include when we have so much
material already on `sympy` for implementing algorithms.]

The printout of `u` reads `-b*x**2/2 + b*x/2 - x + 1`.  Note that
expanding `u`, before simplifying, is necessary in the present case to
get a compact, final expression with `sympy`. Doing `expand` before
`simplify` is a common strategy for simplifying expressions in
`sympy`. However, a non-expanded `u` might be preferable in other
cases - this depends on the problem in question.

The exact solution $\uex(x)$ can be derived by some `sympy` code that
closely follows the examples in Section
ref{fem:deq:1D:models:simple}. The idea is to integrate $-u''=b$ twice
and determine the integration constants from the boundary conditions:

!bc pycod
C1, C2 = sym.symbols('C1 C2')    # integration constants
f1 = sym.integrate(f, x) + C1
f2 = sym.integrate(f1, x) + C2
# Find C1 and C2 from the boundary conditions u(0)=0, u(1)=1
s = sym.solve([u_e.subs(x,0) - 1, u_e.subs(x,1) - 0], [C1, C2])
# Form the exact solution
u_e = -f2 + s[C1]*x + s[C2]
print 'analytical solution:', u_e
print 'error:', sym.simplify(sym.expand(u - u_e))
!ec
The last line prints `0`, which is not surprising when
$\uex(x)$ is a parabola and our approximate $u$ contains polynomials up to
degree 4. It suffices to have $N=1$, i.e., polynomials of degree
2, to recover the exact solution.

We can play around with the code and test that with $f=Kx^p$, for
some constants $K$ and $p$,
the solution is a polynomial of degree $p+2$, and $N=p+1$ guarantees
that the approximate solution is exact.

Although the symbolic code is capable of integrating many choices of $f(x)$,
the symbolic expressions for $u$ quickly become lengthy and non-informative,
so numerical integration in the code, and hence numerical answers,
have the greatest application potential.


======= Approximations may fail: The convection-diffusion equation =======
label{ch:convdiff}

In the previous examples we have obtained reasonable approximations
of the continuous solution with several approaches. In this
chapter we will consider a convection-diffusion equation where
many methods will fail. The failure is purely numerical
and is often tied to the resolution. This example is perhaps
the prime example of numerical instabilities in the context of
PDEs.

Consider the equation
!bt
\begin{align}
\label{convdiff:1D}
- \epsilon u_{xx} - u_x &= 0, \quad \in  (0,1), \\
u(0) &= 1, \\
u(1) &= 0 .
\end{align}
!et

The problem describes a convection-diffusion problem where
the convection is modelled by the first order term $-u_x$ and diffusion
is described by the second order term $-\epsilon u_{xx}$. In many applications $\epsilon \ll 1$
and the dominating term is $-u_x$. The sign of $-u_x$ is not important, the same problem
occurs for $u_x$. The sign only determine the direction of the convection.

Even though the term $-u_x$ is dominating,
the second order term $-\epsilon u_{xx}$
makes the problem a second order
problem and two boundary conditions are required, one condition on each side. In the limiting
case where $\epsilon=0$ only one condition is needed.
For this reason, the problem is called a *singular perturbation problem* as the problem changes
fundamentally in the sense that different boundary conditions are required in the limiting
$\epsilon=0$.

The solution of the above problem is
!bt
\begin{equation}
\label{convdiff:1D:analytical}
u(x) = \frac{e^{-x/\epsilon} - 1}{ e^{-1/\epsilon} -1 } .
\end{equation}
!et

FIGURE: [fig/conv-diff-analytical.png, width=400] Analytical solution to the convection-diffusion problem for varying $\epsilon$. label{convdiff:analytical}


The solution is plotted in Figure ref{convdiff:analytical} for different values of $\epsilon$.
Clearly, as $\epsilon$ decrease the exponential function represents a sharper and sharper gradient.
From a physical or engineering point of view, the equation (ref{convdiff:1D})  represents the simplest problem representing the common
phenomenon of boundary layers that is common in all kinds of fluid-flow problems in nature.
Boundary layers have the characteristics of the solution
(ref{convdiff:1D:analytical}), that is; a sharp local exponential gradient.
The parameter $\epsilon$ is related to the inverse of the Reynolds number which frequently
in engineering is significantly larger than $10^3$. In these applications the boundary layer is extremely thin and the gradient extremely sharp.

The case $epsilon=0$ is qualitatively different from the case when $epsilon\rightarrow 0$.
This case corresponds to the equation
!bt
\begin{align*}
\label{convdiff:1D:0}
- u_x &= 0, \quad \in  (0,1), \\
u(0) &= 1, \\
\end{align*}
!et
The solution to this problem is $u=1$. Hence, in the limiting case the solution lack the
sharp boundary layer.



In this chapter we will not embark on the fascinating and complex issue of boundary layer
theory but only consider the numerical issues related to this phenomenon.
Let us as earlier therefore consider an approximate solution on the following form
!bt
\begin{equation}
u(x) = \sum^{N-1}_{i=1} c_i \baspsi_i(x)  + B(x)
\end{equation}
!et


As earlier $\{\baspsi_i(x)\}_{i=1}^{N-1}\}$ are zero at the boundary $x=0$
and $x=1$ and the boundary conditions are accounted for by the function $B(x)$.
Let
!bt
\begin{equation}
B(x) = c_0 (1-x) + c_N x .
\end{equation}
!et

Then we fixate $c_0=0$ and $c_N=1$ which makes $B(x) = x$.
To determine $\{c_i\}_{i=1}^{N-1}\}$  we consider the homogeneous Dirichlet problem
where we solve for $\hat{u} = u - B$. The homogeneous Dirichlet problem reads
!bt
\begin{align*}
\label{convdiff:1D:homo}
- \epsilon \hat{u}_{xx} + \hat{u}_x &= 1, \quad \in  (0,1), \\
\hat{u}(0) &= 0, \\
\hat{u}(1) &= 0 .
\end{align*}
!et

The Galerkin formulation of (ref{convdiff:1D:homo}) is obtained as
!bt
\begin{equation}
\int_0^1
(- \epsilon \hat{u}_{xx} + \hat{u}_x - 1) \baspsi_j(x) \dx .
\end{equation}
!et
Integration by parts leads to
!bt
\begin{equation}
\int_0^1
\epsilon \hat{u}_{x} \baspsi_j(x)_{x} + \hat{u}_x \baspsi_j(x) - 1 \baspsi_j(x) \dx .
\end{equation}
!et
In other words, we need to solve the linear system $\sum_j A_{i,j} c_j = b_i$ where
!bt
\begin{align*}
A_{i,j} &= \int_0^1\epsilon \baspsi_i(x)_{x} \baspsi_j(x)_{x} + \baspsi_i(x)_x \baspsi_j(x) \dx
b_i &=\int_0^1 1 \baspsi_j(x) \dx
\end{align*}
!et
[kam: check that we use the right symbol for basis]
A corresponding code where we also plot the behavior of the solution
with respect to different $\epsilon$ is:
!bc pycod
N = 8
eps_vals =[1.0, 0.1, 0.01, 0.001]
for eps in eps_vals:
  A = sym.zeros((N-1), (N-1))
  b = sym.zeros((N-1))

  for i in range(0, N-1):
    integrand = f*psi[i]
    integrand = sym.lambdify([x], integrand)
    b[i,0] = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])
    for j in range(0, N-1):
      integrand = eps*sym.diff(psi[i], x)* sym.diff(psi[j], x) -  sym.diff(psi[i], x)*psi[j]
      integrand = sym.lambdify([x], integrand)
      A[i,j] = sym.mpmath.quad(integrand, [Omega[0], Omega[1]])

  c = A.LUsolve(b)
  u = sum(c[r,0]*psi[r] for r in range(N-1)) + x

  U = sym.lambdify([x], u)
  xx = numpy.arange(Omega[0], Omega[1], 1/((N+1)*100.0))
  UU = U(xx)
  pylab.plot(xx, UU)
!ec





In the previous chapter we saw that there was a difference between ...

FIGURE: [fig/Lagrange_convdiff_8.png, width=400] Solution obtained with Galerkin approximation using Lagrangian polynomials of order up to 8 for various $\epsilon$. label{convdiff:osc:Lagrange8}

FIGURE: [fig/Lagrange_convdiff_16.png, width=400] Solution obtained with Galerkin approximation using Lagrangian polynomials of order up to 16 for various $\epsilon$. label{convdiff:osc:Lagrange16}

The numerical solutions for different $\epsilon$ is shown in Figure ref{convdiff:osc:Lagrange8}
and ref{convdiff:osc:Lagrange16} for $N=8$ and $N=16$, respectively.
From these figures we can make two observations. The first observation is that
the numerical solution contains non-physical oscillations that grows as
$\epsilon$ decreases. These oscillations are so strong that for $N=8$, the
numerical solutions do not resemble the true solution at all
when $\epsilon$ is less than $1/10$. The true solution
is always in the interval $[0,1]$ while the numerical solution has values
larger than 2 for $\epsilon=1/100$ and larger than 10 for $\epsilon=1/1000$.
The second observation is that the numerical solutions appear to improve
as $N$ increases. While the numerical solution is outside the interval $[0,1]$
for $\epsilon$ less than $1/10$ the magnitude of the oscillations clearly
has decreased.

We will return to this example later and show examples of
techniques that can be used to improve the approximation.
The complete source code can be found in
 "`test_epsilon`":"${fem_src}/conv_diff.py".



!split
======= Exercises =======


# examples on model4 with 1, x, x^2 etc. Show that N=2 recovers
# the exact solution

# heat conduction in the ground with radioactivity, need good  model, not
#  just the old one
# string with load
# hanging cable, see ideas/5620 articles about that (nonlinear)

===== Exercise: Refactor functions into a more general class =====
label{fem:deq:exer:BVP1D:class}
file=uxx_f_sympy_class

Section ref{fem:deq:1D:models:simple} lists three functions for
computing the analytical solution of some simple model problems. There
is quite some repetitive code, suggesting that the functions can
benefit from being refactored into a class hierarchy, where the super
class solves $-(a(x)u'(x))'=f(x)$ and where subclasses define the
equations for the boundary conditions in a model. Make a method for
returning the residual in the differential equation and the boundary
conditions when the solution is inserted in these equations. Create a
test function that verifies that all three residuals vanish for each
of the model problems in Section ref{fem:deq:1D:models:simple}.  Also
make a method that returns the solution either as `sympy` expression
or as a string in LaTeX format.  Add a fourth subclass for the problem
$-(au')'=f$ with a Robin boundary condition:

!bt
\[ u(0)=0,\quad -u'(L) = C(u - D)\tp\]
!et
Demonstrate the use of this subclass for the case $f=0$ and $a=\sqrt{1+x}$.

!bsol
This is an exercise in software engineering.
The model-specific information is related to the boundary
conditions only. We can then let the super class take care of the
differential equation and the solution process, while subclasses
provide a method `get_bc` to
return the symbolic expressions for the boundary equations.

The super class may be coded as
shown below.

@@@CODE exer/u_xx_f_sympy_class.py fromto: import sympy@class Model1
The various subclasses deal with the boundary conditions of the
various model problems:

@@@CODE exer/u_xx_f_sympy_class.py fromto: class Model1@class Model4

A suitable test function gets quite compact:

@@@CODE exer/u_xx_f_sympy_class.py fromto: def test_Tw@def demo_Model4

The fourth model is just about defining the boundary conditions as equations:

@@@CODE exer/u_xx_f_sympy_class.py fromto: class Model4@def test_Tw
A demo function goes like

@@@CODE exer/u_xx_f_sympy_class.py fromto: def demo_Model4@if __name
The printout shows that the solution is

!bt
\[ u(x) = \frac{2CD\sqrt{1+L}(\sqrt{1+x}-1}{2C\sqrt{1+L} + 2C+1}\tp\]
!et

!esol

===== Exercise: Compute the deflection of a cable with sine functions =====
label{fem:deq:exer:tension:cable}
file=cable_sin

## next branch:
## Figur! Nice to have for both symmetric and complete problem
## See also exer/cable_sin_JDokken.py


A hanging cable of length $L$
with significant tension $T$ has a deflection $w(x)$
governed by

!bt
\[
T w''(x) = \ell(x),
\]
!et
where $\ell(x)$ the vertical load per unit length.
The cable is fixed at $x=0$ and $x=L$ so the boundary conditions become
$w(0)=w(L)=0$. The deflection $w$ is positive upwards, and $\ell$ is
positive when it acts downwards.

If we assume a constant load $\ell(x)=\hbox{const}$,
the solution is expected to be symmetric around $x=L/2$. For a function
$w(x)$ that is symmetric around some point $x_0$, it means that
$w(x_0-h) = w(x_0+h)$, and then $w'(x_0)=\lim_{h\rightarrow 0}(w(x_0+h)-
w(x_0-h))/(2h)=0$. We can therefore utilize symmetry to halve the domain.
We then seek $w(x)$ in $[0,L/2]$ with boundary conditions $w(0)=0$ and
$w'(L/2)=0$.

The problem can be scaled by introducing dimensionless  variables,


!bt
\[ \bar x = \frac{x}{L/2},\quad \bar u = \frac{w}{w_c},\]
!et
where $w_c$ is a characteristic size of $w$.
Inserted in the problem for $w$,

!bt
\[ \frac{4Tw_c}{L^{2}}\frac{d^2\bar u}{d\bar x^2} = \ell\ (= \hbox{const})\tp\]
!et
A desire is to have $u$ and its derivatives about unity, so
choosing $w_c$ such that $|d^2\bar u/d\bar x^2|=1$ is an idea.
Then $w_c=\frac{1}{4}\ell L^2/T$, and the problem for the scaled vertical
deflection $u$ becomes

!bt
\[
u'' = 1,\quad x\in (0,1),\quad u(0)=0,\ u'(1)=0\tp
\]
!et
Observe that there are no physical parameters in this scaled problem.
From now on we have for convenience
renamed $x$ to be the scaled quantity $\bar x$.

!bsubex
Find the exact solution for the deflection $u$.

!bsol
Exercise ref{fem:deq:exer:BVP1D:class} or
Section ref{fem:deq:1D:models:simple} features tools for finding
the analytical solution of this differential equation.
The present
model problem is close to model 2
in Section ref{fem:deq:1D:models:simple}. We can modify the `model2`
function:

@@@CODE exer/cable_sin.py fromto: def model@def midpoint_rule
The solution becomes

!bt
\[ u(x) = \frac{1}{2}x(x-2)\tp\]
!et
Plotting $u(x)$ shows that $|u|\in [0,\half]$ which is compatible with
the aim of the scaling, i.e., to have $u$ of size *about* unity (at least
not very small or very large).
!esol
!esubex

!bsubex
A possible function space is spanned by $\baspsi_i=\sin ((2i+1)\pi x/2)$,
$i=0,\ldots,N$. These functions
fulfill the necessary condition $\baspsi_i(0)=0$,
but they also fulfill $\baspsi_i'(1)=0$ such that both boundary
conditions are fulfilled by the expansion $u=\sum_jc_j\basphi_j$.

Use a Galerkin and a least squares method to find the coefficients
$c_j$ in $u(x)=\sum_j c_j\baspsi_j$. Find how fast the coefficients
decrease in magnitude by looking at $c_j/c_{j-1}$.
Find the error in the maximum deflection at $x=1$ when only one
basis function is used ($N=0$).

!bhint
In this case, where the basis functions and their derivatives are
orthogonal, it is easiest to set up the calculations by hand and
use `sympy` to help out with the integrals.
!ehint

!bsol
With $u=\sum_{j=0}^Nc_j\baspsi_j(x)$ the residual becomes

!bt
\[ R = 1 - u'' = 1 +\sum_{j=0}^Nc_j\baspsi_j''(x) =
1 + \sum_{j=0}^Nc_j(2j+1)^2\frac{\pi^2}{4}\sin((2j+1)\frac{\pi x}{2})\tp\]
!et


__Least squares method.__
The minimization of $\int_0^1R^2dx$ leads to the equations

!bt
\[ (R,\frac{\partial R}{\partial c_i})=0,\quad i=0,\ldots,N\tp\]
!et
We find that

!bt
\[ \frac{\partial R}{\partial c_i} =
(2i+1)^2\frac{\pi^2}{4}\sin((2i+1)\frac{\pi x}{2}),\]
!et
so the governing equations become

!bt
\[ (1+\sum_{j=0}^Nc_j(2j+1)^2\frac{\pi^2}{4}\sin((2j+1)\frac{\pi x}{2}),
(2i+1)^2\frac{\pi^2}{4}\sin((2i+1)\frac{\pi x}{2}) = 0\tp\]
!et
By linearity of the inner product (or integral) this expression can
be reordered to

!bt
\begin{align*}
\sum_{j=0}^Nc_j((2j+1)^2\frac{\pi^2}{4}\sin((2j+1)\frac{\pi x}{2}), &
(2i+1)^2\frac{\pi^2}{4}\sin((2i+1)\frac{\pi x}{2}) = \\
& -(1, (2i+1)^2\frac{\pi^2}{4}\sin((2i+1)\frac{\pi x}{2})),
\end{align*}
!et
which is nothing but a linear system

!bt
\[ \sum_{j=0}^N A_{i,j}c_j = b_i,\quad i=0,\ldots,N,\]
!et
with

!bt
\begin{align*}
A_{i,j} &= (2j+1)^4\frac{\pi^4}{16}\int_0^1
\sin((2j+1)\frac{\pi x}{2})\sin((2i+1)\frac{\pi x}{2})dx,\\
b_i &= -(2i+1)^2\frac{\pi^2}{4}\int_0^1 \sin((2i+1)\frac{\pi x}{2})dx
\end{align*}
!et
Orthogonality of the sine functions $\sin (k\pi x/2)$ on $[0,1]$
for integer $k$ implies that
$A_{i,j}=0$ for $i\neq j$, and $A_{i,i}$ can be
computed by `sympy`:

!bc pyshell
>>> from sympy import *
>>> i = symbols('i', integer=True)
>>> x = symbols('x', real=True)
>>> integrate(sin(i*pi*x/2)**2, (x, 0, 1))
1/2
!ec
Therefore,

!bt
\[ A_{i,j} = \left\lbrace\begin{array}{ll}
0,& i\neq j\\
\half (2i+1)^4\frac{\pi^4}{16}, & i = j
\end{array}\right.
\]
!et
The right-hand side can also be computed by `sympy`:

!bc pyshell
>>> integrate(sin((2*i+1)*pi*x/2), (x, 0, 1))
2/(pi*(2*i+1))
!ec
One should always be skeptical to symbolic software and integration of
periodic functions like the sine and cosine since the answers can be
too simplistic (see subexercise d!).
A general test is to perform numerical integration with
lots of sampling points to (partially) verify the symbolic formula.
Here is an application of the midpoint rule:

@@@CODE exer/cable_sin.py fromto: def midpoint@def sine_sum\(
The output shows that the difference between numerical and exact
integration is about $10^{-11}$, which is ``small'' (and gets smaller
by just increasing `M`). This result brings evidence that the
`sympy` answer is correct.
Alternatively, in this simple case, we can easily calculate the anti-derivative.
It goes like

!bt
\[ -\frac{2}{\pi(2i+1)}\cos((2k+1)\frac{\pi x}{2}),\]
!et
and for $x=1$ we get
$\cos\frac{\pi}{2}$, $\cos 3\frac{\pi}{2}$, $\cos 5\frac{\pi}{2}$,
and so on, which all evaluates to zero, and since the cosine is 1 for $x=0$,
the formula found by `sympy` is correct.

We then get

!bt
\[ b_i = -(2i+1)^2\frac{\pi^2}{4}\frac{2}{\pi(2i+1)} = -\half (2i+1)\pi,\]
!et
and consequently,

!bt
\[ c_i = \frac{b_i}{A_{i,i}} = -\frac{\half (2i+1)\pi}{\half (2i+1)^4}\frac{\pi^4}{16} = -\frac{16}{\pi^3(2i+1)^3}\tp\]
!et

__Galerkin's method.__
The Galerkin method applied to this problem
starts with

!bt
\[ (u'',v) = (1,v)\quad \forall v\in V,\]
!et
and the requirement that $v(0)=0$ since $u(0)=0$.
Integration by parts and using $u'(1)=0$ and $v(0)=0$ makes the boundary
term vanish, and the variational form becomes

!bt
\[ (u',v') = -(1,v) \quad \forall v\in V\tp\]
!et
Inserting $u=\sum_{j=0}^Nc_j\baspsi_j(x)$ and $v=\baspsi_i$ leads to

!bt
\[ \sum_{j=0}^N (\baspsi_j', \baspsi_i')c_j = (1,\baspsi_i),\quad i=0,\ldots,N\tp
\]
!et
With $\baspsi_i=\sin((2i+1)\frac{\pi x}{2})$ the matrix entries become

!bt
\[ A_{i,j} =  (2i+1)(2j+1)\frac{\pi^2}{4}\int_0^1 \cos((2i+1)\frac{\pi x}{2})
\cos((2j+1)\frac{\pi x}{2})dx\tp\]
!et
Orthogonality of the cosine functions implies $A_{i,j}=0$ for
$i\neq j$, and $A_{i,i}$ is computed by integrating the square
of the cosine function,

!bc pyshell
>>> integrate(cos((k+1)*pi*x/2)**2, (x, 0, 1))
1/2
!ec
Now,

!bt
\[ A_{i,i} = (2i+1)^2\frac{\pi^2}{4}\half = \frac{1}{8}(2i+1)^2 \pi^2\tp\]
!et
The right-hand side has almost the same integral as in the
least squares case,

!bt
\[ b_i = -\int_0^1 \sin((2i+1)\frac{\pi x}{2})dx = -\frac{2}{\pi (2i+1)}\tp\]
!et
Consequently,

!bt
\[ c_i = \frac{b_i}{A_{i,i}} = -\frac{16}{\pi^3(2i+1)^3},\]
!et
which is the same result as we obtained in the least squares method.


__Decay of coefficients.__
The coefficients decay,

!bt
\[ \frac{c_i}{c_{i+1}} = \left(\frac{2i+3}{2i+1}\right)^3 > 0\tp\]
!et
The decay is most pronounced for the first terms:

!bc pyshell
>>> for i in range(10):
...   print (float(2*i+3)/(2*i+1))**3
...
27.0
4.62962962963
2.744
2.12536443149
1.82578875171
1.65063861758
1.53618570778
1.4557037037
1.39609200081
1.35019682169
!ec

__Error in one-term solution.__
Keeping just one term ($N=0$) means that

!bt
\[ u(x) = -\frac{16}{\pi^3}\sin(\frac{\pi x}{2})\tp\]
!et
The maximum deflection at $x=1$ becomes $-16\pi^{-3}=-0.5160$, to be compared
with the exact value $-\half$. The error is 3.2 percent.
!esol
!esubex

!bsubex
Visualize the solutions in b) for $N=0,1,20$.

!bsol
First we need a function to compute the approximate $u$ in this case:

@@@CODE exer/cable_sin.py fromto: def sine_sum\(@def plot_sine_sum\(
Note the need to append `s.copy()`: doing just `u.append(s)` will
make, e.g., `u[0]` a reference to `s`, which at the end of the
loop is an array corresponding to the maximum $i$ value.

We also need a function that can create an appropriate plot:

@@@CODE exer/cable_sin.py fromto: def plot_sine_sum\(@def check_integral_d

The plot shows that the solution for $N=0$ has a slight deviation from the
exact curve, but even $N=1$ catches up visually with the exact solution (!).

FIGURE: [fig/cable_sin_c, width=500 frac=0.8]
!esol
!esubex

!bsubex
The functions in b) were selected such that they fulfill the
condition $\baspsi'(1)=0$. However, in the Galerkin method, where we
integrate by parts, the condition $u'(1)=0$ is incorporated in the
variational form. This leads to the idea of just choosing a simpler
basis, namely ``all'' sine functions $\baspsi_i = \sin((i+1)\frac{\pi x}{2})$.
Will the method adjust the coefficient such that the additional
functions compared with those in b) get vanishing coefficients? Or
will the additional basis functions improve the solution?
Use Galerkin's method.

!bsol
According to the calculations in b), the Galerkin method, with
$\baspsi_i = \sin((i+1)\frac{\pi x}{2})$, leads to the almost the
same
matrix entries on the diagonal:

!bt
\begin{align*}
A_{i,i} &= (i+1)(j+1)\frac{\pi^2}{4}\int_0^1 \cos((i+1)\frac{\pi x}{2})
\cos((j+1)\frac{\pi x}{2})dx\\
&= (i+1)^2\frac{\pi^2}{4}\half = \frac{1}{8}(i+1)^2 \pi^2\tp
\end{align*}
!et
The right-hand side becomes (as before)

!bt
\[ b_i = -\int_0^1 \sin((i+1)\frac{\pi x}{2})dx = -\frac{2}{\pi (i+1)}\tp\]
!et
We may use `sympy` to integrate,

!bc pyshell
>>> integrate(sin((i+1)*pi*x/2), (x, 0, 1))
2/(pi*(i+1))
!ec
As noted in b), let us be a bit skeptical to this answer and check it.
A quick check with numerical integration,

@@@CODE exer/cable_sin.py fromto: def check_integral_d_sympy_answer\(@def sine_sum_d
gives the output

!bc
0 6.54487575247e-12
1 0.31830988621
2 1.96350713466e-11
3 0.159154943092
4 3.27249061183e-11
5 0.106103295473
6 4.58150045679e-11
7 0.0795774715459
8 5.89047144395e-11
9 0.0636619773677
10 7.19949447281e-11
11 0.0530516476973
!ec
It is clear that for $i$ odd, there are significant differences between
the `sympy` answer and the midpoint rule with high resolution!

We therefore need to do hand calculations to investigate this problem
further.
The anti-derivative is very easy to realize in this case:

!bt
\begin{align*}
\int_0^1\sin ((i+1)\pi x/2)dx &= -\frac{2}{\pi(i+1)}(\cos((i+1)\frac{\pi}{2}) - \cos(0))\\
&= \frac{2}{\pi(i+1)}(1 - \cos((i+1)\frac{\pi}{2}))\tp
\end{align*}
!et
The value of the cosine expression depends on $i$, and the first values are

|-------------------------------|
| $i=0$ | $i=1$ | $i=2$ | $i=3$ |
|---c-------c-------c-------c---|
| 0     | -1    | 0     |  1    |
|-------------------------------|

This pattern repeats and is the same for four consecutive values of $i$.
Hence, the integral is $2/(\pi (i+1))$ for even $i$ ($i=2k$ for
integer $k$, or equivalently: when $i\mbox{ mod } 2 = 0$). For $i=4k+1$, or
equivalently: when $(i-1)\mbox{ mod } 4 = 0$, the
integral is $4/(\pi(4k+1))$, while for $i=4k+3$, the integral vanishes.
This is a more complicated answer than what `sympy` provides!

We can check our new answers against numerical integration:

@@@CODE exer/cable_sin.py fromto: def check_integral_d\(@def check_integral_d_sympy_answer\(
The output now is around $10^{-10}$ and we take that as a sign that
our exact results are reliable.

!bwarning Carefully check symbolic computations!
The example above shows how `sympy` can fail.
"Wolfram Alpha": "http://wolframalpha.com" does a better job: writing
`integrate sin(k*x*pi/2) from 0 to 1` (use `k` instead of `i` since the latter
is the imaginary unit) returns the "result": "http://www.wolframalpha.com/input/?i=integrate+sin%28k*x*pi%2F2%29+from+0+to+1" $4\sin^2(\pi k/4)/(\pi k)$,
which coincides with out result.

There are three general techniques
to verify a symbolic computation:

 * Use alternative software like Wolfram Alpha for comparison
 * Check that the result satisfies the problem to be solved
 * Make a high-resolution numerical approximation and compare

(The second technique is not so applicable here, since we work with
a definite integral, but one could compute the indefinite integral
instead, which is done correctly by `sympy`, and discuss values for
$x=1$.)
!ewarning

The final result for $c_i$ is now

!bt
\[ c_i = \frac{b_i}{A_{i,i}} = \left\lbrace\begin{array}{ll}
-\frac{16}{\pi^3(i+1)^3}, & i\hbox{ even, or } i \hbox{ mod } 2 = 0\\
-\frac{32}{\pi^3(i+1)^3}, & (i-1)\hbox{ mod } 4 = 0,\\
0, & (i+1)\hbox{ mod } 4 = 0
\end{array}\right.
\]
!et
We recognize that for $i$ even, say $i=2k$ for integer $k$, we
have exactly the same result as in b):

!bt
\[ -\sum_k \frac{16}{\pi^3(2k+1)^3}\sin((2k+1)x\frac{\pi x}{2}),\]
!et
but we get an additional set of terms for $i=4k+1$,

!bt
\begin{equation}
-\sum_k \frac{32}{\pi^3(i+1)^3}\sin((4k+1)x\frac{\pi x}{2})\tp
label{fem:deq:exer:tension:cable:badterms}
\end{equation}
!et

We can modify the software from c) to compute the approximate $u$
with the present set of basis functions and coefficients:

@@@CODE exer/cable_sin.py fromto: def sine_sum_d\(@if __name__
The approximations for $N=0,1,3,20$ appear below.

FIGURE: [fig/cable_sin_d, width=500 frac=0.8]

While the approximation for $N=0$ coincides with the one in b), we
see that $N=1$ and higher values of $N$ lead to a clearly wrong curve.
This strange feature has to be investigated!

Let us start by plotting the basis functions for $i=0,1,\ldots,7$:

FIGURE: [fig/sinix_int, width=800 frac=1]

We observe from the figure that all the basis functions corresponding to
even $i$ are symmetric around $x=1$, which is an important property of
the solution. The functions for odd $i$ are anti-symmetric. However,
for $i=3,7,11,\ldots$ the basis function has an integer number of
periods on $[0,1]$ so the integral becomes zero, $c_i=0$, and
consequently
there is no effect from these functions. The functions corresponding
to $i=1,5,9,13,\ldots$ are anti-symmetric around $x=1$ with nonzero
coefficients. The derivative of an anti-symmetric function at the point
of anti-symmetry is unity in size. Since the derivatives of all the
basis functions corresponding to even $i$ vanish at $x=1$, the
extra terms ($i=1,5,9,13,\ldots$) in (ref{fem:deq:exer:tension:cable:badterms})
have a nonzero derivative, resulting in $u'(1)\neq 0$. That is,
these terms destroy the solution!

But we computed $c_i$ by a Galerkin method, which is equivalent to a
least squares method, which gives us the ``best'' approximation possible?
That is true, but it is the best approximation in the chosen space $V$.
The problem is that we have populated (or rather polluted) the space
$V$ with some basis functions that have a wrong mathematical property: they
are anti-symmetric around $x=1$.
!esol
!esubex

!bsubex
Now we drop the symmetry condition at $x=1$ and extend the domain to
$[0,2]$ such that it covers the entire (scaled) physical cable. The
problem now reads

!bt
\[ u'' = 1,\quad x\in (0,2),\quad u(0)=u(2)=0\tp\]
!et
This time we need basis functions that are zero at $x=0$ and $x=2$.
The set $\sin((i+1)\frac{\pi x}{2})$ from d) is a candidate since
they vanish $x=2$ for any $i$. Compute the approximation in this case.
Why is this approximation without the problem that this set of
basis functions introduced in d)?

!bsol
The formulas are almost the same as in d), only the integration domain
is different. Since the sine functions or orthogonal on $[0,1]$, they
are also orthogonal on $[0,2]$. Because

!bc pyshell
>>> integrate(cos((i+1)*pi*x/2)**2, (x, 0, 2))
1
!ec
we get (in Galerkin's method)

!bt
\begin{align*}
A_{i,i} &= (i+1)(j+1)\frac{\pi^2}{4}\int_0^2 \cos((i+1)\frac{\pi x}{2})
\cos((i+1)\frac{\pi x}{2})dx\\
& = (i+1)^2\frac{\pi^2}{4}\tp
\end{align*}
!et
and

!bt
\[ b_i = -\int_0^2 \sin((i+1)\frac{\pi x}{2})dx = \frac{2}{\pi (i+1)}(\cos((i+1)\pi) - 1)\tp\]
!et
We have that $\cos((i+1)\pi = -1$ for $i$ even and
$\cos((i+1)\pi = 1$ for $i$ odd. That is,

!bt
\[ b_i =\left\lbrace\begin{array}{ll}
-\frac{4}{\pi (i+1)}, & i\hbox{ even }\\
0, & i\hbox{ odd }
\end{array}\right.\]
!et
The coefficients become

!bt
\[ c_i =\frac{b_i}{A_{i,i}} =\left\lbrace\begin{array}{ll}
-\frac{16}{\pi^3(i+1)^3}, & i\hbox{ even }\\
0, & i\hbox{ odd }
\end{array}\right.\]
!et
Introducing $i=2k$ and then switching from $k$ to $i$ as summation index
gives $c_i = -\frac{16}{\pi^3(2i+1)^3}$ and

!bt
\[ u(x) = -\sum_{i=0}^N \frac{16}{\pi^3(2i+1)^3}\sin((i+1)\frac{\pi x}{2}),\]
!et
which is the same expansion as in b).

The reason why the basis functions $\baspsi_i=\sin((i+1)\frac{\pi x}{2})$
work well in this case is that the problematic functions for $i=1,5,\ldots$
in d) now live on $[0,2]$ instead of $[0,1]$. On $[0,2]$ these functions
have an integer number of periods such that the integral from 0 to 2
becomes zero. These basis functions are therefore excluded from the
expansion since their coefficients vanish.
The lesson learned is that two equivalent boundary value
problems may make different demands to the basis functions.
!esol
!esubex

# BIG point: use polynomials, without integration by parts we cannot
# handle the boundary condition.

===== Exercise: Compute the deflection of a cable with power functions =====
label{fem:deq:exer:tension:cable_xn}
file=cable_xn

!bsubex
Repeat Exercise ref{fem:deq:exer:tension:cable} b), but work with
the space

!bt
\[ V = \hbox{span}\{x, x^2, x^3, x^4, \ldots\}\tp \]
!et
Choose the dimension of $V$ to be 4 and observe that the exact solution
is recovered by the Galerkin method.

!bhint
Use the `solver` function from `varform1D.py`.
!ehint

!bsol
The Galerkin formulation of $u''=1$, $u(0)=0$, $u'(1)=0$, reads

!bt
\[ (u',v') = -(1,v)\quad\forall v\in V,\]
!et
and the linear system becomes

!bt
\[ \sum_{j=}^N (\baspsi_i', \baspsi_j')c_j = -(1,\baspsi_i),\quad i=0,1,\ldots,N\tp\]
!et
The `varform1D.solver` function needs a function specifying the integrands
on the left- and right-hand sides of the variational formulation.
Moreover, we must compute a dictionary of $\baspsi_i$ and $\baspsi_i'$.
The appropriate code becomes

@@@CODE exer/cable_xn.py fromto: from varform1D@# Least squares
Running this code gives the output

!bc
solution u: x*(x - 2)/2
!ec
which coincides with the exact solution ($c_3=c_4=0$).
!esol
!esubex

!bsubex
What happens if we use a least squares method for this problem with
the basis in a)?

!bsol
The least squares formulation leads to

!bt
\[ (R,\frac{\partial R}{\partial c_i}=0,\quad i=0,\ldots,N,\]
!et
with

!bt
\[ R = 1 - u'' = 1 - \sum_jc_j\baspsi_j''\tp\]
!et
We have

!bt
\[ \frac{\partial R}{\partial c_i} = \baspsi_i'',\]
!et
leading to the equations

!bt
\[ (1 + \sum_jc_j\baspsi_j'', \baspsi_i''),\quad i=0,\ldots,N,\]
!et
which is a linear system

!bt
\[ \sum_{j=0}^N(\baspsi_j'',\baspsi_i'') = (-1,\baspsi_i''),\quad i=0,\ldots,N\tp\]
!et
The fundamental problem with the basis in a) is that $\baspsi_0''=0$, so
if power functions of $x$ are wanted, we need to work with the basis
$V=\hbox{span}\{x^2, x^3,\ldots\}$. If we do so, we can easily modify
the code from a),

@@@CODE exer/cable_xn.py fromto: # Least squares@
The result is $u=-\half x^2$. This function does not obey $u'(1)=0$ and
is completely wrong. In this least squares method we cannot access the basis
function $x$, which is needed in the exact solution, and we have no means
to obtain $u'(1)=0$.

__Remark.__ There is a modification of the least squares method that
can be applied here. The
problem $u''=1$ must be rewritten as a system of two equations,
$u_1'=u_2$, $u_2' =1$. We expand $u_1=\sum_{j=0}^N c_j\baspsi_j$ and
$u_2=\sum_{j=0}^N d_j\baspsi_j$. The residuals in both equations are
added, squared, and differentiated with respect to $c_i$ and $d_i$,
$i=0,\ldots,N$. The result is a coupled equation system for the
$c_i$ and $d_i$ coefficients.
!esol
!esubex

===== Exercise: Check integration by parts =====
label{fem:deq:exer:intg:parts}
file=cable_integr_by_parts

Consider the Galerkin method for the problem involving $u$
in Exercise ref{fem:deq:exer:tension:cable}.
Show that the formulas for $c_j$ are independent of whether we perform
integration by parts or not.

!bsol
The Galerkin method is

!bt
\[ (u'',v)=(1,v)\quad\forall v\in V,\]
!et
and with the choice of $V$ we get

!bt
\begin{align*}
A_{i,j} &=-(i+1)^2\pi^2 \int_0^1\sin^2((i+1)\frac{\pi x}{2})dx,\\
b_i &= \int_0^1\sin((i+1)\frac{\pi x}{2})dx
\end{align*}
!et
From Exercise ref{fem:deq:exer:tension:cable} we realize that
the integrals are the same as in the least squares method, and
those results were identical to those of the Galerkin method with
integration by parts.
!esol



!split
========= Variational formulations with finite elements =========
label{ch:varform:fe}

We shall now take the ideas from previous chapters and put together such that
we can solve PDEs using the flexible finite element basis functions.
This is quite a machinery with many details, but the chapter is mostly an
assembly of concepts and details we have already met.

======= Computing with global polynomials =======

We start out with a computational example using global polynomials and show
that if our solution, modulo boundary conditions, lies in the space spanned
by these polynomials, then the Galerkin method recovers the exact solution.

[hpl: Cannot remember why we need an opening example with *global* polynomials
in a chapter about local polynomials. Could it just be the property of
Galerkin recovering the exact solution? Chaperwise it fits better in the
previous one...]

===== Computing with Dirichlet and Neumann conditions =====
label{fem:deq:1D:varform:ex:DN:case}

# ex_varform1D.py: case2

Let us perform the necessary calculations to solve

!bt
\begin{equation*}
-u''(x)=2,\quad x\in \Omega=[0,1],\quad u'(0)=C,\ u(1)=D,
\end{equation*}
!et
using a global polynomial basis $\baspsi_i\sim x^i$.
The requirements on $\baspsi_i$ is that $\baspsi_i(1)=0$, because $u$ is
specified at $x=1$, so a proper set of polynomial basis functions can be

!bt
\[ \baspsi_i(x)=(1-x)^{i+1}, \quad i\in\If\tp\]
!et
A suitable $B(x)$ function
to handle the boundary condition $u(1)=D$ is $B(x)=Dx$.
The variational formulation becomes

!bt
\[ (u',v') = (2,v) - Cv(0)\quad\forall v\in V\tp \]
!et
From inserting $u=B + \sum_{j}c_j\baspsi_j$ and choosing $v=\baspsi_i$ we get

!bt
\[ \sum_{j\in\If} (\baspsi_j',\baspsi_i')c_j = (2,\baspsi_i)
- (B',\baspsi_i') - C\baspsi_i(0),\quad i\in\If\tp\]
!et
The entries in the linear system are then

!bt
\begin{align*}
A_{i,j} &= (\baspsi_j',\baspsi_i') = \int_{0}^1 \baspsi_i'(x)\baspsi_j'(x)\dx
= \int_0^1 (i+1)(j+1)(1-x)^{i+j}\dx\\
&= \frac{(i+1)(j+1)}{i + j + 1},\\
b_i &= (2,\baspsi_i) - (D,\baspsi_i') -C\baspsi_i(0)\\
&= \int_0^1\left( 2\baspsi_i(x) - D\baspsi_i'(x)\right)\dx -C\baspsi_i(0)\\
&= \int_0^1 \left( 2(1-x)^{i+1} + D(i+1)(1-x)^i\right)\dx  -C\\
&= \frac{(D-C)(i+2) + 2}{i+2} = D - C + \frac{2}{i+2}
\tp
\end{align*}
!et
Relevant `sympy` commands to help calculate these expressions are


@@@CODE src/u_xx_2_CD.py fromto: from sympy@N = 1
The output becomes
!bc dat
A_ij: (i + 1)*(j + 1)/(i + j + 1)
b_i: ((-C + D)*(i + 2) + 2)/(i + 2)
!ec
We can now choose some $N$ and form the linear system, say for $N=1$:

@@@CODE src/u_xx_2_CD.py fromto: N = 1@print 'A:'
The system becomes

!bt
\begin{equation*}
\left(\begin{array}{cc}
1 & 1\\
1 & 4/3
\end{array}\right)
\left(\begin{array}{c}
c_0\\
c_1
\end{array}\right)
=
\left(\begin{array}{c}
1-C+D\\
2/3 -C + D
\end{array}\right)
\end{equation*}
!et
The solution (`c = A.LUsolve(b)`)
becomes $c_0=2 -C+D$ and $c_1=-1$, resulting in

!bt
\begin{equation}
u(x) = 1 -x^2 + D + C(x-1),
\end{equation}
!et
We can form this $u$ in `sympy` and check that the differential equation
and the boundary conditions are satisfied:

!bc pycod
u = sum(c[r,0]*psi_i.subs(i, r) for r in range(N+1)) + D*x
print 'u:', simplify(u)
print "u'':", simplify(diff(u, x, x))
print 'BC x=0:', simplify(diff(u, x).subs(x, 0))
print 'BC x=1:', simplify(u.subs(x, 1))
!ec
The output becomes

!bc dat
u: C*x - C + D - x**2 + 1
u'': -2
BC x=0: C
BC x=1: D
!ec
The complete `sympy` code is found in "`u_xx_2_CD.py`":
"${fem_src}/u_xx_2_CD.py".

The exact solution is found by integrating twice and applying the
boundary conditions, either by hand or using `sympy` as shown in
Section ref{fem:deq:1D:models:simple}.  It appears that the numerical
solution coincides with the exact one.  This result is to be expected
because if $(\uex - B)\in V$, $u = \uex$, as proved next.

===== When the numerical method is exact =====

We have some variational formulation: find $(u-B)\in V$ such that
$a(u,v)=L(u)\ \forall v\in V$. The exact solution also fulfills
$a(\uex,v)=L(v)$, but normally $(\uex -B)$ lies in a much larger
(infinite-dimensional) space. Suppose, nevertheless, that
$\uex - B = E$, where $E\in V$. That is, apart from Dirichlet conditions,
$\uex$ lies in our finite-dimensional space $V$ which we use to compute $u$.
Writing also $u$ on the same form $u=B+F$, $F\in V$, we have

!bt
\begin{align*}
a(B+E,v) &= L(v)\quad\forall v\in V,\\
a(B+F,v) &= L(v)\quad\forall v\in V\tp
\end{align*}
!et
Since these are two variational statements in the same space, we
can subtract them and use the bilinear property of $a(\cdot,\cdot)$:

!bt
\begin{align*}
a(B+E,v) - a(B+F, v) &= L(v) - L(v)\\
a(B+E-(B+F),v) &= 0\\
a(E-F),v) &= 0
\end{align*}
!et
If $a(E-F),v) = 0$ for all $v$ in $V$, then $E-F$ must be zero everywhere
in the domain, i.e., $E=F$. Or in other words: $u=\uex$. This proves
that the exact solution is recovered if $\uex - B$ lies in $V$., i.e.,
can be expressed as $\sum_{j\in\If}d_j\baspsi_j$ where $\{\baspsi_j\}_{j\in\If}$
is a basis for $V$. The method will then compute the solution $c_j=d_j$,
$j\in\If$.

The case treated in Section ref{fem:deq:1D:varform:ex:DN:case}
is of the type where $\uex - B$ is a quadratic function that is 0
at $x=1$, and therefore $(\uex -B)\in V$, and the method
finds the exact solution.

!split
======= Computing with finite elements =======
label{fem:deq:1D:fem1}

The purpose of this section is to demonstrate in detail how
the finite element method can then be applied to the model problem

!bt
\[ -u''(x) = 2,\quad x\in (0,L),\ u(0)=u(L)=0,\]
!et
with variational formulation

!bt
\[ (u',v') = (2,v)\quad\forall v\in V\tp  \]
!et
Any $v\in V$ must obey $v(0)=v(L)=0$ because of the Dirichlet conditions
on $u$.
The variational formulation is derived in
Section ref{fem:deq:1D:varform}.

===== Finite element mesh and basis functions =====

We introduce a finite element mesh with $N_e$ cells, all
with length $h$, and number
the cells from left to right.
Choosing P1 elements, there are two
nodes per cell, and the coordinates of the nodes become

!bt
\begin{equation*}
\xno{i} = i h,\quad h=L/N_e,\quad i=0,\ldots,N_n-1=N_e\tp
\end{equation*}
!et

Any node $i$ is associated with a finite element basis function
$\basphi_i(x)$.  When approximating a given function $f$ by a finite
element function $u$, we expand $u$ using finite element basis
functions associated with *all* nodes in the mesh. The parameter
$N$, which counts the unknowns from 0 to $N$, is then equal to
$N_n-1$ such that the total number of unknowns, $N+1$, is the
total number of nodes.
However, when solving differential equations we will often have
$N<N_n-1$ because of Dirichlet boundary conditions. The reason is
simple: we know what $u$ are at some (here two) nodes, and the number of
unknown parameters is naturally reduced.

In our case with homogeneous Dirichlet boundary conditions, we do not
need any boundary function $B(x)$, so we can work with the expansion

!bt
\begin{equation}
u(x) = \sum_{j\in\If} c_j\baspsi_j(x)\tp
label{fem:deq:1D:fem1:ex:u}
\end{equation}
!et
Because of the boundary conditions, we must demand
$\baspsi_i(0)=\baspsi_i(L)=0$, $i\in\If$. When $\baspsi_i$ for all
$i=0,\ldots,N$ is to be selected among the finite element basis
functions $\basphi_j$, $j=0,\ldots,N_n-1$, we have to avoid using
$\basphi_j$ functions that do not vanish at $\xno{0}=0$ and
$\xno{N_n-1}=L$. However, all $\basphi_j$ vanish at these two nodes for
$j=1,\ldots,N_n-2$.  Only basis functions associated with the end nodes,
$\basphi_0$ and $\basphi_{N_n-1}$, violate the boundary conditions of
our differential equation. Therefore, we select the basis functions
$\basphi_i$ to be the set of finite element basis functions associated
with all the interior nodes in the mesh:

!bt
\[ \baspsi_i=\basphi_{i+1},\quad i=0,\ldots,N\tp\]
!et
The $i$ index runs over all the unknowns $c_i$ in the expansion
for $u$, and in this case $N=N_n-3$.

In the general case, and in particular on domains in higher dimensions,
the nodes are not necessarily numbered from left
to right, so we introduce a mapping from the node numbering, or more
precisely the degree of freedom numbering, to the numbering of
the unknowns in the final equation system. These unknowns take on
the numbers $0,\ldots,N$. Unknown number $j$ in the linear system
corresponds to degree of freedom number $\nu (j)$, $j\in\If$.
We can then write


!bt
\[ \baspsi_i=\basphi_{\nu(i)},\quad i=0,\ldots,N\tp\]
!et
With a regular numbering as in the present example,
$\nu(j) = j+1$, $j=0,\ldots,N=N_n-3$.


===== Computation in the global physical domain =====
label{fem:deq:1D:comp:global}


We shall first perform a computation in the $x$
coordinate system because the integrals can be easily computed
here by simple, visual,
geometric considerations. This is called a global approach
since we work in the $x$ coordinate system and compute integrals on
the global domain $[0,L]$.

The entries in the coefficient matrix and right-hand side are

!bt
\begin{equation*}
A_{i,j}=\int_0^L\baspsi_i'(x)\baspsi_j'(x) \dx,\quad
b_i=\int_0^L2\baspsi_i(x) \dx, \quad i,j\in\If\tp
\end{equation*}
!et
Expressed in terms of finite element basis functions $\basphi_i$ we
get the alternative expressions

!bt
\begin{equation*}
A_{i,j}=\int_0^L\basphi_{i+1}'(x)\basphi_{j+1}'(x) \dx,\quad
b_i=\int_0^L2\basphi_{i+1}(x) \dx,\quad i,j\in\If\tp
\end{equation*}
!et
For the following calculations the subscripts on the finite
element basis functions are more conveniently written as
$i$ and $j$ instead of $i+1$ and $j+1$, so our notation becomes

!bt
\begin{equation*}
A_{i-1,j-1}=\int_0^L\basphi_{i}'(x)\basphi_{j}'(x) \dx,\quad
b_{i-1}=\int_0^L2\basphi_{i}(x) \dx,
\end{equation*}
!et
where the $i$ and $j$ indices run as $i,j=1,\ldots,N+1=N_n-2$.

The $\basphi_i(x)$ function is a hat function with peak at $x=\xno{i}$
and a linear variation in $[\xno{i-1},\xno{i}]$ and
$[\xno{i},\xno{i+1}]$.
The derivative is $1/h$ to the left of $\xno{i}$ and $-1/h$ to
the right, or more formally,

!bt
\begin{equation}
\basphi_i'(x) = \left\lbrace\begin{array}{ll}
0, & x < \xno{i-1},\\
h^{-1},
& \xno{i-1} \leq x < \xno{i},\\
-h^{-1},
& \xno{i} \leq x < \xno{i+1},\\
0, & x\geq \xno{i+1}
\end{array}
\right.
label{fem:approx:fe:Dphi:1:formula2}
\end{equation}
!et
Figure ref{fem:approx:fe:fig:dP1} shows $\basphi_2'(x)$ and $\basphi_3'(x)$.


FIGURE: [fig/fe_mesh1D_dphi_2_3, width=400]  Illustration of the derivative of piecewise linear basis functions associated with nodes in cell 2.  label{fem:approx:fe:fig:dP1}

We realize that $\basphi_i'$ and $\basphi_j'$ has no overlap, and hence their
product vanishes, unless $i$ and $j$ are nodes belonging to the same
cell. The only nonzero contributions to the coefficient matrix are
therefore

!bt
\begin{align*}
A_{i-1,i-2} &=\int_0^L\basphi_i'(x) \basphi_{i-1}'(x) \dx,\\
A_{i-1,i-1}&=\int_0^L\basphi_{i}'(x)^2 \dx, \\
A_{i-1,i}&=\int_0^L\basphi_{i}'(x)\basphi_{i+1}'(x) \dx,
\end{align*}
!et
for $i=1,\ldots,N+1$, but for $i=1$, $A_{i-1,i-2}$ is not defined,
and for $i=N+1$, $A_{i-1,i}$ is not defined.

From Figure ref{fem:approx:fe:fig:dP1},
we see that $\basphi_{i-1}'(x)$ and $\basphi_i'(x)$ have overlap of one
cell $\Omega^{(i-1)}=[\xno{i-1},\xno{i}]$ and that their product
then is $-1/h^{2}$. The integrand is constant and therefore
$A_{i-1,i-2}=-h^{-2}h=-h^{-1}$.
A similar reasoning can be applied to
$A_{i-1,i}$, which also becomes $-h^{-1}$. The integral of
$\basphi_i'(x)^2$ gets contributions from two cells,
$\Omega^{(i-1)}=[\xno{i-1},\xno{i}]$ and
$\Omega^{(i)}=[\xno{i},\xno{i+1}]$, but $\basphi_i'(x)^2=h^{-2}$ in
both cells, and the length of the integration interval is $2h$ so
we get
$A_{i-1,i-1}=2h^{-1}$.

The right-hand side involves an integral of $2\basphi_i(x)$,
$i=1,\ldots,N_n-2$,
which is just the area under a hat function of height 1 and width
$2h$, i.e., equal to $h$. Hence, $b_{i-1}=2h$.

To summarize the linear system, we switch from $i$ to $i+1$ such that
we can write

!bt
\[ A_{i,i-1}=A_{i,i+1}=-h^{-1},\quad A_{i,i}=2h^{-1},\quad
b_i = 2h\tp\]
!et

The equation system to be solved only involves the unknowns
$c_i$ for $i\in\If$. With our numbering of unknowns and
nodes, we have that $c_i$ equals $u(\xno{i+1})$.
The complete matrix system then takes the following form:

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
1 & -1 & 0 &\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & -1 & 1
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
2h \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
2h
\end{array}
\right)
label{fem:deq:1D:ex1:Ab:glob}
\end{equation}
!et


===== Comparison with a finite difference discretization =====
label{fem:deq:1D:fdm_vs_fem}

A typical row in the matrix system (ref{fem:deq:1D:ex1:Ab:glob})
can be written as

!bt
\begin{equation}
-\frac{1}{h}c_{i-1} + \frac{2}{h}c_{i} - \frac{1}{h}c_{i+1} = 2h\tp
label{fem:deq:1D:fem:ex1:c}
\end{equation}
!et
Let us introduce the notation $u_j$ for the value of $u$ at node $j$:
$u_j=u(\xno{j})$, since we have the interpretation
$u(\xno{j})=\sum_jc_j\basphi(\xno{j})=\sum_j c_j\delta_{ij}=c_j$.
The unknowns $c_0,\ldots,c_N$ are $u_1,\ldots,u_{N_n-2}$.
Shifting $i$ with $i+1$ in (ref{fem:deq:1D:fem:ex1:c}) and inserting
$u_i = c_{i-1}$, we get

!bt
\begin{equation}
-\frac{1}{h}u_{i-1} + \frac{2}{h}u_{i} - \frac{1}{h}u_{i+1} = 2h,
label{fem:deq:1D:fem:ex1}
\end{equation}
!et

A finite difference discretization of $-u''(x)=2$ by a centered,
second-order finite difference approximation $u''(x_i)\approx [D_x D_x u]_i$
with $\Delta x = h$
yields

!bt
\begin{equation}
-\frac{u_{i-1} - 2u_{i} + u_{i+1}}{h^2} = 2,
\end{equation}
!et
which is, in fact, equal to (ref{fem:deq:1D:fem:ex1}) if
(ref{fem:deq:1D:fem:ex1}) is divided by $h$.
Therefore, the finite difference and the finite element method are
equivalent in this simple test problem.

Sometimes a finite element method generates the finite difference
equations on a uniform mesh, and sometimes the finite element method
generates equations that are different.  The differences are modest,
but may influence the numerical quality of the solution significantly,
especially in time-dependent problems.
It depends on the problem at hand
whether a finite element discretization is more or less accurate than
a corresponding finite difference discretization.
#There will be many examples illustrating this point.

===== Cellwise computations =====
label{fem:deq:1D:comp:elmwise}

Software for finite element computations normally employs
the cell by cell computational procedure where
an element matrix and vector are calculated for each cell and
assembled in the global linear system.
Let us go through the details of this type of algorithm.

All integrals are mapped to the local reference coordinate system
$X\in [-1,1]$.
In the present case, the matrix entries contain derivatives
with respect to $x$,

!bt
\begin{equation*}
A_{i-1,j-1}^{(e)}=\int_{\Omega^{(e)}} \basphi_i'(x)\basphi_j'(x) \dx
= \int_{-1}^1 \frac{d}{dx}\refphi_r(X)\frac{d}{dx}\refphi_s(X)
\frac{h}{2} \dX,
\end{equation*}
!et
where the global degree of freedom $i$ is related to the local
degree of freedom $r$ through $i=q(e,r)$. Similarly,
$j=q(e,s)$. The local degrees of freedom run as $r,s=0,1$ for a P1
element.

=== The integral for the element matrix ===

There are simple formulas for the basis functions $\refphi_r(X)$ as
functions of $X$.
However, we now
need to find the derivative of $\refphi_r(X)$ with respect to $x$.
Given

!bt
\[ \refphi_0(X)=\half(1-X),\quad\refphi_1(X)=\half(1+X), \]
!et
we can easily compute $d\refphi_r/ dX$:

!bt
\[
\frac{d\refphi_0}{dX} = -\half,\quad  \frac{d\refphi_1}{dX} = \half\tp
\]
!et
From the chain rule,

!bt
\begin{equation}
\frac{d\refphi_r}{dx} = \frac{d\refphi_r}{dX}\frac{dX}{dx}
= \frac{2}{h}\frac{d\refphi_r}{dX}\tp  \end{equation}
!et
The transformed integral is then

!bt
\begin{equation*}
A_{i-1,j-1}^{(e)}=\int_{\Omega^{(e)}} \basphi_i'(x)\basphi_j'(x) \dx
= \int_{-1}^1 \frac{2}{h}\frac{d\refphi_r}{dX}\frac{2}{h}\frac{d\refphi_s}{dX}
\frac{h}{2} \dX
\tp
\end{equation*}
!et

=== The integral for the element vector ===

The right-hand side is transformed according to

!bt
\begin{equation*}
b_{i-1}^{(e)} = \int_{\Omega^{(e)}} 2\basphi_i(x) \dx =
\int_{-1}^12\refphi_r(X)\frac{h}{2} \dX,\quad i=q(e,r),\ r=0,1
\tp
\end{equation*}
!et

=== Detailed calculations of the element matrix and vector ===

Specifically for P1 elements we arrive at the following calculations for
the element matrix entries:

!bt
\begin{align*}
\tilde A_{0,0}^{(e)} &= \int_{-1}^1\frac{2}{h}\left(-\half\right)
\frac{2}{h}\left(-\half\right)\frac{h}{2} \dX = \frac{1}{h}\\
\tilde A_{0,1}^{(e)} &= \int_{-1}^1\frac{2}{h}\left(-\half\right)
\frac{2}{h}\left(\half\right)\frac{h}{2} \dX = -\frac{1}{h}\\
\tilde A_{1,0}^{(e)} &= \int_{-1}^1\frac{2}{h}\left(\half\right)
\frac{2}{h}\left(-\half\right)\frac{h}{2} \dX = -\frac{1}{h}\\
\tilde A_{1,1}^{(e)} &= \int_{-1}^1\frac{2}{h}\left(\half\right)
\frac{2}{h}\left(\half\right)\frac{h}{2} \dX = \frac{1}{h}
\end{align*}
!et
The element vector entries become
!bt
\begin{align*}
\tilde b_0^{(e)} &= \int_{-1}^12\half(1-X)\frac{h}{2} \dX = h\\
\tilde b_1^{(e)} &= \int_{-1}^12\half(1+X)\frac{h}{2} \dX = h\tp
\end{align*}
!et
Expressing these entries in matrix and vector notation, we have

!bt
\begin{equation}
\tilde A^{(e)} =\frac{1}{h}\left(\begin{array}{rr}
1 & -1\\
-1 & 1
\end{array}\right),\quad
\tilde b^{(e)} = h\left(\begin{array}{c}
1\\
1
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm}
\end{equation}
!et

=== Contributions from the first and last cell ===

The first and last cell involve only one unknown and one basis function
because of the Dirichlet boundary conditions at the first and last
node.
The element matrix therefore becomes a $1\times 1$ matrix and there
is only one entry in the element vector. On cell 0, only $\baspsi_0=\basphi_1$
is involved, corresponding to integration with $\refphi_1$. On cell $N_e$,
only $\baspsi_N=\basphi_{N_n-2}$ is involved, corresponding to
integration with $\refphi_0$.
We then get the special end-cell contributions

!bt
\begin{equation}
\tilde A^{(e)} =\frac{1}{h}\left(\begin{array}{r}
1
\end{array}\right),\quad
\tilde b^{(e)} = h\left(\begin{array}{c}
1
\end{array}\right),
label{fem:deq:1D:ex1:Ab:elm:ends}
\end{equation}
!et
for $e=0$ and $e=N_e$. In these cells, we have only one degree of
freedom, not two as in the interior cells.

=== Assembly ===

The next step is to assemble the contributions from the various cells.
The assembly of an element matrix and vector into the global matrix
and right-hand side can be expressed as

!bt
\[
A_{q(e,r),q(e,s)} = A_{q(e,r),q(e,s)} + \tilde A^{(e)}_{r,s},\quad
b_{q(e,r)} = b_{q(e,r)} + \tilde b^{(e)}_{r},\quad
\]
!et
for $r$ and $s$ running over all local degrees of freedom in cell $e$.

To make the assembly algorithm more precise, it is convenient to set up
Python data structures and a code snippet for carrying out all details
of the algorithm.
For a mesh of four equal-sized P1 elements and $L=2$ we have

!bc pycod
vertices = [0, 0.5, 1, 1.5, 2]
cells = [[0, 1], [1, 2], [2, 3], [3, 4]]
dof_map = [[0], [0, 1], [1, 2], [2]]
!ec
The total number of degrees of freedom is 3, being the function
values at the internal 3 nodes where $u$ is unknown.
In cell 0 we have global degree of freedom 0, the next
cell has $u$ unknown at its two nodes, which become
global degrees of freedom 0 and 1, and so forth according to
the `dof_map` list. The mathematical $q(e,r)$ quantity is nothing
but the `dof_map` list.

Assume all element matrices are stored in a list `Ae` such that
`Ae[e][i,j]` is $\tilde A_{i,j}^{(e)}$. A corresponding list
for the element vectors is named `be`, where `be[e][r]` is
$\tilde b_r^{(e)}$.
A Python code snippet
illustrates all details of the assembly algorithm:

!bc pycod
# A[i,j]: coefficient matrix, b[i]: right-hand side
for e in range(len(Ae)):
    for r in range(Ae[e].shape[0]):
        for s in range(Ae[e].shape[1]):
            A[dof_map[e,r],dof_map[e,s]] += Ae[e][i,j]
        b[dof_map[e,r]] += be[e][i,j]
!ec

The general case with `N_e` P1 elements of length `h` has

!bc pycod
N_n = N_e + 1
vertices = [i*h for i in range(N_n)]
cells = [[e, e+1] for e in range(N_e)]
dof_map = [[0]] + [[e-1, e] for i in range(1, N_e)] + [[N_n-2]]
!ec

Carrying out the assembly results in a linear system that is identical
to (ref{fem:deq:1D:ex1:Ab:glob}), which is not surprising, since
the procedures is mathematically equivalent to the calculations
in the physical domain.

So far, our technique for computing the matrix system have assumed
that $u(0)=u(L)=0$. The next section deals with the extension to
nonzero Dirichlet conditions.

!split
======= Boundary conditions: specified nonzero value =======
label{fem:deq:1D:essBC}

We have to take special actions to incorporate nonzero
Dirichlet conditions,
such as $u(L)=D$, into the computational procedures. The present
section outlines alternative, yet mathematically equivalent, methods.


===== General construction of a boundary function =====
label{fem:deq:1D:fem:essBC:Bfunc}

In Section ref{fem:deq:1D:essBC:Bfunc} we introduced a boundary function $B(x)$
to deal with nonzero Dirichlet boundary conditions for $u$. The
construction of such a function is not always trivial, especially not
in multiple dimensions. However, a simple and general constructive idea
exists when the
basis functions have the property

!bt
\begin{equation*}
\basphi_i(\xno{j}) = \delta_{ij},\quad
\delta_{ij} = \left\lbrace\begin{array}{ll}
1, & i=j,\\
0, & i\neq j,
\end{array}\right.
\end{equation*}
!et
where $\xno{j}$ is a boundary point. Examples on such
functions are the Lagrange interpolating polynomials and finite
element functions.

Suppose now that $u$ has Dirichlet boundary conditions at nodes
with numbers $i\in\Ifb$. For example, $\Ifb = \{0,N_n-1\}$ in a 1D
mesh with node numbering from left to right and Dirichlet conditions
at the end nodes $i=0$ and $i=N_n-1$.
Let $U_i$ be the corresponding prescribed values of $u(\xno{i})$.
We can then, in general, use

!bt
\begin{equation}
B(x) = \sum_{j\in\Ifb} U_j\basphi_j(x)\tp
\end{equation}
!et
It is easy to verify that
$B(\xno{i})= \sum_{j\in\Ifb} U_j\basphi_j(\xno{i})=U_i$.


The unknown function can then be written as

!bt
\begin{equation}
u(x) = \sum_{j\in\Ifb} U_j\basphi_j(x) + \sum_{j\in\If}c_j\basphi_{\nu(j)},
\end{equation}
!et
where $\nu(j)$ maps unknown number $j$ in the equation system to
node $\nu(j)$, $\Ifb$ is the set of indices corresponding to basis functions
associated with nodes where Dirichlet conditions apply, and $\If$ is the
set of indices used to number the unknowns from zero to $N$.
We can easily show that with this $u$, a Dirichlet
condition $u(\xno{k})=U_k$ is fulfilled:

!bt
\[
u(\xno{k}) = \sum_{j\in\Ifb} U_j\underbrace{\basphi_j(x_k)}_{\neq 0\,
\Leftrightarrow\,j=k} +
\sum_{j\in\If} c_j\underbrace{\basphi_{\nu(j)}(\xno{k})}_{=0,\ k\not\in\If}
= U_k \]
!et


Some examples will further clarify the notation. With a regular
left-to-right numbering of nodes in a mesh with P1 elements,
and Dirichlet conditions at $x=0$, we use finite element basis
functions associated with the nodes $1, 2, \ldots, N_n-1$, implying
that  $\nu(j)=j+1$, $j=0,\ldots,N$, where $N=N_n-2$. Consider
a particular mesh:

FIGURE: [fig/fe_mesh1D_P1, width=500 frac=0.8]

The expansion associated with this mesh becomes

!bt
\[ u(x) = U_0\basphi_0(x) + c_0\basphi_1(x) +
c_1\basphi_2(x) + \cdots + c_4\basphi_5(x)\tp
\]
!et

Switching to the more standard case of left-to-right numbering and
boundary conditions $u(0)=C$, $u(L)=D$, we have $N=N_n-3$ and

!bt
\begin{align*}
u(x) &= C\basphi_0 + D\basphi_{N_n-1} + \sum_{j\in\If} c_j\basphi_{j+1}\\
&= C\basphi_0 + D\basphi_{N_n} + c_0\basphi_1 + c_1\basphi_2 +\cdots
+ c_N\basphi_{N_n-2}\tp
\end{align*}
!et


Finite element meshes in non-trivial 2D and 3D geometries usually leads
to an irregular cell and node numbering. Let us therefore take a look
at an irregular numbering in 1D:

FIGURE: [fig/fe_mesh1D_random_numbering, width=500 frac=0.8]

Say we in this mesh have Dirichlet conditions on the left-most and
right-most node, with numbers 3 and 1, respectively.  We can number
the unknowns at the interior nodes as we want, e.g., from left to
right, resulting in $\nu(0)=0$, $\nu(1)=4$, $\nu(2)=5$, $\nu(3)=2$.
This gives

!bt
\[ B(x) = U_3\basphi_3(x) + U_1\basphi_1(x),\]
!et
and

!bt
\[ u(x) = B(x) + \sum_{j=0}^3 c_j\basphi_{\nu(j)}
= U_3\basphi_3 + U_1\basphi_1 + c_0\basphi_0 + c_1\basphi_4
+ c_2\basphi_5 + c_3\basphi_2\tp\]
!et

The idea of constructing $B$, described here, generalizes almost
trivially to 2D and 3D problems: $B=\sum_{j\in\Ifb}U_j\basphi_j$,
where $\Ifb$ is the index set containing the numbers of all the
nodes on the boundaries where Dirichlet values are prescribed.

===== Example on computing with a finite element-based boundary function =====

Let us see how the model problem $-u''=2$, $u(0)=C$, $u(L)=D$,
is affected by a $B(x)$ to incorporate boundary values.
Inserting the expression

!bt
\[ u(x) = B(x) + \sum_{j\in\If}c_j\baspsi_j(x)\]
!et
in $-(u'',\baspsi_i)=(f,\baspsi_i)$ and
integrating by parts results in a linear system with

!bt
\[
A_{i,j} = \int_0^L \baspsi_i'(x)\baspsi_j'(x) \dx,\quad
b_i = \int_0^L (f(x)\baspsi_i(x) - B'(x)\baspsi_i'(x)) \dx\tp
\]
!et
We choose $\baspsi_i=\basphi_{i+1}$, $i=0,\ldots,N=N_n-3$
if the node numbering is from left
to right. (Later we also need the assumption that cells too
are numbered from left to right.)
The boundary function becomes

!bt
\[ B(x) = C\basphi_0(x) + D\basphi_{N_n-1}(x)\tp\]
!et
The expansion for $u(x)$ is

!bt
\[ u(x)  = B(x) + \sum_{j\in\If} c_j\basphi_{j+1}(x)\tp \]
!et
We can write the matrix and right-hand side entries as

!bt
\begin{align*}
A_{i-1,j-1} &= \int_0^L \basphi_i'(x)\basphi_j'(x) \dx,\\
b_{i-1} &=
\int_0^L (f(x)\basphi_i'(x) - (C\basphi_{0}'(x) + D\basphi_{N_n-1}'(x))\basphi_i'(x) )\dx,
\end{align*}
!et
for $i,j = 1,\ldots,N+1=N_n-2$. Note that we have here used
$B'=C\basphi_0' + D\basphi_{N_n-1}'$.

=== Computations in physical coordinates ===

Most of the terms in the linear system have already been computed
so we concentrate on the new contribution from the boundary function.
The integral $C\int_0^L \basphi_{0}'(x))\basphi_i'(x) \dx$, associated
with the Dirichlet condition in $x=0$,  can only get
a nonzero contribution from the first cell,
$\Omega^{(0)}=[\xno{0},\xno{1}]$
since $\basphi_{0}'(x)=0$ on all other cells. Moreover,
$\basphi_{0}'(x)\basphi_i'(x) \dx \neq 0$ only for $i=0$ and $i=1$
(but node $i=0$ is excluded from the formulation),
since $\basphi_{i}=0$ on the first cell if $i>1$.
With a similar reasoning we realize that
$D\int_0^L \basphi_{N_n-1}'(x))\basphi_i'(x) \dx$ can only get
a nonzero contribution from the last cell.
From the explanations of the
calculations in ref[Section ref{fem:approx:global:linearsystem}][ in
cite{Langtangen_deqbook_approx}][the document "Approximation of functions": "http://tinyurl.com/k3sdbuv/pub/approx" cite{Langtangen_deqbook_approx}] we then find that

!bt
\begin{align*}
\int_0^L \basphi_{0}'(x)\basphi_{1}'(x) \dx &=
(-\frac{1}{h})\cdot\frac{1}{h}\cdot h = -\frac{1}{h},\\
\int_0^L \basphi_{N_n-1}'(x)\basphi_{N_n-2}'(x) \dx &=
\frac{1}{h}\cdot(-\frac{1}{h})\cdot h = -\frac{1}{h}\tp
\end{align*}
!et
With these expressions we get

!bt
\[ b_0 = \int_0^Lf(x)\basphi_1\dx - C(-\frac{1}{h}),\quad
b_N = \int_0^L f(x)\basphi_{N_n-2}\dx - D(-\frac{1}{h})\tp\]
!et

=== Cellwise computations on the reference element ===

As an equivalent alternative, we now turn to cellwise computations.
The element matrices and vectors are calculated as in Section
ref{fem:deq:1D:comp:elmwise}, so we concentrate on the impact of
the new term involving $B(x)$. This new term,
$B'=C\basphi_0' + D\basphi_{N_n-1}'$, vanishes on all cells except
for $e=0$ and $e=N_e$. Over the first cell ($e=0$) the $B'(x)$ function
in local coordinates reads

!bt
\[ \frac{dB}{dx} = C\frac{2}{h}\frac{d\refphi_0}{dX},\]
!et
while over the last cell ($e=N_e$) it looks like

!bt
\[ \frac{dB}{dx} = D\frac{2}{h}\frac{d\refphi_1}{dX}\tp\]
!et
For an arbitrary interior cell, we have the formula

!bt
\[ \tilde b_r^{(e)} = \int_{-1}^1 f(x(X))\refphi_r(X)\frac{h}{2}\dX,\]
!et
for an entry in the local element vector.
In the first cell, the value at local node 0 is known so only the value
at local node 1 is unknown. The associated element vector entry becomes

!bt
\[
\tilde b_0^{(1)} = \int_{-1}^1 \left(f\refphi_1 -
C\frac{2}{h}\frac{d\refphi_0}{dX}\frac{2}{h}\frac{d\refphi_1}{dX}\right)
\frac{h}{2} \dX = \frac{h}{2} 2\int_{-1}^1 \refphi_1  \dX
- C\frac{2}{h}(-\frac{1}{2})\frac{2}{h}\frac{1}{2}\frac{h}{2}\cdot 2
= h + C\frac{1}{h}\tp
\]
!et
The value at local node 1 in the last cell is known so the
element vector here is

!bt
\[
\tilde b_0^{N_e} = \int_{-1}^1 \left(f\refphi_0 -
D\frac{2}{h}\frac{d\refphi_1}{dX}\frac{2}{h}\frac{d\refphi_0}{dX}\right)
\frac{h}{2} \dX = \frac{h}{2} 2\int_{-1}^1 \refphi_0  \dX
- D\frac{2}{h}\frac{1}{2}\frac{2}{h}(-\frac{1}{2})\frac{h}{2}\cdot 2
= h + D\frac{1}{h}\tp
\]
!et
The contributions from the $B(x)$ function to the global right-hand side
vector becomes $C/h$ for $b_0$ and $D/h$ for $b_N$, exactly as we
computed in the physical domain.

===== Modification of the linear system =====
label{fem:deq:1D:fem:essBC:Bfunc:modsys}

From an implementational point of view, there is a convenient alternative
to adding the $B(x)$ function and using only the basis functions associated
with nodes where $u$ is truly unknown.
Instead of seeking

!bt
\begin{equation}
u(x) = \sum_{j\in\Ifb} U_j\basphi_j(x)
+ \sum_{j\in\If}c_j\basphi_{\nu(j)}(x),
label{fem:deq:1D:fem:essBC:Bfunc:modsys:utrad}
\end{equation}
!et
we use the sum over all degrees of freedom, including the known boundary
values:

!bt
\begin{equation}
u(x) = \sum_{j\in\If}c_j\basphi_j(x)\tp
label{fem:deq:1D:fem:essBC:Bfunc:modsys:uall}
\end{equation}
!et
Note that the collections of unknowns
$\sequencei{c}$ in (ref{fem:deq:1D:fem:essBC:Bfunc:modsys:utrad})
and (ref{fem:deq:1D:fem:essBC:Bfunc:modsys:uall}) are different.
The index set $\If=\{0,\ldots,N\}$ always goes to $N$, and the
number of unknowns is $N+1$, but
in (ref{fem:deq:1D:fem:essBC:Bfunc:modsys:utrad}) the unknowns
correspond to nodes where $u$ is not known, while
in (ref{fem:deq:1D:fem:essBC:Bfunc:modsys:uall}) the unknowns
cover $u$ values at all the nodes. So, if the index set
$\Ifb$ contains $N_b$ node numbers where $u$ is prescribed,
we have that $N=N_n-N_b$ in
(ref{fem:deq:1D:fem:essBC:Bfunc:modsys:utrad}) and
$N=N_n$ in (ref{fem:deq:1D:fem:essBC:Bfunc:modsys:uall}).

The idea is to compute the entries in the linear system as if no
Dirichlet values are prescribed. Afterwards, we modify the linear system
to ensure that the known $c_j$ values are incorporated.

A potential problem arises for the boundary term $[u'v]_0^L$ from the
integration by parts: imagining no Dirichlet conditions means that we
no longer require $v=0$ at Dirichlet points, and the boundary term is
then nonzero at these points. However, when we modify the linear
system, we will erase whatever the contribution from $[u'v]_0^L$
should be at the Dirichlet points in the right-hand side of the linear
system. We can therefore safely forget $[u'v]_0^L$ at any point where
a Dirichlet condition applies.



=== Computations in the physical system ===

Let us redo the computations in the example in
Section ref{fem:deq:1D:fem:essBC:Bfunc}. We solve $-u''=2$ with
$u(0)=0$ and $u(L)=D$. The expressions for $A_{i,j}$ and $b_i$
are the same, but the numbering is different as the numbering of
unknowns and nodes now coincide:

!bt
\[
A_{i,j} = \int_0^L \basphi_i'(x)\basphi_j'(x) \dx,\quad
b_{i} = \int_0^L f(x)\basphi_i(x) \dx,
\]
!et
for $i,j = 0,\ldots,N=N_n-1$.
The integrals involving basis functions
corresponding to interior mesh nodes, $i,j=1,\ldots,N_n-2$, are
obviously the same as before. We concentrate on the contributions
from $\basphi_0$ and $\basphi_{N_n-1}$:

!bt
\begin{align*}
A_{0,0} &= \int_0^L (\basphi_0')^2\dx = \int_{0}^{\xno{1}}
= (\basphi_0')^2\dx \frac{1}{h},\\
A_{0,1} &= \int_0^L \basphi_0'\basphi_1'\dx
= \int_{0}^{\xno{1}} \basphi_0'\basphi_1'\dx = -\frac{1}{h},\\
A_{N,N} &= \int_0^L (\basphi_N')^2\dx
= \int_{\xno{N_n-2}}^{\xno{N_n-1}} (\basphi_N')^2\dx = \frac{1}{h},\\
A_{N,N-1} &= \int_0^L \basphi_N'\basphi_{N-1}'\dx
=\int_{\xno{N_n-2}}^{\xno{N_n-1}} \basphi_N'\basphi_{N-1}'\dx = -\frac{1}{h}\tp
\end{align*}
!et

The new terms on the right-hand side are also those involving
$\basphi_0$ and $\basphi_{N_n-1}$:

!bt
\begin{align*}
b_0 &= \int_0^L 2\basphi_0(x) \dx = \int_0^{\xno{1}} 2\basphi_0(x)\dx = h,\\
b_N &=  \int_0^L 2\basphi_{N_n-1}\dx =
\int_{\xno{N_n-2}}^{\xno{N_n-1}} 2\basphi_{N_n-1}\dx = h\tp
\end{align*}
!et


The complete matrix system, involving all degrees of freedom, takes the form

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
1 & -1 & 0
&\cdots &
\cdots & \cdots & \cdots &
\cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & -1 & 1
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
h \\
2h\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
2h\\
h
\end{array}
\right)
label{fem:deq:1D:ex1:Ab:glob2}
\end{equation}
!et

Incorporation of Dirichlet values can now be done by replacing
the first and last equation by
the very simple equations $c_0=0$ and $c_N=D$, respectively.
Note that the factor $1/h$ in front of the matrix then requires
a factor $h$ to be introduce appropriately on the diagonal in the first and last
row of the matrix.

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
h & 0 & 0
&\cdots &
\cdots & \cdots & \cdots &
\cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & 0 & h
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
0 \\
2h\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
2h\\
D
\end{array}
\right)
label{fem:deq:1D:ex1:Ab:glob3}
\end{equation}
!et

Note that because we do not require $\basphi_i(0)=0$ and
$\basphi_i(L)=0$, $i\in\If$, the boundary term $[u'v]_0^L$,
in principle, gives
contributions $u'(0)\basphi_0(0)$ to $b_0$ and
$u'(L)\basphi_N(L)$ to $b_N$ ($u'\basphi_i$ vanishes for $x=0$ or
$x=L$ for $i=1,\ldots,N-1$).  Nevertheless, we erase these
contributions in $b_0$ and $b_N$ and insert boundary values instead. This
argument shows why we can drop computing $[u'v]_0^L$ at Dirichlet
nodes when we implement the Dirichlet values by modifying the linear
system.

# #ifdef OLD_STUFF
=== Cellwise computations ===

The procedure is to run through all the cells and compute the
element matrix and vector in each cell without thinking about
prescribed boundary values. This means that for P1 elements in
our example, the element matrices in the first and last cell
are $2\times 2$ matrices, exactly as in the other cells.
The element vector has length 2 also in the first and last cell.

In the cells where Dirichlet conditions apply, we perform the
same modification on the element matrix and vector as we did
above on the global matrix system. That is, if $u$ at local
node $r=0$ is known to be zero, we replace the first equation in
the element matrix system
the boundary condition $\tilde c_0 = 0$.
Similarly, if $u$ at local node $r=1$ in the last cell equals $D$,
we replace the last equation in the element matrix system by $\tilde c_1 =D$.

Assembling all the contributions then results in the system
(ref{fem:deq:1D:ex1:Ab:glob2}).
# #endif

===== Symmetric modification of the linear system =====
label{fem:deq:1D:fem:essBC:Bfunc:modsys:symm}

The original matrix system (ref{fem:deq:1D:ex1:Ab:glob}) is symmetric,
but the modifications in (ref{fem:deq:1D:ex1:Ab:glob3}) destroy this
symmetry. Our described modification will in general destroy an
initial symmetry in the matrix system. This is not a particular
computational disadvantage for tridiagonal systems arising in 1D
problems, but may be more serious in 2D and 3D problems when the
systems are large and exploiting symmetry can be important for halving
the storage demands and speeding up computations. Methods for solving
symmetric matrix are also usually more stable and efficient than those
for non-symmetric systems.  Therefore, an alternative modification
which preserves symmetry is attractive.

One can formulate a general algorithm for incorporating a Dirichlet
condition in a symmetric way.
Let $c_k$ be a coefficient corresponding to a known value
$u(\xno{k}) = U_k$.
We want to replace equation $k$ in the system by $c_k=U_k$, i.e.,
insert zeroes in row number $k$ in the coefficient matrix,
set 1 on the diagonal, and replace $b_k$ by $U_k$.
A symmetry-preserving modification consists in first
subtracting column number $k$ in the coefficient matrix, i.e., $A_{i,k}$
for $i\in\If$, times the boundary value $U_k$, from the
right-hand side: $b_i \leftarrow b_i - A_{i,k}U_k$,
$i=0,\ldots,N$. Then we put
zeroes in both row number $k$ *and* column number $k$ in the coefficient matrix,
and finally set $b_k=U_k$. The steps in algorithmic form becomes

  o $b_i \leftarrow b_i - A_{i,k}U_k$ for $i\in\If$
  o $A_{i,k} = A_{k,i} = 0$ for $i\in\If$
  o $A_{k,k}=1$
  o $b_i = U_k$

This modification goes as follows for the specific linear system
written out in (ref{fem:deq:1D:ex1:Ab:glob2}) in
Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys}. First we
subtract the first column in the coefficient matrix, times the boundary
value, from the right-hand side. Because $c_0=0$, this subtraction
has no effect. Then we subtract the last column, times the boundary value $D$,
from the right-hand side. This action results in $b_{N-1}=2h+D/h$ and
$b_N=h-2D/h$. Thereafter, we place zeros in the first and last row and
column in the coefficient matrix and 1 on the two corresponding diagonal
entries. Finally, we set $b_0=0$ and $b_N=D$. The result becomes

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
h & 0 & 0
&\cdots &
\cdots & \cdots & \cdots &
\cdots & 0 \\
0 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & 0 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & 0 & h
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
0 \\
2h\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
2h +D/h\\
D
\end{array}
\right)
label{fem:deq:1D:ex1:Ab:glob3:symm}
\end{equation}
!et

===== Modification of the element matrix and vector =====

The modifications of the global linear system can alternatively
be done for the element matrix and vector. Let us perform the
associated calculations in the computational example where
the element matrix and vector is given by
(ref{fem:deq:1D:ex1:Ab:elm}). The modifications are needed in
cells where one of the degrees of freedom is known. In the
present example, this means
the first and last cell. We compute the element matrix
and vector as if there were no Dirichlet conditions. The boundary term
$[u'v]_0^L$ is simply forgotten at nodes that have Dirichlet conditions
because the modification of the element vector will anyway erase the
contribution from the boundary term. In the first cell,
local degree of freedom number 0
is known and the modification becomes

!bt
\begin{equation}
\tilde A^{(0)} =
A = \frac{1}{h}\left(\begin{array}{rr}
h & 0\\
-1 & 1
\end{array}\right),\quad
\tilde b^{(0)} = \left(\begin{array}{c}
0\\
h
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm:bc:0}
\end{equation}
!et
In the last cell we set

!bt
\begin{equation}
\tilde A^{(N_e)} =
A = \frac{1}{h}\left(\begin{array}{rr}
1 & -1\\
0 & h
\end{array}\right),\quad
\tilde b^{(N_e)} = \left(\begin{array}{c}
h\\
D
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm:bc:N}
\end{equation}
!et

We can also perform the symmetric modification. This operation affects
only the last cell with a nonzero Dirichlet condition. The algorithm
is the same as for the global linear system, resulting in


!bt
\begin{equation}
\tilde A^{(N_e)} =
A = \frac{1}{h}\left(\begin{array}{rr}
1 & 0\\
0 & h
\end{array}\right),\quad
\tilde b^{(N_e)} = \left(\begin{array}{c}
h + D/h\\
D
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm:bc:N:symm}
\end{equation}
!et
The reader is encouraged to assemble the element matrices and vectors and
check that the result coincides with the system
(ref{fem:deq:1D:ex1:Ab:glob3:symm}).

#As a final remark, we repeat that Dirichlet conditions are referred
#to as essential boundary conditions because they require
#quite some work with modifying
#either the linear system or the expansion formula
#for $u$. Boundary conditions for the derivative are much easier to
#implement, as shown next, and therefore deserve the name *natural
#boundary condition*.

!split
======= Boundary conditions: specified derivative =======
label{fem:deq:1D:BC:nat}

Suppose our model problem $-u''(x)=f(x)$ features
the boundary conditions $u'(0)=C$ and $u(L)=D$.
As already indicated in Section ref{fem:deq:1D:varform:ex},
the former condition can be incorporated through the boundary term
that arises from integration by parts. The details of this method will now be
illustrated in the context of finite element basis functions.

===== The variational formulation =====

Starting with the Galerkin method,

!bt
\begin{equation*}
\int_0^L(u''(x)+f(x))\baspsi_i(x) \dx = 0,\quad i\in\If,
\end{equation*}
!et
integrating $u''\baspsi_i$ by parts results in

!bt
\begin{equation*}
\int_0^Lu'(x)'\baspsi_i'(x) \dx -(u'(L)\baspsi_i(L) - u'(0)\baspsi_i(0)) =
\int_0^L f(x)\baspsi_i(x) \dx, \quad i\in\If\tp
\end{equation*}
!et

The first boundary term, $u'(L)\baspsi_i(L)$,
vanishes because $u(L)=D$.
The second boundary
term, $u'(0)\baspsi_i(0)$, can be used to implement the condition $u'(0)=C$,
provided $\baspsi_i(0)\neq 0$ for some $i$ (but with finite elements
we fortunately have $\baspsi_0(0)=1$).
The variational form of the differential equation then becomes

!bt
\begin{equation*} \int_0^Lu'(x)\basphi_i'(x) \dx + C\basphi_i(0) =
\int_0^L f(x)\basphi_i(x) \dx,\quad i\in\If\tp
label{fem:deq:1D:BC:nat:varform}
\end{equation*}
!et

===== Boundary term vanishes because of the test functions =====
label{fem:deq:1D:BC:nat:uLtest}

At points where $u$ is known we may require $\baspsi_i$ to vanish.
Here, $u(L)=D$ and then $\baspsi_i(L)=0$, $i\in\If$. Obviously,
the boundary term $u'(L)\baspsi_i(L)$ then vanishes.

The set of basis functions $\sequencei{\baspsi}$ contains, in this
case, all the finite element basis functions on the mesh, except
the one that is 1 at $x=L$. The basis function that is left out is
used in a boundary function $B(x)$ instead.
With a left-to-right numbering,
$\baspsi_i = \basphi_i$, $i=0,\ldots,N_n-2$, and $B(x)=D\basphi_{N_n-1}$:

!bt
\[ u(x) = D\basphi_{N_n-1}(x) + \sum_{j=0}^{N=N_n-2} c_j\basphi_j(x)\tp\]
!et

Inserting this expansion for $u$ in the variational form
(ref{fem:deq:1D:BC:nat:varform}) leads to the linear system

!bt
\begin{equation}
\sum_{j=0}^{N}\left(
\int_0^L \basphi_i'(x)\basphi_j'(x) \dx \right)c_j =
\int_0^L\left(f(x)\basphi_i(x) -D\basphi_{N_n-1}'(x)\basphi_i'(x)\right) \dx
 - C\basphi_i(0),
label{fem:deq:1D:natBC}
\end{equation}
!et
for $i=0,\ldots,N=N_n-2$.


===== Boundary term vanishes because of linear system modifications =====
label{fem:deq:1D:BC:nat:uLmod}

We may, as an alternative to the approach in the previous section, use
a basis $\sequencei{\baspsi}$ which contains all the finite element
functions on the mesh: $\baspsi_i=\basphi_i$, $i=0,\ldots,N_n-1=N$.  In
this case, $u'(L)\baspsi_i(L)=u'(L)\basphi_i(L)\neq 0$ for the $i$
corresponding to the boundary node at $x=L$ (where $\basphi_i=1$).
The number of this node is $i=N_n-1=N$ if a left-to-right numbering of
nodes is utilized.

However, even though $u'(L)\basphi_{N_n-1}(L)\neq 0$, we do not need to
compute this term.  For $i<N_n-1$ we realize that $\basphi_i(L)=0$.  The
only nonzero contribution to the right-hand side comes from $i=N$
($b_N$). Without a boundary function we must implement the
condition $u(L)=D$ by the equivalent statement $c_N=D$ and modify the
linear system accordingly. This modification will erase the last
row and replace $b_N$ by another value. Any attempt to compute
the boundary term $u'(L)\basphi_{N_n-1}(L)$ and store it in $b_N$ will be
lost. Therefore, we can safely forget about boundary terms
corresponding to Dirichlet boundary conditions also when we use
the methods from Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys}
or Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys:symm}.

The expansion for $u$ reads

!bt
\begin{equation*}
u(x) = \sum_{j\in\If} c_j\basphi_j(x)\tp
\end{equation*}
!et
Insertion in the variational form
(ref{fem:deq:1D:BC:nat:varform}) leads to
the linear system

!bt
\begin{equation}
\sum_{j\in\If}\left(
\int_0^L \basphi_i'(x)\basphi_j'(x) \dx \right)c_j =
\int_0^L\left(f(x)\basphi_i(x)\right) \dx
 - C\basphi_i(0),\quad i\in\If
\tp
label{fem:deq:1D:natBC2}
\end{equation}
!et
After having computed the system, we replace the last row by
$c_N=D$, either straightforwardly as in
Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys} or in a symmetric
fashion as in Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys:symm}.
These modifications can also be performed in the element matrix and
vector for the right-most cell.


===== Direct computation of the global linear system =====
label{fem:deq:1D:BC:nat:Aub}

We now turn to actual computations with P1 finite elements.
The focus is on how the linear system and
the element matrices and vectors are modified by the
condition $u'(0)=C$.

Consider first the approach where Dirichlet conditions are incorporated
by a $B(x)$ function and the known degree of freedom
$C_{N_n-1}$ is left out of the linear system
(see Section ref{fem:deq:1D:BC:nat:uLtest}).
The relevant formula for the linear
system is given by (ref{fem:deq:1D:natBC}).
There are three differences compared to the extensively
computed case where $u(0)=0$ in Sections
ref{fem:deq:1D:comp:global} and ref{fem:deq:1D:comp:elmwise}.
First, because we do not have a Dirichlet
condition at the left boundary, we need to extend the linear system
(ref{fem:deq:1D:ex1:Ab:glob}) with an equation associated with the node
$\xno{0}=0$.
According to Section ref{fem:deq:1D:fem:essBC:Bfunc:modsys}, this
extension consists of including $A_{0,0}=1/h$, $A_{0,1}=-1/h$, and $b_0=h$.
For $i>0$ we have $A_{i,i}=2/h$, $A_{i-1,i}=A_{i,i+1}=-1/h$.
Second, we need to include
the extra term
$-C\basphi_i(0)$ on the right-hand side. Since all $\basphi_i(0)=0$
for $i=1,\ldots,N$, this term reduces to $-C\basphi_0(0)=-C$ and
affects only the first equation ($i=0$). We simply add $-C$ to $b_0$
such that $b_0=h - C$.
Third, the boundary term $-\int_0^L D\basphi_{N_n-1}(x)\basphi_i\dx$
must be computed. Since $i=0,\ldots,N=N_n-2$, this integral can only
get a nonzero contribution with $i=N_n-2$ over the last cell.
The result becomes $-Dh/6$.
The resulting linear system can be summarized in the form

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
1 & -1 & 0 &\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & -1 & 2
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
h - C \\
2h\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
2h - Dh/6
\end{array}
\right)\tp
label{fem:deq:1D:BC:nat:Aub:system}
\end{equation}
!et


Next we consider the technique where we modify the linear system to
incorporate Dirichlet conditions
(see Section ref{fem:deq:1D:BC:nat:uLmod}). Now $N=N_n-1$.
The two differences from the
case above is that the $-\int_0^LD\basphi_{N_n-1}\basphi_i\dx$ term is
left out of the right-hand side and an extra last row associated
with the node $\xno{N_n-1}=L$ where the Dirichlet condition applies
is appended to the system.
This last row is anyway replaced by the condition $c_N=D$ or this
condition can be incorporated in a symmetric fashion. Using the simplest,
former approach gives

!bt
\begin{equation}
\frac{1}{h}\left(
\begin{array}{ccccccccc}
1 & -1 & 0 &\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & -1 & 2  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & 0 & h
\end{array}
\right)
\left(
\begin{array}{c}
c_0 \\
\vdots\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\vdots\\
c_{N}
\end{array}
\right)
=
\left(
\begin{array}{c}
h - C \\
2h\\
\vdots\\
\vdots \\
\vdots \\
\vdots \\
\vdots \\
2h \\
D
\end{array}
\right)\tp
label{fem:deq:1D:BC:nat:Aub:system:mod}
\end{equation}
!et

# Note: show equivalence between the two systems!!!

===== Cellwise computations =====

Now we compute with one element at a time, working in the reference
coordinate system $X\in [-1,1]$.
We need to see how the
$u'(0)=C$ condition affects the element matrix and vector.
The extra term $-C\basphi_i(0)$ in the variational formulation
only affects the element vector in the first cell.
On the reference cell, $-C\basphi_i(0)$ is transformed to
$-C\refphi_r(-1)$, where $r$ counts local degrees of freedom.
We have $\refphi_0(-1)=1$ and $\refphi_1(-1)=0$ so
we are left with the contribution
$-C\refphi_0(-1)=-C$ to $\tilde b^{(0)}_0$:

!bt
\begin{equation}
\tilde A^{(0)} =
A = \frac{1}{h}\left(\begin{array}{rr}
1 & 1\\
-1 & 1
\end{array}\right),\quad
\tilde b^{(0)} = \left(\begin{array}{c}
h - C\\
h
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm:bc:nat}
\end{equation}
!et
No other element matrices or vectors are affected by the $-C\basphi_i(0)$
boundary term.

There are two alternative ways of incorporating the Dirichlet condition.
Following Section ref{fem:deq:1D:BC:nat:uLtest}, we get
a $1\times 1$ element matrix in the last cell and
an element vector with an extra term containing $D$:

!bt
\begin{equation}
\tilde A^{(e)} =\frac{1}{h}\left(\begin{array}{r}
1
\end{array}\right),\quad
\tilde b^{(e)} = h\left(\begin{array}{c}
1 - D/6
\end{array}\right),
label{fem:deq:1D:ex1:Ab:elm:ends2}
\end{equation}
!et

Alternatively, we include the degree of freedom at the node with
$u$ specified. The element matrix and vector must then be modified
to constrain the $\tilde c_1 = c_N$ value at local node $r=1$:

!bt
\begin{equation}
\tilde A^{(N_e)} =
A = \frac{1}{h}\left(\begin{array}{rr}
1 & 1\\
0 & h
\end{array}\right),\quad
\tilde b^{(N_e)} = \left(\begin{array}{c}
h\\
D
\end{array}\right)\tp
label{fem:deq:1D:ex1:Ab:elm:bc:nat:mod}
\end{equation}
!et

# Assemble and show that it is correct

# #ifdef 2DO

===== Summarizing finite element computing =====
label{fem:deq:1D:fem2}

We are now in a position to solve a more general differential equation
problem by the finite element and thereby summarize a lot of details.
The problem to be addressed is

!bt
\begin{equation}
-\frac{d}{dx}\left( a(x)\frac{du}{dx}\right) + \gamma u = f(x),
\quad x\in\Omega=[0,L],\quad u(0)=\alpha,\ u'(L)=\beta\tp
label{fem:deq:1D:model4}
\end{equation}
!et
We choose $a(x)=1+x^2$. Then
!bt
\begin{equation} u(x) = \alpha + \beta(1+L^2)\tan^{-1}(x),
\end{equation}
!et
is an exact solution if
!bt
\begin{equation}
f(x) = \gamma u
\tp
\end{equation}
!et

We shall first go through the computations without any post-modifications
of the linear systems, i.e., we work with $u=B + \sum_j c_j \basphi_j$, where
the sum over $j$ only involves the degrees of freedom that are not known
beforehand.

The test and trial space $V$ must consists of finite element
functions vanishing for
$x=0$ since $u(0)$ is specified as boundary condition.
Numbering cells and nodes from left to right through the domain,
we know that $c_0=\alpha$ and hence
we let $B(x)=\alpha\basphi_0$. The expansion of $u$ is then

!bt
\begin{equation*}
u(x) = \alpha\basphi_0(x) + \sum_{j=1}^N c_j\basphi_j(x)
\tp  \end{equation*}
!et

To derive the appropriate variational formulation, based on Galerkin's
method, we first multiply the differential equation by a test function
$v\in V$ and integrate over the domain:

!bt
\begin{equation*}
\int_0^L (-((1+x^2)u'(x))' + \gamma u)v \dx =
\int_0^L f(x)v \dx\quad\forall v\in V\tp
\end{equation*}
!et
Then we integrate by parts, use that $v(0)=0$ in the boundary term,
and insert the boundary condition $u'(L)=\beta$ in the other boundary term:

!bt
\begin{equation}
\int_0^L ((1+x^2)u'v' + \gamma uv) \dx = \int_0^L f \dx + \beta v(L)
\quad\forall v\in V\tp
label{fem:deq:1D:model4:varform0}
\end{equation}
!et

For detailed calculations by hand we now insert the expansion for $u$
and after some algebraic manipulations arrive at the linear system

!bt
\begin{equation}
\sum_{j=1}^N \left(\int_0^1 \basphi_i'\basphi_j' \right) \dx
= \int_0^1(f - b_0\basphi_0) \dx + b_L\basphi_i(L),\quad i\in\If\tp
label{fem:deq:1D:model4:varform}
\end{equation}
!et
The cellwise computational procedure consists in splitting
the integral to integrals over each cell, and transforming each
cell integral to an integral over a reference cell on $[-1,1]$
having $X$ as coordinate. Sections ref{fem:approx:fe:elementwise}
and ref{fem:approx:fe:mapping} explains the details.

However, there is one complicating factor arising here: we need to
compute the derivatives $\basphi_i'(x)$ in the reference cell.
What we have on the reference cell, is the expression $\refphi_r(X)$,
$r=0,\ldots,d$.
We can easily compute $d\refphi_r/dX$, but what we need is
$d\refphi_r(X)/dx$, which equals the desired factor
$\basphi_{q(e,r)}(x)$ in the expressions for the matrix entries.
By the chain rule we have that

!bt
\begin{equation*}
\frac{d}{dx}\refphi_r(X) = \frac{d}{dX}\refphi_r(X)\frac{dX}{dx}\tp
\end{equation*}
!et
From the mapping (ref{fem:approx:fe:affine:mapping}) it follows
that

!bt
\begin{equation*} \frac{dX}{dx} = \frac{2}{h},\end{equation*}
!et
where $h$ is the length of the current cell. We hence have

!bt
\begin{equation}
\frac{d\refphi_r}{dx} = \frac{2}{h}\frac{d\refphi_r}{dX}\tp
label{fem:deq:1D:dphidx:ref}
\end{equation}
!et

The expressions for the entries in the element matrix and vector become

!bt
\begin{align}
\tilde A^{(e)}_{r,s}
&= \int_{-1}^1 \frac{d\refphi_r}{dx}\frac{d\refphi_s}{dx}\det J\, \dX
= \int_{-1}^1 \frac{2}{h}\frac{d\refphi_r}{dX}\frac{2}{h}\frac{d\refphi_s}{dX}
\frac{h}{2}\, \dX, label{fem:deq:1D:model4:Ae}\\
\tilde b^{(e)}_{r}
&= \int_{-1}^1 f(x(X))\refphi_r(X)\det J\, \dX =
\int_{-1}^1 f(x(X))\refphi_r(X)\frac{2}{2}\, \dX\tp
label{fem:deq:1D:model4:be}
\end{align}
!et


The specific derivatives of the basis functions with respect to $X$ are
easily computed. For P1 (linear, $d=1$) elements we get

!bt
\begin{align}
\refphi_0(X) &= \half (1-X),\\
\frac{d\refphi_0}{dX} &= -\half,\\
\refphi_1(X) &= \half (1+X),\\
\frac{d\refphi_1}{dX} &= \half\tp
label{fem:deq:1D:P1}
\end{align}
!et
The results for P2 (quadratic, $d=2$) elements become

!bt
\begin{align}
\refphi_0(X) &= \half (X-1)X\\
\frac{d\refphi_0}{dX} &= \half(2X - 1)\tp
\refphi_1(X) &= 1 - X^2\\
\frac{d\refphi_1}{dX} &= -2X\tp
\refphi_2(X) &= \half (X+1)X
\frac{d\refphi_2}{dX} &= \half (2X+1)\tp
label{fem:deq:1D:P2}
\end{align}
!et

Let us calculate the element matrix from the formula
(ref{fem:deq:1D:model4:Ae}) in case of P1 elements:

!bt
\begin{align*}
\tilde A^{(e)}_{0,0} &=
\int_{-1}^1 \frac{2}{h}\frac{d\refphi_0}{dX}\frac{2}{h}\frac{d\refphi_0}{dX}
\frac{h}{2}\, \dX
= \frac{2}{h}\int_{-1}^1 \left(\frac{d\refphi_0}{dX}\right)^2 \dX
= \frac{2}{h}\int_{-1}^1 \left(-\half\right)^2 \dX = \frac{1}{h},
\tilde A^{(e)}_{0,1} &=
\frac{2}{h}\int_{-1}^1\frac{d\refphi_0}{dX}\frac{d\refphi_1}{dX}\, \dX =
\frac{2}{h}\int_{-1}^1 (-\half)\half\, \dX = -\frac{1}{h},\\
\tilde A^{(e)}_{1,0} &=
\frac{2}{h}\int_{-1}^1\frac{d\refphi_1}{dX}\frac{d\refphi_0}{dX}\, \dX =
\frac{2}{h}\int_{-1}^1 \half(-\half)\, \dX = -\frac{1}{h},\\
\tilde A^{(e)}_{1,1} &=
\frac{2}{h}\int_{-1}^1\frac{d\refphi_1}{dX}\frac{d\refphi_1}{dX}\, \dX =
\frac{2}{h}\int_{-1}^1 \half\half)\, \dX = \frac{1}{h}\tp  \end{align*}
!et
These are valid expressions for any element but the first. For the
first element, local 0 is on the boundary where $u$ is known, and the
corresponding coefficient $c_0$ is left out of the global system.
We therefore only have one active node in the first element and
consequently we form only the $A^{(0)}_{1,1}$ entry in the
element matrix, which is treated as a $1\times 1$ matrix.

For the right-hand side we achieve these results:

!bt
\begin{align*}
b^{(e)}_0 = \int_{-1}^1 2\refphi_0(X)\det J\, \dX =
2\frac{h}{2}\int_{-1}^1 \half(1-X) \dX = h,\\
b^{(e)}_1 = \int_{-1}^1 2\refphi_1(X)\det J\, \dX =
2\frac{h}{2}\int_{-1}^1 \half(1+X) \dX = h\tp  \end{align*}
!et
Also in the first element we compute only $b^{(0)}_1$ since
the unknown associated with local 0 does not enter the global linear
system.
For the last element, $e=N_e-1=N_n-2$, we get an additional contribution
from the boundary term $b_L\refphi_r(1)$ (which corresponds to
$b_L\basphi_i(L)$, $i=q(e,r)$, $r=0,1$):

!bt
\begin{equation}
b^{(e)}_0 = h + b_L\refphi_0(1)=h,\quad b^{(e)}_1 = h +
b_L\refphi_1(1) = h + b_L
\end{equation}
!et

The various entries can be collected in a $2\times 2$ element matrix and
$2$-vector for all the internal cells (not having boundary nodes),

!bt
\begin{equation}
A^{(e)} = \frac{1}{h}
\left(\begin{array}{rr}
1 & -1\\
-1 & 1
\end{array}\right),\quad
b^{(e)} = h
\left(\begin{array}{c}
1 \\
1
\end{array}\right)
\end{equation}
!et
For the first element we have

!bt
\begin{equation}
A^{(e)} = \frac{1}{h} (1),\quad
b^{(e)} = h(1),
\end{equation}
!et
while for the last element,

!bt
\begin{equation}
A^{(e)} = \frac{1}{h}
\left(\begin{array}{rr}
1 & -1\\
-1 & 1
\end{array}\right),\quad
b^{(e)} =
\left(\begin{array}{c}
h \\
h + b_L
\end{array}\right)\tp  \end{equation}
!et

Assuming that all cells have the same length, we can now assemble these
element matrices and vectors into the global matrix and vector in the
linear system. We introduce a standard 1D mesh with vertices and cells
numbered from left to right. The vertices are then
$\xno{i}=i h$, $i\in\If$, $h=L/N$, and cells are $\Omega^{(e)}
=[\xno{e},\xno{e+1}]$, $e=0,\ldots,N_n-1$.
Here is the result of applying the assembly algorithm from
Section ref{fem:approx:fe:elementwise} to form the global
coefficient matrix

!bt
\begin{equation}
A = \frac{1}{h}
\left(
\begin{array}{cccccccccc}
2 & -1 & 0 &\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
-1 & 2 & -1 & \ddots &   & &  & &  \vdots \\
0 & -1 & 2 & -1 & \ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -1 & 2 & -1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & -1  & 2  & -1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & -1 & 1
\end{array}
\right)
label{fem:deq:1D:P1:Acb}
\end{equation}
!et
The right-hand side becomes

!bt
\begin{equation}
b = \frac{1}{h}
\left(
\begin{array}{c}
2h\\
2h\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
2h\\
h + b_0
\end{array}
\right)
\end{equation}
!et
The unknown vector to be solved for is $c=(c_1,c_2,\ldots,c_N)^T$.

Looking at the product $Ac$,
we may write the equation system on the following alternative form:

!bt
\begin{equation}
\frac{1}{h}(-c_{i-1} + 2c_i - c_{i+1}) = 2h,\quad i=1,\ldots,N-1,
label{fem:deq:1D:model4:FE:FD}
\end{equation}
!et
with $c_0=b_0$,
and

!bt
\begin{equation}
\frac{1}{h}(-c_{N-1} + c_N ) = h + b_L
label{fem:deq:1D:model4:FE:FD:xL}
\end{equation}
!et
for the last equation.

Let us look at what a finite difference discretization of
(ref{fem:deq:1D:model4}) will look like, using a standard
second-order central difference for the second-order derivative:

!bt
\begin{equation}
\frac{-u_{i-1}+2u_j -u_{i+1}}{h^2} = 2,\quad i=1,\ldots,N-1,
label{fem:deq:1D:model4:FD}
\end{equation}
!et
where $u_i$ is the value of $u$ at mesh point $i$. We have assumed
$N+1$ equally spaced mesh points, with spacing $h$.
From the boundary condition $u(0)=b_0$, $u_0=b_0$.
The condition $u'(L)=b_L$ is discretized by

!bt
\begin{equation*} \frac{u_{N+1}-u_{N-1}}{2h}=b_L\quad\Leftrightarrow\quad
u_{N+1}=u_{N-1}-2hb_L,\end{equation*}
!et
which can be inserted in the difference equation for $i=N$, yielding

!bt
\begin{equation*}
\frac{-2u_{N-1}+2u_N }{h^2} = \frac{1}{h}2b_L + 2,
\end{equation*}
!et
which upon multiplication by $h$ equals

!bt
\begin{equation}
\frac{1}{h}(-u_{N-1}+u_N ) = b_L + h\tp
label{fem:deq:1D:model4:FD:xL}
\end{equation}
!et
Multiplying (ref{fem:deq:1D:model4:FD}) by $h$ reveals that
the finite difference equations (ref{fem:deq:1D:model4:FD}) and
(ref{fem:deq:1D:model4:FD:xL}) are identical to
the finite element equations
(ref{fem:deq:1D:model4:FE:FD}) and (ref{fem:deq:1D:model4:FE:FD:xL}).
Recall that $c_i = u(\xno{i})$ which is denoted by $u_i$ in the finite
difference formulation.

In this (simple) test problem all the finite element machinery ends
up with the same equations that a finite difference methods quickly
provides. The next example shows that with a slightly more complicated
$f(x)$ function, the finite element and difference methods differ
in the representation of $f(x)$ in the difference equations.

A nice feature in the present example is that the exact solution
fulfills the discrete equations, implying that the finite element or
difference solution is exact, regardless of $h$. This is easily
proved in a `sympy` session:

!bc ipy
>>> from sympy import *
>>> i, b_L, b_0, h, N = symbols('i b_L b_0 h N')
>>> L = N*h
>>> x = i*h
>>> u_i = -x**2 + (b_L + 2*L)*x + b_0
>>> u_im1 = u_i.subs(i, i-1)
>>> u_ip1 = u_i.subs(i, i+1)
>>>
>>> # General equation
>>> R = 1/h**2*(-u_im1 + 2*u_i - u_ip1) - 2
>>> R = simplify(R)
>>> print R
0
>>>
>>> # Right boundary equation
>>> R = 1/h**2*(-u_im1 + u_i) - b_L/h - 1
>>> R = R.subs(i, N)
>>> R = simplify(R)
>>> print R
0
!ec

# #endif

!split
======= Implementation of finite element algorithms =======
label{fem:deq:1D:code}

At this point, it is sensible to create a program with symbolic
calculations to perform all the steps in the computational machinery,
both for automating the work and for documenting the complete
algorithms.  As we have seen, there are quite many details involved
with finite element computations and incorporation of boundary
conditions.  An implementation will also act as a structured summary
of all these details.


===== Extensions of the code for approximation =====
label{fem:deq:1D:code:fe}

Implementation of the finite element algorithms for differential
equations follows closely the algorithm for approximation of functions.
The new additional ingredients are

 o other types of integrands (as implied by the variational formulation)
 o additional boundary terms in the variational formulation for
   Neumann boundary conditions
 o modification of element matrices and vectors due to Dirichlet
   boundary conditions

Point 1 and 2 can be taken care of by letting the user supply
functions defining the integrands and boundary terms on the
left- and right-hand side of the equation system:

 * Integrand on the left-hand side: `ilhs(e, phi, r, s, X, x, h)`
 * Integrand on the right-hand side: `irhs(e, phi, r, X, x, h)`
 * Boundary term on the left-hand side: `blhs (e, phi, r, s, X, x, h)`
 * Boundary term on the right-hand side: `brhs (e, phi, r, s, X, x, h)`

Here, `phi` is a dictionary where `phi[q]` holds a list of the
derivatives of order `q` of the basis functions with respect to the
physical coordinate $x$.  The derivatives are available as Python
functions of `X`.  For example, `phi[0][r](X)` means $\refphi_r(X)$,
and `phi[1][s](X, h)` means $d\refphi_s (X)/dx$ (we refer to
the file "`fe1D.py`": "${fem_src}/fe1D.py" for details
regarding the function `basis` that computes the `phi`
dictionary).  The `r` and `s`
arguments in the above functions correspond to the index in the
integrand contribution from an integration point to $\tilde
A^{(e)}_{r,s}$ and $\tilde b^{(e)}_r$. The variables `e` and `h` are
the current element number and the length of the cell, respectively.
Specific examples below will make it clear how to construct these
Python functions.

Given a mesh represented by `vertices`, `cells`, and `dof_map` as
explained before, we can write a pseudo Python code to list all
the steps in the computational algorithm for finite element solution
of a differential equation.

!bc pycod
<Declare global matrix and rhs: A, b>

for e in range(len(cells)):

    # Compute element matrix and vector
    n = len(dof_map[e])  # no of dofs in this element
    h = vertices[cells[e][1]] - vertices[cells[e][1]]
    <Initialize element matrix and vector: A_e, b_e>

    # Integrate over the reference cell
    points, weights = <numerical integration rule>
    for X, w in zip(points, weights):
        phi = <basis functions and derivatives at X>
        detJ = h/2
        dX = detJ*w

        x = <affine mapping from X>
        for r in range(n):
            for s in range(n):
                A_e[r,s] += ilhs(e, phi, r, s, X, x, h)*dX
            b_e[r] += irhs(e, phi, r, X, x, h)*dX

    # Add boundary terms
    for r in range(n):
        for s in range(n):
            A_e[r,s] += blhs(e, phi, r, s, X, x)*dX
        b_e[r] += brhs(e, phi, r, X, x, h)*dX

    # Incorporate essential boundary conditions
    for r in range(n):
        global_dof = dof_map[e][r]
        if global_dof in essbc:
            # local dof r is subject to an essential condition
            value = essbc[global_dof]
            # Symmetric modification
            b_e -= value*A_e[:,r]
            A_e[r,:] = 0
            A_e[:,r] = 0
            A_e[r,r] = 1
            b_e[r] = value

    # Assemble
    for r in range(n):
        for s in range(n):
            A[dof_map[e][r], dof_map[e][s]] += A_e[r,s]
        b[dof_map[e][r] += b_e[r]

<solve linear system>
!ec

Having implemented this function, the user only has supply the `ilhs`,
`irhs`, `blhs`, and `brhs` functions that specify the PDE and boundary
conditions. The rest of the implementation forms a generic
computational engine. The big finite element packages Diffpack and FEniCS
are structured exactly the same way. A sample implementation of `ilhs`
for the 1D Poisson problem is:

!bc pycod
   def integrand_lhs(phi, i, j):
        return phi[1][i]*phi[1][j]
!ec
which returns  $d\refphi_i (X)/dx d\refphi_j (X)/dx$.  Reducing the
amount of code the user has to supply and making the code as close as
possible to the mathematical formulation makes the software user-friendly and
easy to debug.

A complete function `finite_element1D_naive` for the 1D finite
algorithm above, is found in the file "`fe1D.py`":
"${fem_src}/fe1D.py". The term ``naive'' refers to a version of the
algorithm where we use a standard dense square matrix as global matrix
`A`. The implementation also has a verbose mode for printing out the
element matrices and vectors as they are computed.  Below is the
complete function without the print statements.  You should study in
detail since it contains all the steps in the finite element
algorithm.

!bc pycod
def finite_element1D_naive(
    vertices, cells, dof_map,     # mesh
    essbc,                        # essbc[globdof]=value
    ilhs,                         # integrand left-hand side
    irhs,                         # integrand right-hand side
    blhs=lambda e, phi, r, s, X, x, h: 0,
    brhs=lambda e, phi, r, X, x, h: 0,
    intrule='GaussLegendre',      # integration rule class
    verbose=False,                # print intermediate results?
    ):
    N_e = len(cells)
    N_n = np.array(dof_map).max() + 1

    A = np.zeros((N_n, N_n))
    b = np.zeros(N_n)

    for e in range(N_e):
        Omega_e = [vertices[cells[e][0]], vertices[cells[e][1]]]
        h = Omega_e[1] - Omega_e[0]

        d = len(dof_map[e]) - 1  # Polynomial degree
        # Compute all element basis functions and their derivatives
        phi = basis(d)

        # Element matrix and vector
        n = d+1  # No of dofs per element
        A_e = np.zeros((n, n))
        b_e = np.zeros(n)

        # Integrate over the reference cell
        if intrule == 'GaussLegendre':
            points, weights = GaussLegendre(d+1)
        elif intrule == 'NewtonCotes':
            points, weights = NewtonCotes(d+1)

        for X, w in zip(points, weights):
            detJ = h/2
            x = affine_mapping(X, Omega_e)
            dX = detJ*w

            # Compute contribution to element matrix and vector
            for r in range(n):
                for s in range(n):
                    A_e[r,s] += ilhs(phi, r, s, X, x, h)*dX
                b_e[r] += irhs(phi, r, X, x, h)*dX

        # Add boundary terms
        for r in range(n):
            for s in range(n):
                A_e[r,s] += blhs(phi, r, s, X, x, h)
            b_e[r] += brhs(phi, r, X, x, h)

        # Incorporate essential boundary conditions
        modified = False
        for r in range(n):
            global_dof = dof_map[e][r]
            if global_dof in essbc:
                # dof r is subject to an essential condition
                value = essbc[global_dof]
                # Symmetric modification
                b_e -= value*A_e[:,r]
                A_e[r,:] = 0
                A_e[:,r] = 0
                A_e[r,r] = 1
                b_e[r] = value
                modified = True

        # Assemble
        for r in range(n):
            for s in range(n):
                A[dof_map[e][r], dof_map[e][s]] += A_e[r,s]
            b[dof_map[e][r]] += b_e[r]

    c = np.linalg.solve(A, b)
    return c, A, b, timing
!ec
The `timing` object is a dictionary holding the CPU spent on computing
`A` and the CPU time spent on solving the linear system. (We have
left out the timing statements.)

===== Utilizing a sparse matrix =====
label{fem:deq:1D:code:fe_sparse}

A potential efficiency problem with the `finite_element1D_naive` function
is that it uses dense $(N+1)\times (N+1)$ matrices, while we know that
only $2d+1$ diagonals around the main diagonal are different from zero.
Switching to a sparse matrix is very easy. Using the DOK (dictionary of
keys) format, we declare `A` as

!bc pycod
import scipy.sparse
A = scipy.sparse.dok_matrix((N_n, N_n))
!ec
Assignments or in-place arithmetics are done as for a dense matrix,

!bc
A[i,j] += term
A[i,j]  = term
!ec
but only the index pairs `(i,j)` we have used in assignments or
in-place arithmetics are actually stored.
A tailored solution algorithm is needed. The most reliable is
sparse Gaussian elimination:

!bc pycod
import scipy.sparse.linalg
c = scipy.sparse.linalg.spsolve(A.tocsr(), b, use_umfpack=True)
!ec
The declaration of `A` and the solve statement are the only
changes needed in the `finite_element1D_naive` to utilize
sparse matrices. The resulting modification is found in the
function `finite_element1D`.

===== Example: application to our model problem  =====

Let us demonstrate the finite element software on

!bt
\[ -u''(x)=f(x),\quad x\in (0,L),\quad u'(0)=C,\ u(L)=D\tp\]
!et
This problem can be analytically solved by the
`model2` function from Section ref{fem:deq:1D:models:simple}.
Let $f(x)=x^2$. Calling `model2(x**2, L, C, D)` gives

!bt
\[ u(x) = D + C(x-L) + \frac{1}{12}(L^4 - x^4) \]
!et

The variational formulation reads

!bt
\[ (u', v) = (x^2, v) - Cv(0)\tp\]
!et
The entries in the element matrix and vector,
which we need to set up the `ilhs`, `irhs`,
`blhs`, and `brhs` functions, becomes

!bt
\begin{align*}
A^{(e)}_{r,s} &= \int_{-1}^1 \frac{d\refphi_r}{dx}\frac{\refphi_s}{dx}(\det J\dX),\\
b^{(e)} &= \int_{-1}^1 x^2\refphi_r\det J\dX - C\refphi_r(-1)I(e,0),
\end{align*}
!et
where $I(e)$ is an indicator function: $I(e,q)=1$ if $e=q$, otherwise $I(e)=0$.
We use this indicator function to formulate that the boundary term
$Cv(0)$, which in the local element coordinate system becomes $C\refphi_r(-1)$,
is only included for the element $e=0$.

The functions for specifying the element matrix and vector entries
must contain the integrand, but without the $\det J\dX$ term
as this term is taken care of by the quadrature loop, and
the derivatives $d\refphi_r(X)/dx$
with respect to the physical $x$ coordinates are
contained in `phi[1][r](X)`, computed by the function `basis`.


!bc pycod
def ilhs(e, phi, r, s, X, x, h):
    return phi[1][r](X, h)*phi[1][s](X, h)

def irhs(e, phi, r, X, x, h):
    return x**2*phi[0][r](X)

def blhs(e, phi, r, s, X, x, h):
    return 0

def brhs(e, phi, r, X, x, h):
    return -C*phi[0][r](-1) if e == 0 else 0
!ec
We can then make the call to `finite_element1D_naive` or `finite_element1D`
to solve the problem with two P1 elements:

!bc pycod
from fe1D import finite_element1D_naive, mesh_uniform
C = 5;  D = 2;  L = 4
d = 1

vertices, cells, dof_map = mesh_uniform(
    N_e=2, d=d, Omega=[0,L], symbolic=False)
essbc = {}
essbc[dof_map[-1][-1]] = D

c, A, b, timing = finite_element1D(
    vertices, cells, dof_map, essbc,
    ilhs=ilhs, irhs=irhs, blhs=blhs, brhs=brhs,
    intrule='GaussLegendre')
!ec
It remains to plot the solution (with high resolution in each element).
To this end, we use the `u_glob` function imported from
`fe1D`, which imports it from `fe_approx1D_numit` (the
`u_glob` function in `fe_approx1D.py`
works with `elements` and `nodes`, while `u_glob` in
`fe_approx1D_numint` works with `cells`, `vertices`,
and `dof_map`):

!bc pycod
u_exact = lambda x: D + C*(x-L) + (1./6)*(L**3 - x**3)
from fe1D import u_glob
x, u, nodes = u_glob(c, cells, vertices, dof_map)
u_e = u_exact(x, C, D, L)
print u_exact(nodes, C, D, L) - c  # difference at the nodes

import matplotlib.pyplot as plt
plt.plot(x, u, 'b-', x, u_e, 'r--')
plt.legend(['finite elements, d=%d' %d, 'exact'], loc='upper left')
plt.show()
!ec
The result is shown in Figure ref{fem:deq:1D:code:fe:fig1}. We see
that the solution using P1 elements is exact at the nodes, but
feature considerable discrepancy between the nodes.
Exercise ref{fem:deq:exer:1D:exact_numerics} asks you to explore
this problem further using other $m$ and $d$ values.

FIGURE: [fig/uxx_x2, width=500 frac=0.8] Finite element and exact solution using two cells. label{fem:deq:1D:code:fe:fig1}


!split
======= Variational formulations in 2D and 3D =======
label{fem:deq:2D:varform}

The major difference between deriving variational formulations in 2D
and 3D compared to 1D is the rule for integrating by parts.  The cells
have shapes different from an interval, so basis functions look a bit
different, and there is a technical difference in actually calculating
the integrals over cells. Otherwise, going to 2D and 3D is not a big
step from 1D. All the fundamental ideas still apply.

===== Integration by parts =====

A typical second-order term in a PDE may be written in dimension-independent
notation as

!bt
\[ \nabla^2 u \quad\hbox{or}\quad \nabla\cdot\left( \dfc(\x)\nabla u\right)
\tp
\]
!et
The explicit forms in a 2D problem become
!bt
\[ \nabla^2 u = \nabla\cdot\nabla u =
\frac{\partial^2 u}{\partial x^2} +
\frac{\partial^2 u}{\partial y^2},
\]
!et
and
!bt
\[
\nabla\cdot\left( a(\x)\nabla u\right) =
\frac{\partial}{\partial x}\left( \dfc(x,y)\frac{\partial u}{\partial x}\right) +
\frac{\partial}{\partial y}\left( \dfc(x,y)\frac{\partial u}{\partial y}\right)
\tp
\]
!et
We shall continue with the latter operator as the former arises from
just setting $\dfc =1$.

!bnotice The integration by parts formula for $\int\nabla\cdot(\dfc\nabla)$
The general rule for integrating by parts is often referred to as
"Green's first identity": "http://en.wikipedia.org/wiki/Green's_identities":

!bt
\begin{equation}
-\int_{\Omega} \nabla\cdot (\dfc(\x)\nabla u) v\dx =
\int_{\Omega} \dfc(\x)\nabla u\cdot\nabla v \dx -
\int_{\partial\Omega} a\frac{\partial u}{\partial n} v \ds,
label{fem:deq:2D:int:by:parts}
\end{equation}
!et
where $\partial\Omega$ is the boundary of $\Omega$ and
$\partial u/\partial n = \normalvec\cdot\nabla u$ is the derivative
of $u$ in the outward normal direction, $\normalvec$ being an outward
unit normal to $\partial\Omega$. The integrals $\int_\Omega ()\dx$ are
area integrals in 2D and volume integrals in 3D, while
$\int_{\partial\Omega} ()\ds$ is a line integral in 2D and a surface
integral in 3D.
!enotice

It will be convenient to divide the boundary into two parts:

 * $\partial\Omega_N$, where we have Neumann conditions
   $-a\frac{\partial u}{\partial n} = g$, and
 * $\partial\Omega_D$, where we have Dirichlet conditions
   $u = u_0$.

The test functions $v$ are (as usual) required to vanish on
$\partial\Omega_D$.

===== Example on a multi-dimensional variational problem =====
label{sec:varform:general:convdiff}

Here is a quite general, stationary, linear PDE arising in many problems:

!bt
\begin{align}
label{varform:conv:diff:pde:pre}
\v\cdot\nabla u + \beta u &= \nabla\cdot\left( \dfc\nabla u\right) + f,
\quad\x\in\Omega,\\
label{varform:conv:diff:bc1:pre}
u &= u_0,\quad\x\in\partial\Omega_D,\\
label{varform:conv:diff:bc2:pre}
-\dfc\frac{\partial u}{\partial n} &= g,\quad\x\in\partial\Omega_N
\tp
\end{align}
!et
The vector field $\v$ and the scalar functions $a$, $\alpha$, $f$, $u_0$, and
$g$ may vary with the spatial coordinate $\x$ and must be known.

Such a second-order PDE needs exactly one boundary condition at each
point of the boundary, so $\partial\Omega_N\cup\partial\Omega_D$
must be the complete boundary $\partial\Omega$.

Assume that the boundary function $u_0(\x)$ is defined for all $\x\in\Omega$.
The unknown function can then be expanded as

!bt
\[ u = B + \sum_{j\in\If} c_j\baspsi_j,\quad B = u_0 \tp \]
!et
As long as any $\baspsi_j=0$ on $\partial\Omega_D$, we realize that $u=u_0$
on $\partial\Omega_D$.

The variational formula is obtained from Galerkin's method, which
technically means multiplying the PDE by a test
function $v$ and integrating over $\Omega$:

!bt
\[
\int_{\Omega} (\v\cdot\nabla u + \beta u)v\dx =
\int_{\Omega} \nabla\cdot\left( \dfc\nabla u\right)\dx + \int_{\Omega}fv \dx
\tp
\]
!et
The second-order term is integrated by parts, according to the formula
(ref{fem:deq:2D:int:by:parts}):

!bt
\[
\int_{\Omega} \nabla\cdot\left( \dfc\nabla u\right)v \dx =
-\int_{\Omega} \dfc\nabla u\cdot\nabla v\dx
+ \int_{\partial\Omega} \dfc\frac{\partial u}{\partial n} v\ds
\tp
\]
!et
Galerkin's method therefore leads to

!bt
\[
\int_{\Omega} (\v\cdot\nabla u + \beta u)v\dx =
-\int_{\Omega} \dfc\nabla u\cdot\nabla v\dx
+ \int_{\partial\Omega} \dfc\frac{\partial u}{\partial n} v\ds
+ \int_{\Omega} fv \dx
\tp
\]
!et
The boundary term can be developed further by noticing that $v\neq 0$
only on $\partial\Omega_N$,

!bt
\[ \int_{\partial\Omega} \dfc\frac{\partial u}{\partial n} v\ds
= \int_{\partial\Omega_N} \dfc\frac{\partial u}{\partial n} v\ds,
\]
!et
and that on $\partial\Omega_N$, we have the condition
$a\frac{\partial u}{\partial n}=-g$, so the term becomes

!bt
\[
-\int_{\partial\Omega_N} gv\ds\tp
\]
!et
The final variational form is then

!bt
\[
\int_{\Omega} (\v\cdot\nabla u + \beta u)v\dx =
-\int_{\Omega} \dfc\nabla u\cdot\nabla v \dx
- \int_{\partial\Omega_N} g v\ds
+ \int_{\Omega} fv \dx
\tp
\]
!et

Instead of using the integral signs, we may use the inner product
notation:

!bt
\[
(\v\cdot\nabla u, v) + (\beta u,v) =
- (\dfc\nabla u,\nabla v) - (g,v)_{N} + (f,v)
\tp
\]
!et
The subscript $\,{}_N$ in $(g,v)_{N}$ is a notation for a line or surface
integral over $\partial\Omega_N$, while $(\cdot,\cdot)$ is the area/volume
integral over $\Omega$.

We can derive explicit expressions for the linear system for $\sequencej{c}$
that arises from the variational formulation.
Inserting the $u$ expansion results in

!bt
\begin{align*}
\sum_{j\in\If} ((\v\cdot\nabla \baspsi_j, \baspsi_i) &+ (\beta \baspsi_j ,\baspsi_i) + (\dfc\nabla \baspsi_j,\nabla \baspsi_i))c_j = \\
& (g,\baspsi_i)_{N} + (f,\baspsi_i) -
(\v\cdot\nabla u_0, \baspsi_i) + (\beta u_0 ,\baspsi_i) +
(\dfc\nabla u_0,\nabla \baspsi_i)
\tp
\end{align*}
!et
This is a linear system with matrix entries

!bt
\[
A_{i,j} = (\v\cdot\nabla \baspsi_j, \baspsi_i) + (\beta \baspsi_j ,\baspsi_i) + (\dfc\nabla \baspsi_j,\nabla \baspsi_i)
\]
!et
and right-hand side entries

!bt
\[
b_i = (g,\baspsi_i)_{N} + (f,\baspsi_i) -
(\v\cdot\nabla u_0, \baspsi_i) + (\beta u_0 ,\baspsi_i) +
(\dfc\nabla u_0,\nabla \baspsi_i),
\]
!et
for $i,j\in\If$.

In the finite element method, we usually express $u_0$ in terms of
basis functions and restrict $i$ and $j$ to run over the degrees of
freedom that are not prescribed as Dirichlet conditions.
However, we can also keep all the $\sequencej{c}$ as unknowns,
drop the $u_0$ in the expansion for $u$, and incorporate all the
known $c_j$ values in the linear system. This has been explained
in detail in the 1D case, and the technique is the same for 2D and
3D problems.

===== Transformation to a reference cell in 2D and 3D =====

The real power of the finite element method first becomes evident when
we want to solve partial differential equations posed on two- and
three-dimensional domains of non-trivial geometric shape.  As in 1D,
the domain $\Omega$ is divided into $N_e$ non-overlapping cells. The
elements have simple shapes: triangles and quadrilaterals are popular
in 2D, while tetrahedra and box-shapes elements dominate in 3D.  The
finite element basis functions $\basphi_i$ are, as in 1D, polynomials
over each cell.  The integrals in the variational formulation are, as
in 1D, split into contributions from each cell, and these
contributions are calculated by mapping a physical cell, expressed in
physical coordinates $\x$, to a reference cell in a local coordinate
system $\X$. This mapping will now be explained in detail.


We consider an integral of the type

!bt
\begin{equation}
\int_{{\Omega}^{(e)}} \dfc(\x)\nabla\basphi_i\cdot\nabla\basphi_j\dx,
\end{equation}
!et
where the $\basphi_i$ functions are finite element basis functions in
2D or 3D, defined in the physical domain.
Suppose we want to calculate this integral over a reference cell,
denoted by $\tilde\Omega^r$, in a coordinate system with coordinates
$\X = (X_0, X_1)$ (2D) or $\X = (X_0, X_1, X_2)$ (3D).
The mapping between a point $\X$ in the reference coordinate system  and
the corresponding point $\x$ in the physical coordinate system is
given by a vector relation $\x(\X)$.
The corresponding Jacobian, $J$, of this mapping has entries
!bt
\[ J_{i,j}=\frac{\partial x_j}{\partial X_i}\tp \]
!et

The change of variables requires $\dx$ to be replaced by $\det J\dX$.
The derivatives in the $\nabla$ operator in the variational form are
with respect to $\x$, which we may denote by $\nabla_{\x}$.
The $\basphi_i(\x)$ functions in the integral
are replaced by local basis functions $\refphi_r(\X)$ so
the integral features $\nabla_{\x}\refphi_r(\X)$. We readily have
$\nabla_{\X}\refphi_r(\X)$ from formulas for the basis functions in
the reference cell, but
the desired quantity $\nabla_{\x}\refphi_r(\X)$ requires some efforts
to compute. All the details are provided below.

Let $i=q(e,r)$ and consider two space dimensions. By the chain rule,

!bt
\[
\frac{\partial \refphi_r}{\partial X} =
\frac{\partial \basphi_i}{\partial X} =
\frac{\partial \basphi_i}{\partial x}\frac{\partial x}{\partial X} +
\frac{\partial \basphi_i}{\partial y}\frac{\partial y}{\partial X},
\]
!et
and
!bt
\[
\frac{\partial \refphi_r}{\partial Y} =
\frac{\partial \basphi_i}{\partial Y} =
\frac{\partial \basphi_i}{\partial x}\frac{\partial x}{\partial Y} +
\frac{\partial \basphi_i}{\partial y}\frac{\partial y}{\partial Y}
\tp
\]
!et
We can write these two equations as a vector equation
!bt
\[
\left[\begin{array}{c}
\frac{\partial \refphi_r}{\partial X}\\
\frac{\partial \refphi_r}{\partial Y}
\end{array}\right]
=
\left[\begin{array}{cc}
\frac{\partial x}{\partial X} & \frac{\partial y}{\partial X}\\
\frac{\partial x}{\partial Y} & \frac{\partial y}{\partial Y}
\end{array}\right]
\left[\begin{array}{c}
\frac{\partial \basphi_i}{\partial x}\\
\frac{\partial \basphi_i}{\partial y}
\end{array}\right]
\]
!et
Identifying
!bt
\[ \nabla_{\X}\refphi_r = \left[\begin{array}{c}
\frac{\partial \refphi_r}{\partial X}\\
\frac{\partial \refphi_r}{\partial Y}
\end{array}\right],
\quad
J =
\left[\begin{array}{cc}
\frac{\partial x}{\partial X} & \frac{\partial y}{\partial X}\\
\frac{\partial x}{\partial Y} & \frac{\partial y}{\partial Y}
\end{array}\right],
\quad
\nabla_{\x}\basphi_r =
\left[\begin{array}{c}
\frac{\partial \basphi_i}{\partial x}\\
\frac{\partial \basphi_i}{\partial y}
\end{array}\right],
\]
!et
we have the relation

!bt
\[ \nabla_{\X}\refphi_r = J\cdot\nabla_{\x}\basphi_i,\]
!et
which we can solve with respect to $\nabla_{\x}\basphi_i$:


!bt
\begin{equation}
\nabla_{\x}\basphi_i = J^{-1}\cdot\nabla_{\X}\refphi_r\tp
\end{equation}
!et
On the reference cell, $\basphi_i(\x) = \refphi_r(\X)$, so

!bt
\begin{equation}
\nabla_{\x}\refphi_r(\X) = J^{-1}(\X)\cdot\nabla_{\X}\refphi_r(\X)\tp
\end{equation}
!et


This means that we have the following transformation of the
integral in the physical domain to its counterpart over the reference cell:

!bt
\begin{equation}
\int_{\Omega^{(e)}} \dfc(\x)\nabla_{\x}\basphi_i\cdot\nabla_{\x}\basphi_j\dx =
\int_{\tilde\Omega^r} \dfc(\x(\X))(J^{-1}\cdot\nabla_{\X}\refphi_r)\cdot
(J^{-1}\cdot\nabla\refphi_s)\det J\dX
\end{equation}
!et

===== Numerical integration =====

Integrals are normally computed by numerical integration rules.
For multi-dimensional cells, various families of rules exist.
All of them are similar to what is shown in 1D:
$\int f \dx\approx \sum_jw_if(\x_j)$, where $w_j$ are weights and
$\x_j$ are corresponding points.

The file "`numint.py`": "${fem_src}/numint.py" contains the functions
`quadrature_for_triangles(n)` and `quadrature_for_tetrahedra(n)`,
which returns lists of points and weights corresponding to integration
rules with `n` points over the reference triangle
with vertices $(0,0)$, $(1,0)$, $(0,1)$, and the reference tetrahedron
with vertices $(0,0,0)$, $(1,0,0)$, $(0,1,0)$, $(0,0,1)$,
respectively. For example, the first two rules for integration over
a triangle have 1 and 3 points:

!bc ipy
>>> import numint
>>> x, w = numint.quadrature_for_triangles(num_points=1)
>>> x
[(0.3333333333333333, 0.3333333333333333)]
>>> w
[0.5]
>>> x, w = numint.quadrature_for_triangles(num_points=3)
>>> x
[(0.16666666666666666, 0.16666666666666666),
 (0.66666666666666666, 0.16666666666666666),
 (0.16666666666666666, 0.66666666666666666)]
>>> w
[0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
!ec
Rules with 1, 3, 4, and 7 points over the triangle will exactly integrate
polynomials of degree 1, 2, 3, and 4, respectively.
In 3D, rules with 1, 4, 5, and 11 points over the tetrahedron will
exactly integrate polynomials of degree 1, 2, 3, and 4, respectively.



## See http://www.mems.rice.edu/~akin/Elsevier/Chap_10.pdf
## for order of polynomials

===== Convenient formulas for P1 elements in 2D =====

We shall now provide some formulas for piecewise linear $\basphi_i$ functions
and their integrals *in the physical coordinate system*.
These formulas make it convenient to compute with P1 elements without
the need to work in the reference coordinate system and deal with mappings
and Jacobians.
A lot of computational and algorithmic details are hidden by this approach.

Let $\Omega^{(e)}$ be cell number $e$, and let the three vertices
have global vertex numbers $I$, $J$, and $K$.
The corresponding coordinates are
$(\xno{I},\yno{I})$, $(\xno{J},\yno{J})$, and $(\xno{K},\yno{K})$.
The basis function $\basphi_I$ over $\Omega^{(e)}$ have the explicit
formula

!bt
\begin{equation}
\basphi_I (x,y) = \half\Delta \left( \alpha_I + \beta_Ix
+ \gamma_Iy\right),
label{fem:approx:fe:2D:phi:I}
\end{equation}
!et
where
# must split align in two because we need an array with & and \\
# (sphinx, ipynb, pandoc requires splitting of align and & in the
# array confuses the splitting)

!bt
\begin{align}
\alpha_I &= \xno{J}\yno{K} - \xno{K}\yno{J},
label{fem:approx:fe:2D:phi:alpha:I}\\
\beta_I &= \yno{J} - \yno{K},
label{fem:approx:fe:2D:phi:beta:I}\\
\gamma_I &= \xno{K} - \xno{J},
label{fem:approx:fe:2D:phi:gamma:I},
\end{align}
!et
and

!bt
\begin{equation}
2\Delta = \det\left(\begin{array}{rrr}
1 & \xno{I} & \yno{I} \\
1 & \xno{J} & \yno{J} \\
1 & \xno{K} & \yno{K} \end{array}\right)
\tp
label{fem:approx:fe:2D:phi:Delta}
\end{equation}
!et
The quantity $\Delta$ is the area of the cell.

The following formula is often convenient when computing element matrices
and vectors:

!bt
\begin{equation}
\int_{\Omega^{(e)}} \basphi_I^{p}\basphi_J^{q}\basphi_K^{r} dx dy =
{p!q!r!\over (p+q+r+2)!}2\Delta
label{fem:approx:fe:2D:phi:integral}
\tp
\end{equation}
!et
(Note that the $q$ in this formula is not to be mixed with the $q(e,r)$
mapping of degrees of freedom.)

As an example, the element matrix entry
$\int_{\Omega^{(e)}} \basphi_I\basphi_J\dx$
can be computed by setting
$p=q=1$ and $r=0$, when $I\neq J$, yielding $\Delta/12$, and
$p=2$ and $q=r=0$, when $I=J$, resulting in $\Delta/6$.
We collect these numbers in a local element matrix:

!bt
\[
\frac{\Delta}{12}
\left[\begin{array}{ccc}
2 & 1 & 1\\
1 & 2 & 1\\
1 & 1 & 2
\end{array}\right]
\]
!et

The common element matrix entry $\int_{\Omega^{(e)}} \nabla\basphi_I\cdot\nabla\basphi_J\dx$, arising from a Laplace term $\nabla^2u$, can also easily be
computed by the formulas above. We have

!bt
\[ \nabla\basphi_I\cdot\nabla\basphi_J =
\frac{\Delta^2}{4}(\beta_I\beta_J + \gamma_I\gamma_J) = \hbox{const},\]
!et
so that the element matrix entry becomes
$\frac{1}{4}\Delta^3(\beta_I\beta_J + \gamma_I\gamma_J)$.

From an implementational point of view, one will work with local vertex
numbers $r=0,1,2$, parameterize the coefficients in the basis
functions by $r$, and look up vertex coordinates through $q(e,r)$.

Similar formulas exist for integration of P1 elements in 3D.


===== A glimpse of the mathematical theory of the finite element method =====

Almost all books on the finite element method that introduces the
abstract variational problem $a(u,v)=L(v)$ spend considerable pages on
deriving error estimates and other properties of the approximate
solution. The machinery with function spaces and bilinear and linear
forms has the great advantage that a very large class of PDE problems
can be analyzed in a unified way.  This feature is often taken as an
advantage of finite element methods over finite difference and volume
methods.  Since there are so many excellent textbooks on the
mathematical properties of finite element methods
cite{Larson_2013,Braess,Brenner_Scott,JohnsonFEM87,JohnsonCDE96,Quarteroni_Valli}, this text
will not repeat the theory, but give a glimpse of typical assumptions
and general results for elliptic PDEs.

__Remark.__
The mathematical theory of finite element methods is primarily
developed for to stationary PDE problems of elliptic nature whose
solutions are smooth. However, such problems can be solved with the
desired accuracy by most numerical methods and pose no difficulties.
Time-dependent problems, on the other hand, easily lead to
non-physical features in the numerical solutions and therefore
requires more care and knowledge by the user.  Our focus on the
accuracy of the finite element method will of this reason be centered
around time-dependent problems, but then we need a different set of
tools for the analysis. These tools are based on converting finite
element equations to finite difference form and studying Fourier wave
components.

[kam: This is really two remarks in one and we can split
and put them in more appropriate place. Time is dealt with later. ]

=== Abstract variational forms ===

To list the main results from the mathematical theory of finite elements,
we consider linear PDEs with an abstract variational form

!bt
\[ a(u,v) = L(v)\quad\forall v\in V\tp\]
!et
This is the discretized problem (as usual in this book) where we
seek $u\in V$.
The weak formulation of the corresponding continuous problem,
fulfilled by the exact solution $\uex\in\Vex$ is here written as

!bt
\[ a(\uex, v) = L(v)\quad\forall v\in\Vex\tp\]
!et
The space $V$ is finite dimensional (with dimension $N+1$),
while $\Vex$ is infinite dimensional.
Normally
The hope is that $u\rightarrow\uex$ as $N\rightarrow\infty$ and
$V\rightarrow\Vex$.

=== Example on an abstract variational form and associated spaces ===

Consider the problem $-u''(x)=f(x)$ on $\Omega=[0,1]$, with $u(0)=0$ and
$u'(1)=\beta$. The weak form is

!bt
\[ a(u,v) = \int_0^1 u'v'dx,\quad L(v)=\int_0^1fvdx + \beta v(1)\tp\]
!et
The space $V$ for the approximate solution $u$ can be chosen in many
ways as previously described.
The exact solution $\uex$ fulfills $a(u,v)=L(v)$ for all $v$ in $\Vex$,
and to specify what $\Vex$ is, we need to introduce *Hilbert spaces*.
The Hilbert
space $L^2(\Omega)$ consists of all functions that are square-integrable
on $\Omega$:

!bt
\[ L^2(\Omega) = \left\lbrace\int_\Omega v^2dx < \infty\right\rbrace\tp\]
!et
The space $\Vex$ is the space of all functions whose first-order
derivative is also square-integrable:

!bt
\[ \Vex = H^1_0(\Omega) = \left\lbrace v\in L^2(\Omega)\,\vert\,
\frac{dv}{dx}\in L^2(\Omega),\hbox{ and }v(0)=0\right\rbrace\tp\]
!et
The requirements of square-integrable zeroth- and first-order derivatives
are motivated from the formula for $a(u,v)$ where products of the
first-order derivatives are to be integrated on $\Omega$.
We remark that it is common that  $H^1_0$ denote the
space of $H^1$ functions that are zero everywhere on the boundary, but
here we use it for functions that are zero only at $x=0$.

The Sobolev space $H^1_0(\Omega)$ has an inner product

!bt
\[ (u,v)_{H^1} = \int_\Omega (uv + \frac{du}{dx}\frac{dv}{dx})dx,\]
!et
and associated norm

!bt
\[ ||v||_{H^1} = \sqrt{(v,v)_{H^1}}\tp\]
!et

=== Assumptions ===

A set of general results builds on the following
assumptions. Let $\Vex$ be an infinite-dimensional inner-product space
such that $\uex\in\Vex$. The space has an associated norm $||v||$
(e.g., $||v||_{H^1}$ in the example above with $\Vex=H^1_0(\Omega)$).

 o $L(v)$ is linear in its argument.
 o $a(u,v)$ is a bilinear in its arguments.
 o $L(v)$ is bounded (also called continuous) if there exists a positive
   constant $c_0$ such that $|L(v)|\leq c_0||v||$ $\forall v\in \Vex$.
 o $a(u,v)$ is bounded (or continuous) if there exists a positive constant
   $c_1$ such that $|a(u,v)|\leq c_1||u|| ||v||\ \forall u,v\in\Vex$.
 o $a(u,v$) is elliptic (or coercive) if there exists a positive
   constant $c_2$ such that $a(v,v)\geq c_2||v||^2\ \forall v\in\Vex$.
 o $a(u,v)$ is symmetric: $a(u,v)=a(v,u)$.

Based on the above assumptions, which must be verified in each specific
problem, one can derive some general results that are listed below.

=== Existence and uniqueness ===

There exists a unique solution of the problem: find $\uex\in\Vex$
such that

!bt
\[ a(\uex,v)=L(v)\quad\forall v\in\Vex\tp\]
!et
(This result is known as the Lax-Milgram Theorem.
We remark that symmetry is not strictly needed for this theorem.)


=== Stability ===

The solution $\uex\in\Vex$ obeys the stability estimate

!bt
\[ ||u||\leq \frac{c_0}{c_2}\tp\]
!et

=== Equivalent minimization problem ===

The solution $\uex\in\Vex$ also fulfills the minimization problem

!bt
\[ \min_{v\in\Vex} F(v),\quad F(v)=\frac{1}{2}a(v,v) - L(v)\tp\]
!et

=== Best approximation principle ===

The *energy norm* is defined as

!bt
\[ ||v||_a = \sqrt{a(v,v)}\tp\]
!et
The discrete solution $u\in V$ is the best approximation in energy norm,

!bt
\[ ||\uex -  u||_a \leq ||\uex - v||_a\quad\forall v\in V\tp\]
!et
This is quite remarkable: once we have $V$ (i.e., a mesh and a finite element), the Galerkin
method finds the best approximation in this space.
In the example above, we have $||v||_a=\int_0^1 (v')^2dx$, so
the derivative $u'$ is closer to $\uex'$ than any other possible
function in $V$:

!bt
\[ \int_0^1 (\uex' - u')^2dx \leq \int_0^1(u' - v')dx\quad\forall v\in V\tp\]
!et

=== Best approximation property in the norm of the space ===

If $||v||$ is the norm associated with $\Vex$, we have another
best approximation property:

!bt
\[ ||\uex - u||\leq\left(\frac{c_1}{c_2}\right)^{\half}||\uex - v||\quad\forall v\in\V\tp\]
!et

=== Symmetric, positive definite coefficient matrix ===

The discrete problem $a(u,v)=L(v)$ $\forall v\in V$ leads to a linear
system $Ac=b$, where the coefficient matrix $A$ is symmetric ($A^T=A$)
and positive definite ($x^TAx > 0$ for all vectors $x\neq 0$).  One
can then use solution methods that demand less storage and that are
faster and more reliable than solvers for general linear systems. One
is also guaranteed the existence and uniqueness of the discrete
solution $u$.

=== Equivalent matrix minimization problem ===

The solution $c$ of the linear system $Ac=b$ also solves the minimization
problem $\min_w(\half w^TAw - b^Tw$ in the vector space $\Real^{N+1}$.

=== A priori error estimate for the derivative ===

In our sample problem, $-u''=f$ on $\Omega=[0,1]$, $u(0)=0$, $u'(1)=\beta$,
one can derive the following error estimate for Lagrange finite element
approximations of degree $s$:

!bt
\[
\left(\int_0^1 (\uex' - u')^2dx\right)^{\half} \leq Ch^s||\uex||_{H^{s+1}},\\
\]
!et
where $||u||_{H^{s+1}}$ is a norm that integrates the sum of the square of all
derivatives up to order $s+1$,
$C$ is a constant, and $h$ is the maximum
cell length.
The estimate shows that choosing
elements with higher-degree polynomials (large $s$) requires more
smoothness in $\uex$ since higher-order derivatives need to be square-integrable.

A consequence of the error estimate is that $u'\rightarrow \uex'$
as $h\rightarrow 0$, i.e., the approximate solution converges to
the exact one.

The constant $C$ in  depends on the shape
of triangles in 2D and tetrahedra in 3D: squeezed elements with a
small angle lead to a large $C$, and such deformed elements are
not favorable for the accuracy.


One can generalize the above estimate to the general problem class
$a(u,v)=L(v)$: the error in the derivative is proportional
to $h^s$. Note that the expression $||\uex - u||$ in the example
is $||\uex - u||_{H^1}$ so it involves the sum of the zeroth and
first derivative. The appearance of the derivative makes the error
proportional to $h^s$ - if we only look at the solution it
converges as $h^{s+1}$ (see below).

The above estimate is called an *a priori* estimate because the bound
contains the exact solution, which is not computable. There are also
*a posteriori* estimates where the bound involves the approximation
$u$, which is available in computations.

=== A priori error estimate for the solution ===

The finite element solution of our sample problem  fulfills

!bt
\[
||\uex - u|| \leq Ch^{s+1} ||\uex||_{H^{s+1}},
\]
!et
This estimate shows that the error converges as $h^2$ for P1 elements.
An equivalent finite difference method, see Section ref{fem:deq:1D:fdm_vs_fem}, is known to have an error proportional to $h^2$, so the above estimate
is expected.
In general, the convergence is $h^{s+1}$ for elements with polynomials
of degree $s$. Note that the estimate for $u'$ is proportional to
$h$ raised to one power less.
We remark that the second estimate strictly speaking requires extra smoothness (regularity).


# #ifdef UNFINISHED
=== A posteriori error estimate ===

A posteriori error estimates involved the computed approximation $u$ and
can be used to find the local error in each element and guide a
mesh refinement process such that elements with large errors are
subdivided into smaller elements. This is idea of an *adaptive mesh*.
For the present model problem, a typical a posteriori error estimate
is cite{Larson_2013}

!bt
\[ ||\uex - u|| \leq \tilde C\left(\sum_{k\in\mathcal{K}} h^2_k
||f + u''||^2 + \frac{1}{4}h_k
# #endif

!split

======= Convection-Diffusion and Petrov-Galerkin =======

Let us now return to the problem in Section ref{sec:varform:general:convdiff}
!bt
\begin{align}
label{varform:conv:diff:pde}
\v\cdot\nabla u + \beta u &= \nabla\cdot\left( \dfc\nabla u\right) + f,
\quad\x\in\Omega,\\
label{varform:conv:diff:bc1}
u &= u_0,\quad\x\in\partial\Omega_D,\\
label{varform:conv:diff:bc2}
-\dfc\frac{\partial u}{\partial n} &= g,\quad\x\in\partial\Omega_N
\tp
\end{align}
!et

In Section ref{ch:convdiff} we investigated the simplified case in
1D
!bt
\begin{align*}
-u_x - \dfc u_{xx} &= 0, \\
u(0) = 0, u(1) &= 1 \tp
\end{align*}
!et
and the analytical solution was:
!bt
\[
u_e(x) = \frac{e^{-x/\dfc} - 1}{e^{-1/\dfc} - 1}.
\]
!et

The approximation
with global functions failed when $\dfc$ was significantly smaller than
$\v$. The computed solution contained non-physical oscillations that
were orders of magnitude larger than the true solution. The approximation
did however improve as the number of degrees of freedom increased.

The variational problem is:
Find $u\in V$ such that
!bt
\[
a(u,v) = L(v) \forall v
\]
!et
where
!bt
\begin{align*}
a(u, v) &= \int_0^1 -u_x v + \mu u_x v_x \dx, \\
L(v) &=  \int_0^1  0 v \dx = 0  .
\end{align*}
!et

A Galerkin approximation with a finite element approximation is obtained
by letting $u=\sum_{j\in\If} c_j\baspsi_j(x)$ and
$v =  \baspsi_i(x)$ which leads to a linear system
of equations $\sum_j A_{i,j}c_j=b_i$ where
!bt
\begin{align*}
A_{i,j} &= \int_0^1
\mu \baspsi_i(x)_{x} \baspsi_j(x)_{x} + \baspsi_j(x)_x \baspsi_i(x) \dx, \\
b_i &= \int_0^1 0 \baspsi_i(x) \dx \tp
\end{align*}
!et

Figure ref{varform:fe:convdiff:plot} shows the finite element solution on a coarse mesh of 10 elements
for $\mu=0.01$. Clearly, the finite element method has the same problem as was observed
earlier in global functions in Section  ref{sec:varform:general:convdiff}.

#FIGURE: [fig/conv-diff.png, width=400] Solution obtained with Galerkin approximation using linear elements $N_e=10$ and $\mu=0.01$. label{varform:fe:convdiff:plot}


For finite differences, the cure for these oscillations is *upwinding*.
That is, instead
of using a central difference scheme, we employ the following difference
scheme:
!bt
\begin{eqnarray*}
\frac{du}{dx} (x_i) = \frac{1}{h}[u_{i+1}-u_{i}] \quad   \mbox{ if } \ v < 0, \\
\frac{du}{dx} (x_i) = \frac{1}{h}[u_{i}-u_{i-1}] \quad  \mbox{ if } \ v > 0 .
\end{eqnarray*}
!et
Using this scheme, oscillations will disappear as can be seen in
Figure ref{varform:fe:convdiffstab:plot}.

There is a relationship between upwinding and artificial diffusion.  If we discretize $u_x$ with a central difference and add diffusion as $\epsilon =h/2 \Delta $ we get
!bt
\begin{eqnarray*}
 \frac{u_{i+1}  -  u_{i-1}}{2 h}    &  \textrm{central scheme, first order derivative}  \\
+ \frac{h}{2} \, \frac{-u_{i+1}   + 2 u_{i}    -u_{i-1}}{h^2}  &  \textrm{central scheme, second order derivate}   \\
= \frac{u_{i} -u_{i-1}}{h} &  \textrm{upwind scheme}
\end{eqnarray*}
!et

#FIGURE: [fig/conv-diff-stab.png, width=400] Solution obtained upwinding $N_e=10$ and $\mu=0.01$. label{varform:fe:convdiffstab:plot}

Hence, upwinding is equivalent to adding artificial diffusion with $\epsilon=h/2$; that is, in both cases we actually solve the problem

!bt
\[-(\mu+\epsilon)u_{xx} + vu_x = f.\]
!et
using a central difference scheme.

Finite difference upwinding is difficult to express using finite
elements methods, but it is closely to adding some kind of diffusion
to the scheme.  A clever method of adding diffusion is the so-called
*Petrov-Galerkin* method.  The Galerkin approximation consist of
finding $u\in V$ such that for all $v\in V$ the variational
formulation is satisfied.

!bt
\begin{align*}
a(u, v) &= \int_0^1 -u_x v + \mu u_x v_x \dx, \\
L(v) &=  \int_0^1  0 v \dx = 0  .
\end{align*}
!et
The Petrov-Galerkin method is a seemingly innocent and straightforward extension
of Galerkin where
we want to find $u\in V$ such that for all $w\in W$ the
variational formulation is fulfilled.
!bt
\begin{align*}
a(u, w) &= \int_0^1 -u_x w + \mu u_x w_x \dx, \\
L(w) &=  \int_0^1  0 w \dx = 0  .
\end{align*}
!et
However, $W$ cannot be chosen freely. For instance, to obtain a quadratic matrix
the number of basis functions in   $V$ and $W$ must be the same. Let
 $w = v  + \beta v\cdot\nabla v$.

!bt
\begin{eqnarray*}
A_{ij} &=& a(\baspsi_i, \baspsi_j + \beta v\cdot\nabla \baspsi_j )\\
&=& \int_\Omega\mu\nabla \baspsi_i\cdot\nabla (\baspsi_j + \beta v\cdot\nabla \baspsi_j) \dx + \int_\Omega v\nabla \baspsi_i\cdot (\baspsi_j + \beta v\cdot\nabla \baspsi_j) \dx\\
&=&\underbrace{
\int_\Omega\mu\nabla \baspsi_i \cdot \nabla \baspsi_j \dx + \int_\Omega v \cdot \nabla \baspsi_i\, \baspsi_j \dx}_
{\textrm{standard Galerkin}}
\\
&&\lefteqn{
+\underbrace{
\beta\int_\Omega\mu\nabla \baspsi_i\cdot\nabla(v \cdot \nabla \baspsi_j) \dx
}_
{=0\ \textrm{for linear elements}}+
\underbrace{\beta\int_\Omega (v\cdot\nabla \baspsi_i) (v\cdot\nabla \baspsi_j) \dx}
_{\textrm{Artificial diffusion in $v$ direction}}
}
\end{eqnarray*}
!et


[kam: improve figures]
======= Summary =======

 * When approximating $f$ by $u = \sum_j c_j\basphi_j$, the least squares
   method and the Galerkin/projection method give the same result.
   The interpolation/collocation method is simpler and yields different
   (mostly inferior) results.
 * Fourier series expansion can be viewed as a least squares or Galerkin
   approximation procedure with sine and cosine functions.
 * Basis functions should optimally be orthogonal or almost orthogonal,
   because this gives little round-off errors when solving the linear
   system, and the coefficient matrix becomes diagonal or sparse.
   One way to obtain almost orthogonal basis functions is to localize the
   basis by making the basis functions that have local support in only a few
   cells of a mesh. This is utilized in the finite element method.
 * Finite element basis functions are *piecewise* polynomials, normally
   with discontinuous derivatives at the cell boundaries. The basis
   functions overlap very little, leading to stable numerics and sparse
   matrices.
 * To use the finite element method for differential equations, we use
   the Galerkin method or the method of weighted residuals
   to arrive at a variational form. Technically, the differential equation
   is multiplied by a test function and integrated over the domain.
   Second-order derivatives are integrated by parts to allow for typical finite
   element basis functions that have discontinuous derivatives.
 * The least squares method is not much used for finite element solution
   of differential equations of second order, because
   it then involves second-order derivatives which cause trouble for
   basis functions with discontinuous derivatives.
 * We have worked with two common finite element terminologies and
   associated data structures
   (both are much used, especially the first one, while the other is more
   general):
     o *elements*, *nodes*, and *mapping between local and global
       node numbers*
     o an extended element concept consisting of *cell*, *vertices*,
       *degrees of freedom*, *local basis functions*,
       *geometry mapping*, and *mapping between
       local and global degrees of freedom*
 * The meaning of the word "element" is multi-fold: the geometry of a finite
   element (also known as a cell), the geometry and its basis functions,
   or all information listed under point 2 above.
 * One normally computes integrals in the finite element method element
   by element (cell by cell), either in a local reference coordinate
   system or directly in the physical domain.
 * The advantage of working in the reference coordinate system is that
   the mathematical expressions for the basis functions depend on the
   element type only, not the geometry of that element in the physical
   domain.  The disadvantage is that a mapping must be used, and
   derivatives must be transformed from reference to physical
   coordinates.
 * Element contributions to the global linear system are collected in
   an element matrix and vector, which must be assembled into the
   global system using the degree of freedom mapping (`dof_map`) or
   the node numbering mapping (`elements`), depending on which terminology
   that is used.
 * Dirichlet conditions, involving prescribed values of $u$ at the
   boundary, are mathematically taken care of via a boundary function that takes
   on the right Dirichlet values, while the basis functions vanish at
   such boundaries. The finite element method features a general
   expression for the boundary function. In software implementations,
   it is easier to drop the boundary function and the requirement that
   the basis functions must vanish on Dirichlet boundaries and instead
   manipulate the global matrix system (or the element matrix and vector)
   such that the Dirichlet values are imposed on the unknown parameters.
 * Neumann conditions, involving prescribed values of the derivative
   (or flux) of $u$, are incorporated in boundary terms arising from
   integrating terms with second-order derivatives by part.
   Forgetting to account for the boundary terms implies the
   condition $\partial u/\partial n=0$ at parts of the boundary where
   no Dirichlet condition is set.

!split


======= Exercises =======

===== Exercise: Compute the deflection of a cable with 2 P1 elements =====
label{fem:deq:exer:cable:2P1}
file=cable_2P1

Solve the problem for $u$ in Exercise ref{fem:deq:exer:tension:cable}
using two P1 linear elements. Incorporate the condition $u(0)=0$
by two methods: 1) excluding the unknown at $x=0$, 2) keeping the unknown
at $x=0$, but modifying the linear system.

!bsol
From Exercise ref{fem:deq:exer:tension:cable},
the Galerkin method, after integration by parts, reads

!bt
\[ (u',v')=-(1,v)\quad\forall v\in V\tp\]
!et
We have two elements,
$\Omega^{(0)}=[0,\half]$ and $\Omega^{(1)}=[\half,1]$.

__Method 1: Excluding the unknown at $x=0$.__
Since $u(0)=0$, we exclude the value at $x=0$ as degree of freedom in
the linear system. (There is no need for any boundary function.)
The expansion reads $u=c_0\basphi_1(x) + c_1\basphi_2(x)$.
The element matrix has then only one entry in the first element,

!bt
\[ \tilde A^{(0)} = \frac{1}{h}(1)\tp\]
!et
From element 1 we get the usual element matrix

!bt
\[ \tilde A^{(1)} = \frac{1}{h}
\left(\begin{array}{rr}
1 & -1\\
-1 & 1
\end{array}\right)\tp
\]
!et
The element vector in element 0 becomes

!bt
\[ \tilde b^{(0)} = \frac{h}{2}(-1),\]
!et
while the second element gives a contribution

!bt
\[ \tilde b^{(1)} = \frac{h}{2}
\left(\begin{array}{c}
-1 \\
-1
\end{array}\right)\tp
\]
!et
Assembling the contributions gives

!bt
\[ \frac{1}{h}
\left(\begin{array}{cc}
2 & -1\\
-1 & 1
\end{array}\right)
\left(\begin{array}{c}
c_0 \\
c_1
\end{array}\right)
=
- \frac{h}{2}
\left(\begin{array}{c}
2 \\
1
\end{array}\right)\tp
\]
!et
Note that $h=\half$.
Solving this system yields

!bt
\[ c_0 = -\frac{3}{8},\quad c_1=-\half\quad\Rightarrow\quad u=-\frac{3}{8}\basphi_1(x)-\half\baspsi_2(x)\tp\]
!et
Evaluating the exact solution for $x=\half$ and $x=1$, we get $3/8$
and $1/2$, respectively, a result which shows
that the numerical solution with P1 is exact at the three node points.
The difference between the numerical and exact solution is that the
numerical solution varies linearly over the two elements while the
exact solution is quadratic.

__Method 2: Modifying the linear system.__
Now we let $c_i$ correspond to the value at node $\xno{i}$, i.e.,
all known Dirichlet values become part of the linear system.
The expansion is now simply $u=\sum_{i=0}^2c_i\basphi_i(x)$, with
three unknowns $c_0$, $c_1$, and $c_2$.
Now the element matrix in the first and second element are equal.
The same is true for the element vectors.
Assembling yields

!bt
\[ \frac{1}{h}
\left(\begin{array}{ccc}
1 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 1
\end{array}\right)
\left(\begin{array}{c}
c_0 \\
c_1\\
c_2
\end{array}\right)
=
- \frac{h}{2}
\left(\begin{array}{c}
1\\
2\\
1
\end{array}\right)\tp
\]
!et
The next step is to modify the linear system to implement the
Dirichlet condition $c_0=0$. We first multiply by $h=\half$ and replace the
first equation by $c_0=0$:

!bt
\[
\left(\begin{array}{ccc}
1 & 0 & 0\\
-1 & 2 & -1\\
0 & -1 & 1
\end{array}\right)
\left(\begin{array}{c}
c_0 \\
c_1\\
c_2
\end{array}\right)
=
- \left(\begin{array}{c}
0\\
\frac{1}{4}\\
\frac{1}{8}
\end{array}\right)\tp
\]
!et
We see that the remaining $2\times 2$ system is identical to the one
previously solved, and the solution is the same.

!bt
\[ u = 0\basphi_0(x) - \frac{3}{8}\basphi_1(x) - \half\baspsi_2(x)\tp\]
!et

!esol

===== Exercise: Compute the deflection of a cable with 1 P2 element =====
label{fem:deq:exer:cable:1P2}
file=cable_1P2

Solve the problem for $u$ in Exercise ref{fem:deq:exer:tension:cable}
using one P2 element with quadratic basis functions.

!bsol
The P2 basis functions on a reference element $[-1,1]$ are

!bt
\begin{align*}
\refphi_0(X) &= \half (X-1)X
\\
\refphi_1(X) &= 1 - X^2
\\
\refphi_2(X) &= \half (X+1)X
\end{align*}
!et
The element matrix and vector are easily calculated by some lines with
`sympy`:

@@@CODE exer/cable_1P2.py fromto: import sympy@
The formatted element matrix and vector output becomes

!bc
[[7/(3*h) -8/(3*h) 1/(3*h)]
 [-8/(3*h) 16/(3*h) -8/(3*h)]
 [1/(3*h) -8/(3*h) 7/(3*h)]]
[-h/6 -2*h/3 -h/6]
!ec
or in mathematical notation:

!bt
\[ \tilde A^{(e)}=\frac{1}{3h}
\left(\begin{array}{ccc}
7 & -8 & 1\\
-8 & 16 & -8\\
1 & -8 & 7
\end{array}\right),\quad
\tilde b^{(e)} = - \frac{h}{6}
\left(\begin{array}{c}
1\\
4\\
1
\end{array}\right)\tp
\]
!et

__Method 1: Excluding the unknown at $x=0$.__
The expansion is $u=c_0\basphi_1(x) + c_1\basphi_2(x)$. The element matrix
corresponding to the first element excludes contributions associated with
the unknown at the left node, i.e., we exclude row and column 0. In the
element vector, we exclude the first entry.

!bt
\[ \tilde A^{(0)}=\frac{1}{3h}
\left(\begin{array}{cc}
16 & -8\\
-8 & 7
\end{array}\right),\quad
\tilde b^{(e)} = - \frac{h}{6}
\left(\begin{array}{c}
4\\
1
\end{array}\right)\tp
\]
!et
Now, $h=1$.
The solution of the linear system

!bt
\[ \frac{1}{3h}
\left(\begin{array}{cc}
16 & -8\\
-8 & 7
\end{array}\right)
\left(\begin{array}{c}
c_1\\
c_2
\end{array}\right)
=
- \frac{h}{6}
\left(\begin{array}{c}
4\\
1
\end{array}\right)
\]
!et
is $c_1=3/8$ and $c_2=1/2$. As for P1 elements in Exercise
ref{fem:deq:exer:cable:2P1}, the values at the nodes are exact, but
this time the variation between the nodes is quadratic, i.e., exact.
One P2 element produces the complete, exact solution.

__Method 2: Modifying the linear system.__
This time the expansion reads $u=\sum_{i=0}^2 c_i\basphi_i(x)$ with
three unknowns $c_0$, $c_1$, and $c_2$. The linear system consists
of the complete $3\times 3$ element matrix and the corresponding
element vector:

!bt
\[ \frac{1}{3h}
\left(\begin{array}{ccc}
7 & -8 & 1\\
-8 & 16 & -8\\
1 & -8 & 7
\end{array}\right)
\left(\begin{array}{c}
c_0\\
c_1\\
c_2
\end{array}\right)
=
- \frac{h}{6}
\left(\begin{array}{c}
1\\
4\\
1
\end{array}\right)\tp
\]
!et
The boundary condition is incorporated by replacing the first equation
by $c_0=0$, but prior to taking that action, we multiply by $3h$ and
insert $h=1$.

!bt
\[
\left(\begin{array}{ccc}
1 & 0 & 0   \\
-8 & 16 & -8\\
1 & -8 & 7
\end{array}\right)
\left(\begin{array}{c}
c_0\\
c_1\\
c_2
\end{array}\right)
=
\left(\begin{array}{c}
0\\
- 2\\
- \half
\end{array}\right)\tp
\]
!et
Realizing that $c_0=0$, which means we can remove the first column of
the system, shows that the equations are the same as above and hence
that the solution is identical.
!esol


===== Exercise: Compute the deflection of a cable with a step load =====
label{fem:deq:exer:cable:stepload}
file=cable_discont_load

We consider the deflection of a tension cable as described in
Exercise ref{fem:deq:exer:tension:cable}: $w''=\ell$, $w(0)=w(L)=0$.
Now the load is discontinuous:

!bt
\[ \ell (x) =\left\lbrace\begin{array}{ll}
\ell_1, & x <L/2,\\
\ell_2, & x \geq L/2
\end{array}\right.\quad x\in [0,L]
\tp
\]
!et
This load is not symmetric
with respect to the midpoint $x=L/2$ so the solution loses its symmetry.
Scaling the problem by introducing

!bt
\[ \bar x = \frac{x}{L/2},\quad u = \frac{w}{w_c},\quad\bar\ell = \frac{\ell - \ell_1}{\ell_2 - \ell_1}\tp\]
!et
This leads to a scaled problem on $[0,2]$ (we rename $\bar x$ as $x$
for convenience):

!bt
\[ u'' = \bar\ell(x) = \left\lbrace\begin{array}{ll}
1, & x <1,\\
0, & x \geq 1
\end{array}\right.
\quad x\in (0,1),\quad u(0)=0,\ u(2)=0
\tp \]
!et

!bsubex
Find the analytical solution of the problem.

!bhint
Integrate the equation separately for $x<1$ and $x>1$. Use
the conditions that $u$ and $u'$ must be continuous at $x=1$.
!ehint

!bsol
For $x<1$ we get $u_1(x) = C_1x + C_2$, and the boundary condition
$u_1(0)=0$ implies $C_2=0$. For $x>1$ we get $u_2(x)=\half x^2 + C_3x + C_4$.
Continuity of $u'(1)$ leads to

!bt
\[ C_1 = 1 + C_3,\]
!et
and continuity of $u(1)$ means

!bt
\[  C_1 = \half + C_3 + C_4,\]
!et
while the condition $u_2(2)=0$ gives the third equation we need:

!bt
\[ 2 + 2C_3 + C_4 = 0\tp\]
!et
We use `sympy` to solve them:

!bc pyshell
>>> from sympy import symbols, Rational, solve
>>> C1, C3, C4 = symbols('C1 C3 C4')
>>> solve([C1 - 1 - C3,
           C1 - Rational(1,2) - C3 - C4,
	   2 + 2*C3 + C4], [C1,C3,C4])
{C1: -1/4, C4: 1/2, C3: -5/4}
!ec
Then

!bt
\[ u(x) = \left\lbrace\begin{array}{ll}
-\frac{1}{4}x, & x\leq 1,\\
\half x^2 - \frac{5}{4}x + \half, & x\geq 1
\end{array}\right.
\]
!et

FIGURE: [fig/cable_discont_load_u_exact, width=500 frac=0.8]

!esol
!esubex

!bsubex
Use $\baspsi_i = \sin((i+1)\frac{\pi x}{2})$,
$i=0,\ldots,N$ and the Galerkin method to find an approximate
solution $u=\sum_j c_j\baspsi_j$.
Plot how fast the coefficients $c_j$ tend to zero (on a log scale).

!bsol
The Galerkin formulation of the problem becomes

!bt
\[ (u',v') = -(\bar\ell, v) = \left\lbrace\begin{array}{ll}
0, & x\leq 1,\\
-(1,v), & x\geq 1
\end{array}\right.\quad\forall v\in V\tp\]
!et
A requirement is that $v(0)=v(2)=0$ because of the boundary conditions
on $u$. The chosen basis functions
fulfill this requirement for any integer $i$. Inserting
$u=\sum_{j=0}^N c_j\baspsi_j$ and $v=\baspsi_i$, $i=0,\ldots,N$,
gives as usual the linear system $\sum_j A_{i,j}c_j = b_i$, $i=0,\ldots,N$,
where

!bt
\[
A_{i,j} = (i+1)(j+1)\frac{\pi^2}{4}\int_0^2 \cos((i+1)\frac{\pi x}{2})
\cos((j+1)\frac{\pi x}{2})dx\tp\]
!et
The cosine functions are orthogonal on $[0,2]$ so $A_{i,j}=0$ for
$i\neq j$, while $A_{i,i}$ is computed (e.g., by `sympy`) as in
Exercise ref{fem:deq:exer:tension:cable}, part e. The result is

!bt
\[ A_{i,i} = (i+1)^2\frac{\pi^2}{4}\tp\]
!et
The right-hand side is

!bt
\[ b_i =
-\int_1^2 \sin((i+1)\frac{\pi x}{2})dx
= \frac{2}{\pi (i+1)}(\cos((i+1)\pi) - \cos((i+1)\pi/2))\tp\]
!et
(Trying to do the integral in `sympy` gives a complicated expression that
needs discussion - it
is easier to do all calculations by hand.)
We have that $\cos((i+1)\pi = -1$ for $i$ even and
$\cos((i+1)\pi = 1$ for $i$ odd, while $\cos((i+1)\pi/2)$ is
discussed in
Exercise ref{fem:deq:exer:tension:cable}, part d. The values
of $\cos((i+1)\pi) - \cos((i+1)\pi/2)$ can be summarized in the following
table:

|-------------------------------|
| $i\hbox{ mod } 4 = 0$ | $(i-1)\hbox{ mod } 4 = 0$ | $(i-2)\hbox{ mod } 4 = 0$ | $(i-3)\hbox{ mod } 4 = 0$ |
|---c-------c-------c-------c---|
| $-1 -0$     |  $1 - (-1)$    | $-1 - 0$     |  $1-1$  |
|-------------------------------|

The following function computes the approximate solution:

@@@CODE exer/cable_discont_load.py fromto: def sine_solution@def plot_sine_sol

The exact solution is a function defined in a piecewise way. Below we make an implementation that works both for array and scalar arguments:

@@@CODE exer/cable_discont_load.py fromto: def exact_solution@def sine_sol
Now we can make a plot of the exact solution and approximate solutions for
various $N$:

@@@CODE exer/cable_discont_load.py fromto: def plot_sine_sol@P1_sol

FIGURE: [fig/cable_discont_load_sines, width=500 frac=0.8]

!esol
!esubex

!bsubex
Solve the problem with P1 finite elements.
Plot the solution for $N_e=2,4,8$ elements.

!bsol
The element matrices and vectors are as for the well-known model
problem $u''=1$, except that the element vectors vanish for all
elements in $[0,1]$. The following function defines a uniform mesh
of P1 elements and runs a finite element algorithm where we use
ready-made/known formulas for the element matrix and vector:

@@@CODE exer/cable_discont_load.py fromto: def P1_sol@if __

FIGURE: [fig/cable_discont_load_P1, width=500 frac=0.8]

!esol
!esubex


# #ifdef EXTRA
===== Exercise: Show equivalence between linear systems =====
label{fem:deq:exer:Aub:essbc:equiv}

Incorporation of Dirichlet conditions at $x=0$ and $x=L$ in a finite
element mesh on $\Omega=[0,L]$ can either be done by introducing an
expansion $u(x)=U_0\basphi_0 + U_{N_n-1}\basphi_{N_n-1} +
\sum_{j=0}^{N} c_j\basphi_{\nu(j)}$, with $N=N_n-3$ and considering
$u$ values at the inner nodes as unknowns, *or* one can assemble the
matrix system with $u(x)=\sum_{j=0}^{N=N_n-1} c_j\basphi_j$ and
afterwards replace the rows corresponding to known $c_j$ values by the
boundary conditions.  Show that the two approaches are equivalent.

# Start with the system N=N_n-1 and eliminate c_0 and c_N and show that
# the resulting system is equal to the other. You need two numberings
# of the unknowns in the two systems. Essentiall, the B(x) function
# gives an a(u,v) term in the first and last equation that is A[0,0]
# and A[N,N] times the U0 and UN, resp.
# #endif

===== Exercise: Compute with a non-uniform mesh =====
label{fem:deq:exer:1D:mesh:nonuniform}
file=nonuniform_P1

!bsubex
Derive the linear system for the problem $-u''=2$ on $[0,1]$, with
$u(0)=0$ and $u(1)=1$, using P1 elements and a *non-uniform* mesh. The
vertices have coordinates $\xno{0}=0 < \xno{1} <\cdots <
\xno{N_n-1}=1$, and the length of cell number $e$ is $h_e = \xno{e+1}
-\xno{e}$.

!bsol
The element matrix and vector for this problem is given by
(ref{fem:deq:1D:ex1:Ab:elm}). The change in this exercise is that
$h$ is not a constant element length, but varying with the element
number $e$. We therefore write

!bt
\[
\tilde A^{(e)} =\frac{1}{h_e}\left(\begin{array}{rr}
1 & -1\\
-1 & 1
\end{array}\right),\quad
\tilde b^{(e)} = h_e\left(\begin{array}{c}
1\\
1
\end{array}\right)\tp
\]
!et
Assembling such element matrices yields

!bt
\[
\left(
\begin{array}{ccccccccc}
h_0^{-1} & -h_0^{-1} & 0 &\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
-h_0^{-1} & h_0^{-1}+h_1^{-1} & -h_1^{-1} & \ddots &   & &  & &  \vdots \\
0 & -h_1^{-1} & h_1^{-1} + h_2^{-1} & -h_2^{-1} &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & -h_{i-1}^{-1} & h_{i-1}^{-1} + h_i^{-1} & -h_i^{-1} & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & \ddots &\ddots  & -h_{N_e}^{-1} \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & -h_{N_e}^{-1} & h_{N_e}^{-1}
\end{array}
\right)
\]
!et
The element vectors assemble to

!bt
\[
\left(
\begin{array}{c}
h_0 \\
h_0 + h_1\\
\vdots\\
\vdots \\
\vdots \\
h_{i-1} + h_i\\
\vdots \\
\vdots\\
h_{N_e}
\end{array}
\right)
\]
!et
!esol
!esubex


!bsubex
It is of interest to compare the discrete equations for the finite
element method in a non-uniform mesh with the corresponding discrete
equations arising from a finite difference method. Go through the
derivation of the finite difference formula $u''(x_i) \approx [D_x D_x
u]_i$ and modify it to find a natural discretization of $u''(x_i)$ on
a non-uniform mesh. Compare the finite element and difference
discretizations

!bsol
Using the definition of the centered, 2nd-order finite difference approximation
to $u''$ we can set up

!bt
\[ [D_xD_x u]_i = [D_x(D_x u)]_i = \frac{\frac{u_{i+1}-u_i}{x_{i+1}-x_i} -
\frac{u_{i}-u_{i-1}}{x_{i}-x_{i-1}}}{x_{i+1/2} - x_{i-1/2}}\tp\]
!et
Now,

!bt
\[ x_{i+1/2} - x_{i-1/2} = \half (x_i - x_{i-1}) + \half(x_{i+1}-x_i) = \half(x_{i+1}-x_{i-1})\tp\]
!et
We then get the difference equation

!bt
\[ u''(x_i)\approx \frac{2}{h_i + h_{i-1}}\left(
\frac{u_{i+1}-u_i}{h_{i}} - \frac{u_{i}-u_{i-1}}{h_{i-1}}\right) = 2\tp\]
!et
The factor 2 on either side cancels.

Looking at the finite element equations in a), the equation for a general
row $i$ reads

!bt
\[ -\frac{1}{h_{i-1}}c_{i-1} + (\frac{1}{h_{i-1}} - \frac{1}{h_{i}})c_i
+ \frac{1}{h_{i}}c_{i+1} = h_{i-1} + h_i\tp\]
!et
Replacing $c_i$ by $u_i$ (assuming we keep unknowns at all nodes) and
rearranging gives

!bt
\[ \frac{1}{h_{i-1}}(u_i - u_{i-1}) - \frac{1}{h_{i}}(u_{i+1}-u_i)
 = h_{i-1} + h_i\tp\]
!et
Dividing by the right-hand side gives

!bt
\[ \frac{1}{h_{i-1} + h_i}\left(\frac{1}{h_{i-1}}(u_i - u_{i-1}) - \frac{1}{h_{i}}(u_{i+1}-u_i)\right) = 1\tp\]
!et
This is the same difference equation as we have in the finite difference
method.

[hpl: Is a minus wrong here?]
!esol
!esubex

===== Problem: Solve a 1D finite element problem by hand =====
label{fem:deq:exer:1D:gen:problem1}
file=convdiff1D_P1

The following scaled 1D problem is a very simple, yet relevant, model
for convective transport in fluids:

!bt
\begin{equation}
u' = \epsilon u'' ,\quad u(0)=0,\ u(1)=1,\ x\in [0,1]
\tp
\end{equation}
!et

!bsubex
Find the analytical solution to this problem.
(Introduce $w=u'$, solve the first-order differential equation for $w(x)$,
and integrate once more.)
!esubex

!bsubex
Derive the variational form of this problem.
!esubex

!bsubex
Introduce a finite element mesh with uniform partitioning.
Use P1 elements and compute the element matrix and vector for
a general element.
!esubex

!bsubex
Incorporate the boundary conditions and
assemble the element contributions.
!esubex

!bsubex
Identify the resulting linear system as a finite difference discretization
of the differential equation using

!bt
\[ [D_{2x}u = \epsilon D_xD_x u]_i \tp  \]
!et
!esubex

!bsubex
Compute the numerical solution and plot it together with the exact solution
for a mesh with 20 elements and
$\epsilon=10, 1, 0.1, 0.01$.
!esubex

===== Exercise: Investigate exact finite element solutions =====
label{fem:deq:exer:1D:exact_numerics}
file=u_xx_xm_P1to4

Consider

!bt
\[ -u''(x)=x^m,\quad x\in (0,L),\quad u'(0)=C,\ u(L)=D,\]
!et
where $m\geq 0$ is an integer, and $L$, $C$, and $D$ are given numbers.
Utilize a mesh with two (non-uniform) elements: $\Omega^{(0)}=[0,3]$ and
$\Omega^{(0)}=[3,4]$.
Plot the exact solution and the finite element solution for
polynomial degree $d=1,2,3,4$ and $m=0, 1, 2, 3, 4$. Find values of $d$ and $m$
that make the finite element solution exact at the nodes in the mesh.


!bhint
Use the `mesh_uniform`, `finite_element1D`, and `u_glob2` functions
from the `fe1D.py` module.
!ehint

!bsol
The `model2` function from Section ref{fem:deq:1D:models:simple}
can find the exact solution by `model2(x**m, L, C, D)`.
We fix, for simplicity, the values of $L$, $C$, and $D$ as
$L=4$, $C=5$, and $D=2$.
After calculating a symbolic solution, we can convert the expression
to a Python function with `sympy.lambdify`.
For each $d$ value we then create a uniform mesh and displace the
vertex with number 1 to the value 3.
The various functions for specifying the element matrix and vector
entries are as given in Section ref{fem:deq:1D:code:fe_sparse},
since the model problem is the same. Our code then becomes

@@@CODE exer/u_xx_xm_P1to4.py fromto: from u_xx@for ext in

First we look at the numerical solution at the nodes:

!bc
m=0, u: -x**2/2 + 5*x - 10
Max diff at nodes, d=1: 2.22044604925e-16
Max diff at nodes, d=2: 3.5527136788e-15
Max diff at nodes, d=3: 1.7763568394e-15
Max diff at nodes, d=4: 2.46913600677e-13

m=1, u: -x**3/6 + 5*x - 22/3
Max diff at nodes, d=1: 8.881784197e-16
Max diff at nodes, d=2: 1.7763568394e-15
Max diff at nodes, d=3: 7.9936057773e-15
Max diff at nodes, d=4: 3.01092484278e-13

m=2, u: -x**4/12 + 5*x + 10/3
Max diff at nodes, d=1: 3.10862446895e-15
Max diff at nodes, d=2: 0.084375
Max diff at nodes, d=3: 0.0333333333333
Max diff at nodes, d=4: 5.20472553944e-13

m=3, u: -x**5/20 + 5*x + 166/5
Max diff at nodes, d=1: 1.35555555556
Max diff at nodes, d=2: 0.3796875
Max diff at nodes, d=3: 0.185714285714
Max diff at nodes, d=4: 0.0254255022334

m=4, u: -x**6/30 + 5*x + 1778/15
Max diff at nodes, d=1: 4.8
Max diff at nodes, d=2: 1.4428125
Max diff at nodes, d=3: 0.719047619047
Max diff at nodes, d=4: 0.16865583147
!ec
We observe that all elements are capable of computing the exact values
at the nodes for $m=0$ and $m=1$. With $m=0$, the solution is quadratic
in $x$, and P2, P3, and P4 will be exact. It is more of a surprise that
also the P1 elements are exact in this case.
A peculiar feature is that P1
elements are also exact at the nodes $m=2$, but not P2 and P3 elements
(the solution goes like $x^4$ so it is not surprising that P2 and P3
elements give a numerical error also at the nodes).
Clearly, P4 elements produce the exact solution for $m=4$ since $u$
is a polynomial of degree 4. For larger $m$ values we have
discrepancy between the numerical and exact values at the nodes.

% for m in [0, 1, 2, 3, 4]:

__Plots for m=${m}.__

FIGURE: [fig/u_xx_xm${m}_P1to4, width=800 frac=1]
% endfor
!esol


# Could have shooting method as a project

===== Exercise: Compare finite elements and differences for a radially symmetric Poisson equation =====
label{fem:deq:exer:1D:Poisson:polar}
file=radial_Poisson1D_P1

We consider the Poisson problem in a disk with radius $R$ with
Dirichlet conditions at the boundary.
Given that the solution is radially symmetric and hence dependent only on
the radial coordinate ($r=\sqrt{x^2+y^2}$), we can reduce the problem
to a 1D Poisson equation

!bt
\begin{equation}
-\frac{1}{r}\frac{d}{dr}\left( r\frac{du}{dr}\right) = f(r),\quad r\in (0,R),\
u'(0)=0,\ u(R)=U_R
\tp
label{fem:deq:exer:1D:Poisson:polar:eq}
\end{equation}
!et

!bsubex
Derive a variational form of (ref{fem:deq:exer:1D:Poisson:polar:eq})
by integrating over the whole disk, or posed equivalently: use
a weighting function $2\pi r v(r)$ and integrate $r$ from $0$ to $R$.
!esubex

!bsubex
Use a uniform mesh partition with P1 elements and show what the
resulting set of equations becomes. Integrate the matrix entries
exact by hand, but use a Trapezoidal rule to integrate the $f$ term.
!esubex

!bsubex
Explain that an intuitive
finite difference method applied to (ref{fem:deq:exer:1D:Poisson:polar:eq})
gives

!bt
\[
\frac{1}{r_i}\frac{1}{h^2}\left( r_{i+\half}(u_{i+1}-u_i) -
r_{i-\half}(u_{i}-u_{i-1})\right) = f_i,\quad i=rh
\tp
\]
!et

For $i=0$ the factor $1/r_i$ seemingly becomes problematic. One must always
have $u'(0)=0$, because of the radial symmetry, which implies
$u_{-1}=u_1$, if we allow introduction of a fictitious value $u_{-1}$.
Using this $u_{-1}$ in the difference equation for $i=0$ gives

!bt
\begin{align*}
&\frac{1}{r_0}\frac{1}{h^2}\left( r_{\half}(u_{1}-u_0) -
r_{-\half}(u_{0}-u_{1})\right) = \\
& \qquad
\frac{1}{r_0}\frac{1}{2h^2}\left( (r_0 + r_1)(u_{1}-u_0) -
(r_{-1} + r_0)(u_{0}-u_{1})\right) \approx
2(u_1-u_0),
\end{align*}
!et
if we use $r_{-1}+r_1\approx 2r_0$.

Set up the complete set of equations for the finite difference method
and compare to the finite element method in case a Trapezoidal rule
is used to integrate the $f$ term in the latter method.
!esubex

===== Exercise: Compute with variable coefficients and P1 elements by hand =====
label{fem:deq:exer:1D:gen:problem2}
file=atan1D_P1

Consider the problem
!bt
\begin{equation}
-\frac{d}{dx}\left( \dfc(x)\frac{du}{dx}\right) + \gamma u = f(x),
\quad x\in\Omega=[0,L],\quad u(0)=\dfc,\ u'(L)=\beta\tp
label{fem:deq:1D:model4}
\end{equation}
!et
We choose $\dfc(x)=1+x^2$. Then
!bt
\begin{equation} u(x) = \dfc + \beta(1+L^2)\tan^{-1}(x),
\end{equation}
!et
is an exact solution if $f(x) = \gamma u$.

Derive a variational formulation and compute general expressions for the
element matrix and vector in an arbitrary element, using P1 elements
and a uniform partitioning of $[0,L]$. The right-hand side
integral is challenging and can be computed by a numerical integration
rule. The Trapezoidal rule (ref{fem:approx:fe:numint1:trapez})
gives particularly simple expressions.

===== Exercise: Solve a 2D Poisson equation using polynomials and sines =====
label{fem:deq:exer:2D:torsion:xy:sin}
file=torsion_sin_xy

The classical problem of applying a torque to the ends of a rod
can be modeled by a Poisson equation defined in the cross section $\Omega$:

!bt
\[ -\nabla^2 u = 2,\quad (x,y)\in\Omega,\]
!et
with $u=0$ on $\partial\Omega$. Exactly the same problem arises for
the deflection of a membrane with shape $\Omega$ under a constant load.

For a circular cross section one can readily
find an analytical solution. For a rectangular cross section the analytical
approach ends up with a sine series. The idea in this exercise is to
use a single basis function to obtain an approximate answer.

We assume for simplicity that the cross section is the unit square:
$\Omega = [0,1]\times [0,1]$.

!bsubex
We consider the basis
$\baspsi_{p,q}(x,y) = \sin((p+1)\pi x)\sin (q\pi y)$, $p,q=0,\ldots,n$.
These basis functions fulfill the Dirichlet condition.
Use a Galerkin method and $n=0$.
!esubex

!bsubex
The basis function involving sine functions are orthogonal.
Use this property in the Galerkin method
to derive the coefficients $c_{p,q}$ in a
formula $u=\sum_p\sum_q c_{p,q}\baspsi_{p,q}(x,y)$.
!esubex

!bsubex
Another possible basis is
$\baspsi_i(x,y) = (x(1-x)y(1-y))^{i+1}$, $i=0,\ldots,N$.
Use the Galerkin method to compute the solution for $N=0$.
Which choice of a single basis function is best,
$u\sim x(1-x)y(1-y)$ or $u\sim \sin(\pi x)\sin(\pi y)$?
In order to answer the question,
it is necessary to search the web or the literature for an accurate
estimate of the maximum $u$ value at $x=y=1/2$.
!esubex

# #ifdef 2DO
===== Exercise: Solve a 2D Poisson equation utilizing symmetry =====
label{fem:deq:exer:2D:torsion:xy:sin:symm}
file=torsion_symmetry

The solution of the Poisson problem in Exercise ref{fem:deq:exer:2D:torsion:xy:sin} is symmetric with respect to the lines $x=1/2$ and $y=1/2$.
One can therefore reduce the domain and pose the problem in, e.g.,
$\Omega = [0,1/2]\times [0,1/2]$. The boundary conditions are
$u=0$ on the $x=0$ and $y=0$ boundaries, as before, while the new
boundaries $x=1/2$ and $y=1/2$ must have a symmetry condition
$\partial u/\partial n = 0$.

No point in just repeating the previous calculations as the basis functions
work directly here. If we recsale to the unit square the basis functions
must be adjusted, but the scaling can just be applied to them to.
There must be some better point. Obviously, using P1 elements the
reduction in size is important, so maybe wait until a program for P1
elements is available...?

# #endif
